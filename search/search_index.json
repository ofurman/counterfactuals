{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"CEL: Counterfactual Explanations Library","text":"<p>A comprehensive Python framework for generating and evaluating counterfactual explanations in machine learning models.</p> <p>CEL (Counterfactual Explanations Library) provides a unified framework for counterfactual explanation methods, datasets, and evaluation metrics. Whether you're researching new methods, benchmarking existing approaches, or building explainable AI systems, this library has you covered.</p>"},{"location":"#features","title":"Features","text":"<p>17+ Explanation Methods</p> <p>Local, global, and group-level counterfactual methods including PPCEF, DiCE, GLOBE-CE, and more.</p> <p>Explore Methods \u2192</p> <p>22 Pre-configured Datasets</p> <p>Ready-to-use datasets for classification and regression tasks with built-in preprocessing.</p> <p>View Datasets \u2192</p> <p>18+ Evaluation Metrics</p> <p>Comprehensive metrics for validity, proximity, sparsity, plausibility, and diversity.</p> <p>See Metrics \u2192</p> <p>End-to-End Pipelines</p> <p>Hydra-based configuration system for reproducible experiments with MLflow logging.</p> <p>Run Pipelines \u2192</p>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<pre><code># Using uv (recommended)\nuv sync\n\n# Or using pip\npip install -e .\n</code></pre>"},{"location":"#generate-your-first-counterfactual","title":"Generate Your First Counterfactual","text":"<pre><code>from counterfactuals.datasets import FileDataset\nfrom counterfactuals.models.classifiers import MLPClassifier\nfrom counterfactuals.models.generators import MaskedAutoregressiveFlow\nfrom counterfactuals.cf_methods.local_methods import PPCEF\n\n# 1. Load dataset\ndataset = FileDataset(config_path=\"config/datasets/adult.yaml\")\n\n# 2. Train classifier\nclassifier = MLPClassifier(input_dim=14, hidden_dims=[64, 32], output_dim=2)\nclassifier.fit(train_loader, test_loader, epochs=50)\n\n# 3. Train generative model\nflow = MaskedAutoregressiveFlow(input_dim=14, hidden_dims=[64, 64], n_layers=5)\nflow.fit(train_loader, test_loader, epochs=100)\n\n# 4. Generate counterfactual (using PPCEF - one of 17+ available methods)\nmethod = PPCEF(gen_model=flow, disc_model=classifier, ...)\nresult = method.explain(X=instance, y_origin=0, y_target=1, ...)\n\nprint(f\"Original: {instance}\")\nprint(f\"Counterfactual: {result.x_cfs}\")\n</code></pre> <p>Full Quick Start Tutorial \u2192</p>"},{"location":"#method-categories","title":"Method Categories","text":"<p>The library organizes counterfactual methods into three categories:</p> <pre><code>flowchart TD\n    A[Counterfactual Methods] --&gt; B[Local Methods]\n    A --&gt; C[Global Methods]\n    A --&gt; D[Group Methods]\n\n    B --&gt; B1[PPCEF]\n    B --&gt; B2[DICE]\n    B --&gt; B3[DiCoFlex]\n    B --&gt; B4[WACH]\n    B --&gt; B5[...]\n\n    C --&gt; C1[GLOBE-CE]\n    C --&gt; C2[AReS]\n\n    D --&gt; D1[ReViCE]\n    D --&gt; D2[GLANCE]\n    D --&gt; D3[Group GLOBE-CE]</code></pre> Category Scope Best For Local Single instance Individual recourse, debugging Global Entire dataset Policy insights, systematic patterns Group Subpopulations Semi-personalized recourse"},{"location":"#why-counterfactuals","title":"Why Counterfactuals?","text":"<p>Counterfactual explanations answer: \"What would need to change for a different outcome?\"</p> <p>Example</p> <p>Loan Application Denied</p> <p>\"If your income were $5,000 higher OR your debt-to-income ratio were below 30%, your loan would be approved.\"</p> <p>This provides actionable recourse - concrete steps users can take to achieve their desired outcome.</p>"},{"location":"#architecture","title":"Architecture","text":"<pre><code>flowchart LR\n    subgraph Data\n        D1[Datasets]\n        D2[Preprocessing]\n    end\n\n    subgraph Models\n        M1[Classifiers]\n        M2[Generative Models]\n    end\n\n    subgraph Methods\n        CF1[Local CFs]\n        CF2[Global CFs]\n        CF3[Group CFs]\n    end\n\n    subgraph Evaluation\n        E1[Metrics]\n        E2[Benchmarks]\n    end\n\n    D1 --&gt; D2\n    D2 --&gt; M1\n    D2 --&gt; M2\n    M1 --&gt; Methods\n    M2 --&gt; Methods\n    Methods --&gt; E1\n    E1 --&gt; E2</code></pre>"},{"location":"#next-steps","title":"Next Steps","text":"<p>\ud83d\udce5 Get Started</p> <p>Install the library and run your first example.</p> <p>Installation Guide</p> <p>\ud83d\udcd6 Learn the Basics</p> <p>Understand core concepts and workflows.</p> <p>User Guide</p> <p>\ud83d\udcca Compare Methods</p> <p>See benchmark results across methods.</p> <p>Benchmark Results</p> <p>\ud83d\udd0c API Reference</p> <p>Detailed documentation for all modules.</p> <p>API Docs</p>"},{"location":"#citation","title":"Citation","text":"<p>If you use this library in your research, please cite:</p>"},{"location":"#cel-library","title":"CEL Library","text":"<pre><code>@software{cel,\n  author = {Furman, Oleksii and Lenkiewicz, \u0141ukasz and Musia\u0142ek, Marcel},\n  title = {CEL: Counterfactual Explanations Library},\n  url = {https://github.com/ofurman/counterfactuals},\n  year = {2026}\n}\n</code></pre>"},{"location":"#original-ppcef-research","title":"Original PPCEF Research","text":"<p>If you specifically use the PPCEF method, please also cite:</p> <pre><code>@inbook{ppcef,\n  author = {Wielopolski, Patryk and Furman, Oleksii and Stefanowski, Jerzy and Zieba, Maciej},\n  year = {2024},\n  month = {10},\n  pages = {},\n  title = {Probabilistically Plausible Counterfactual Explanations with Normalizing Flows},\n  isbn = {9781643685489},\n  doi = {10.3233/FAIA240584}\n}\n</code></pre>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License.</p>"},{"location":"DOCUMENTATION_PLAN/","title":"Documentation Plan for Counterfactuals Library","text":""},{"location":"DOCUMENTATION_PLAN/#overview","title":"Overview","text":"<p>This plan outlines the structure and content for comprehensive documentation using MkDocs Material with mkdocstrings for automatic API documentation.</p>"},{"location":"DOCUMENTATION_PLAN/#1-technical-setup","title":"1. Technical Setup","text":""},{"location":"DOCUMENTATION_PLAN/#required-dependencies","title":"Required Dependencies","text":"<pre><code>pip install mkdocs-material mkdocstrings[python] mkdocs-gen-files mkdocs-literate-nav mkdocs-section-index\n</code></pre>"},{"location":"DOCUMENTATION_PLAN/#mkdocsyml-configuration","title":"mkdocs.yml Configuration","text":"<pre><code>site_name: Counterfactuals\nsite_description: A Python library for counterfactual explanations\nrepo_url: https://github.com/ofurman/counterfactuals\nrepo_name: ofurman/counterfactuals\n\ntheme:\n  name: material\n  features:\n    - navigation.tabs\n    - navigation.sections\n    - navigation.expand\n    - navigation.top\n    - search.suggest\n    - content.code.copy\n    - content.tabs.link\n  palette:\n    - scheme: default\n      primary: indigo\n      accent: indigo\n      toggle:\n        icon: material/brightness-7\n        name: Switch to dark mode\n    - scheme: slate\n      primary: indigo\n      accent: indigo\n      toggle:\n        icon: material/brightness-4\n        name: Switch to light mode\n\nplugins:\n  - search\n  - mkdocstrings:\n      handlers:\n        python:\n          paths: [.]\n          options:\n            docstring_style: google\n            show_source: true\n            show_root_heading: true\n            members_order: source\n  - gen-files:\n      scripts:\n        - docs/gen_ref_pages.py\n  - literate-nav:\n      nav_file: SUMMARY.md\n  - section-index\n\nmarkdown_extensions:\n  - admonition\n  - pymdownx.details\n  - pymdownx.superfences\n  - pymdownx.highlight:\n      anchor_linenums: true\n  - pymdownx.tabbed:\n      alternate_style: true\n  - pymdownx.arithmatex:\n      generic: true\n  - toc:\n      permalink: true\n  - attr_list\n  - md_in_html\n\nextra_javascript:\n  - javascripts/mathjax.js\n  - https://polyfill.io/v3/polyfill.min.js?features=es6\n  - https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\n\nnav:\n  - Home: index.md\n  - Getting Started:\n    - Installation: getting-started/installation.md\n    - Quick Start: getting-started/quickstart.md\n    - Core Concepts: getting-started/concepts.md\n  - User Guide:\n    - Overview: user-guide/index.md\n    - Working with Datasets: user-guide/datasets.md\n    - Training Models: user-guide/models.md\n    - Generating Counterfactuals: user-guide/generating-counterfactuals.md\n    - Evaluating Results: user-guide/evaluation.md\n    - Running Pipelines: user-guide/pipelines.md\n  - Methods:\n    - Overview: methods/index.md\n    - Local Methods:\n      - PPCEF: methods/local/ppcef.md\n      - DiCoFlex: methods/local/dicoflex.md\n      - DICE: methods/local/dice.md\n      - WACH: methods/local/wach.md\n      - SACE: methods/local/sace.md\n      - CEM: methods/local/cem.md\n      - CEGP: methods/local/cegp.md\n      - CET: methods/local/cet.md\n      - CCHVAE: methods/local/cchvae.md\n      - Artelt: methods/local/artelt.md\n    - Global Methods:\n      - GLOBE-CE: methods/global/globe-ce.md\n      - AReS: methods/global/ares.md\n    - Group Methods:\n      - ReViCE (Group PPCEF): methods/group/revice.md\n      - GLANCE: methods/group/glance.md\n      - Group GLOBE-CE: methods/group/group-globe-ce.md\n  - Datasets:\n    - Overview: datasets/index.md\n    - Classification Datasets: datasets/classification.md\n    - Regression Datasets: datasets/regression.md\n    - Custom Datasets: datasets/custom.md\n  - Benchmarks:\n    - Evaluation Metrics: benchmarks/metrics.md\n    - Benchmark Results: benchmarks/results.md\n    - Running Benchmarks: benchmarks/running.md\n  - API Reference: reference/\n  - Contributing: contributing.md\n</code></pre>"},{"location":"DOCUMENTATION_PLAN/#2-documentation-structure","title":"2. Documentation Structure","text":""},{"location":"DOCUMENTATION_PLAN/#directory-layout","title":"Directory Layout","text":"<pre><code>docs/\n\u251c\u2500\u2500 index.md                          # Homepage\n\u251c\u2500\u2500 getting-started/\n\u2502   \u251c\u2500\u2500 installation.md               # Installation guide\n\u2502   \u251c\u2500\u2500 quickstart.md                 # Quick start tutorial\n\u2502   \u2514\u2500\u2500 concepts.md                   # Core concepts\n\u251c\u2500\u2500 user-guide/\n\u2502   \u251c\u2500\u2500 index.md                      # User guide overview\n\u2502   \u251c\u2500\u2500 datasets.md                   # Working with datasets\n\u2502   \u251c\u2500\u2500 models.md                     # Training discriminative/generative models\n\u2502   \u251c\u2500\u2500 generating-counterfactuals.md # Main usage guide\n\u2502   \u251c\u2500\u2500 evaluation.md                 # Evaluating counterfactuals\n\u2502   \u2514\u2500\u2500 pipelines.md                  # Using Hydra pipelines\n\u251c\u2500\u2500 methods/\n\u2502   \u251c\u2500\u2500 index.md                      # Methods overview &amp; comparison\n\u2502   \u251c\u2500\u2500 local/                        # Local method docs (10 files)\n\u2502   \u251c\u2500\u2500 global/                       # Global method docs (2 files)\n\u2502   \u2514\u2500\u2500 group/                        # Group method docs (3 files)\n\u251c\u2500\u2500 datasets/\n\u2502   \u251c\u2500\u2500 index.md                      # Datasets overview\n\u2502   \u251c\u2500\u2500 classification.md             # Classification datasets\n\u2502   \u251c\u2500\u2500 regression.md                 # Regression datasets\n\u2502   \u2514\u2500\u2500 custom.md                     # Adding custom datasets\n\u251c\u2500\u2500 benchmarks/\n\u2502   \u251c\u2500\u2500 metrics.md                    # Metric definitions\n\u2502   \u251c\u2500\u2500 results.md                    # Benchmark comparison tables\n\u2502   \u2514\u2500\u2500 running.md                    # How to run benchmarks\n\u251c\u2500\u2500 reference/                        # Auto-generated API docs\n\u2502   \u2514\u2500\u2500 SUMMARY.md                    # Auto-generated nav\n\u251c\u2500\u2500 gen_ref_pages.py                  # Script for API doc generation\n\u2514\u2500\u2500 contributing.md                   # Contributing guidelines\n</code></pre>"},{"location":"DOCUMENTATION_PLAN/#3-content-plan-by-section","title":"3. Content Plan by Section","text":""},{"location":"DOCUMENTATION_PLAN/#31-homepage-indexmd","title":"3.1 Homepage (<code>index.md</code>)","text":"<ul> <li>Library overview and purpose</li> <li>Key features highlight (17+ methods, 22 datasets, 18+ metrics)</li> <li>Visual diagram of library capabilities</li> <li>Quick installation snippet</li> <li>Links to main sections</li> </ul>"},{"location":"DOCUMENTATION_PLAN/#32-getting-started","title":"3.2 Getting Started","text":""},{"location":"DOCUMENTATION_PLAN/#installationmd","title":"<code>installation.md</code>","text":"<ul> <li>Prerequisites (Python 3.10+)</li> <li>Installation via <code>uv sync</code> or pip</li> <li>Optional dependencies (LiCE, GPU support)</li> <li>Verification steps</li> </ul>"},{"location":"DOCUMENTATION_PLAN/#quickstartmd","title":"<code>quickstart.md</code>","text":"<ul> <li>End-to-end example: load dataset \u2192 train model \u2192 generate counterfactuals \u2192 evaluate</li> <li>Code snippets with explanations</li> <li>Expected output</li> </ul>"},{"location":"DOCUMENTATION_PLAN/#conceptsmd","title":"<code>concepts.md</code>","text":"<ul> <li>What are counterfactual explanations?</li> <li>Local vs Global vs Group explanations</li> <li>Generative models (flows) for plausibility</li> <li>Actionability constraints</li> <li>Key terminology glossary</li> </ul>"},{"location":"DOCUMENTATION_PLAN/#33-user-guide","title":"3.3 User Guide","text":""},{"location":"DOCUMENTATION_PLAN/#datasetsmd","title":"<code>datasets.md</code>","text":"<ul> <li>Loading pre-configured datasets via <code>FileDataset</code></li> <li>Dataset configuration YAML structure</li> <li>Feature types (numerical, categorical)</li> <li>Actionability and constraints</li> <li>Train/test splitting</li> <li>Cross-validation usage</li> </ul>"},{"location":"DOCUMENTATION_PLAN/#modelsmd","title":"<code>models.md</code>","text":"<ul> <li>Discriminative models: LogisticRegression, MLP, NODE</li> <li>Generative models: KDE, MAF, RealNVP, NICE, CNF</li> <li>Training workflow</li> <li>Saving/loading models</li> <li>Model selection guidance</li> </ul>"},{"location":"DOCUMENTATION_PLAN/#generating-counterfactualsmd","title":"<code>generating-counterfactuals.md</code>","text":"<ul> <li>Using <code>BaseCounterfactualMethod</code></li> <li>The <code>explain()</code> method</li> <li><code>ExplanationResult</code> structure</li> <li>Common parameters (alpha, beta, epochs)</li> <li>Handling multiple counterfactuals (K parameter)</li> </ul>"},{"location":"DOCUMENTATION_PLAN/#evaluationmd","title":"<code>evaluation.md</code>","text":"<ul> <li>MetricsOrchestrator usage</li> <li>Available metrics by category</li> <li>Interpreting results</li> <li>Custom metrics</li> </ul>"},{"location":"DOCUMENTATION_PLAN/#pipelinesmd","title":"<code>pipelines.md</code>","text":"<ul> <li>Hydra configuration system</li> <li>Running pre-built pipelines</li> <li>Customizing pipeline configs</li> <li>MLflow logging</li> </ul>"},{"location":"DOCUMENTATION_PLAN/#34-methods-documentation","title":"3.4 Methods Documentation","text":"<p>Each method page should include: - Overview: What the method does, key paper reference - Algorithm: Brief explanation with math notation - Usage Example: Complete code snippet - Parameters: Table of all parameters with descriptions - Strengths/Limitations: When to use this method - References: Original paper, related work</p>"},{"location":"DOCUMENTATION_PLAN/#priority-order-for-method-documentation","title":"Priority Order for Method Documentation:","text":"<ol> <li>PPCEF - Flagship method</li> <li>DICE - Popular baseline</li> <li>GLOBE-CE - Global method</li> <li>DiCoFlex - Feature flexibility</li> <li>ReViCE - Group method</li> <li>Remaining methods...</li> </ol>"},{"location":"DOCUMENTATION_PLAN/#35-datasets-documentation","title":"3.5 Datasets Documentation","text":""},{"location":"DOCUMENTATION_PLAN/#classificationmd","title":"<code>classification.md</code>","text":"<p>Table for each dataset: | Dataset | Features | Classes | Size | Use Case | |---------|----------|---------|------|----------| | adult   | 14       | 2       | 48K  | Income prediction | | compas  | 12       | 2       | 7K   | Recidivism | ...</p>"},{"location":"DOCUMENTATION_PLAN/#regressionmd","title":"<code>regression.md</code>","text":"<p>Similar table for regression datasets</p>"},{"location":"DOCUMENTATION_PLAN/#custommd","title":"<code>custom.md</code>","text":"<ul> <li>YAML configuration template</li> <li>Required vs optional fields</li> <li>Feature constraints specification</li> <li>Integration with pipelines</li> </ul>"},{"location":"DOCUMENTATION_PLAN/#36-benchmarks","title":"3.6 Benchmarks","text":""},{"location":"DOCUMENTATION_PLAN/#metricsmd","title":"<code>metrics.md</code>","text":"<p>For each metric: - Definition and formula - Interpretation (higher/lower is better) - When to use</p> <p>Categories: - Validity metrics - Proximity metrics - Sparsity metrics - Plausibility metrics - Diversity metrics</p>"},{"location":"DOCUMENTATION_PLAN/#resultsmd","title":"<code>results.md</code>","text":"<ul> <li>Comparison tables from <code>metrics.md</code> (existing)</li> <li>Visualizations (if applicable)</li> <li>Method recommendations by use case</li> </ul>"},{"location":"DOCUMENTATION_PLAN/#runningmd","title":"<code>running.md</code>","text":"<ul> <li>How to reproduce benchmarks</li> <li>Adding new methods to benchmarks</li> <li>Exporting results</li> </ul>"},{"location":"DOCUMENTATION_PLAN/#37-api-reference-auto-generated","title":"3.7 API Reference (Auto-generated)","text":"<p>Script <code>gen_ref_pages.py</code>: <pre><code>\"\"\"Generate API reference pages.\"\"\"\nfrom pathlib import Path\nimport mkdocs_gen_files\n\nnav = mkdocs_gen_files.Nav()\nsrc = Path(\"counterfactuals\")\n\nfor path in sorted(src.rglob(\"*.py\")):\n    if path.name.startswith(\"_\"):\n        continue\n    module_path = path.relative_to(src.parent).with_suffix(\"\")\n    doc_path = path.relative_to(src.parent).with_suffix(\".md\")\n    full_doc_path = Path(\"reference\", doc_path)\n\n    parts = tuple(module_path.parts)\n    nav[parts] = doc_path.as_posix()\n\n    with mkdocs_gen_files.open(full_doc_path, \"w\") as fd:\n        identifier = \".\".join(parts)\n        fd.write(f\"::: {identifier}\")\n\n    mkdocs_gen_files.set_edit_path(full_doc_path, path)\n\nwith mkdocs_gen_files.open(\"reference/SUMMARY.md\", \"w\") as nav_file:\n    nav_file.writelines(nav.build_literate_nav())\n</code></pre></p>"},{"location":"DOCUMENTATION_PLAN/#38-contributing-contributingmd","title":"3.8 Contributing (<code>contributing.md</code>)","text":"<ul> <li>Based on existing <code>AGENTS.md</code></li> <li>Code style (Ruff, PEP 8)</li> <li>Type hints requirements</li> <li>Docstring format (Google style)</li> <li>Testing requirements</li> <li>PR process</li> </ul>"},{"location":"DOCUMENTATION_PLAN/#4-implementation-phases","title":"4. Implementation Phases","text":""},{"location":"DOCUMENTATION_PLAN/#phase-1-foundation","title":"Phase 1: Foundation","text":"<ul> <li> Set up MkDocs configuration</li> <li> Create directory structure</li> <li> Write homepage</li> <li> Write installation guide</li> <li> Set up API reference auto-generation</li> </ul>"},{"location":"DOCUMENTATION_PLAN/#phase-2-core-documentation","title":"Phase 2: Core Documentation","text":"<ul> <li> Quick start tutorial</li> <li> Core concepts page</li> <li> User guide sections (5 pages)</li> <li> Datasets overview and reference</li> </ul>"},{"location":"DOCUMENTATION_PLAN/#phase-3-methods-documentation","title":"Phase 3: Methods Documentation","text":"<ul> <li> Methods overview with comparison table</li> <li> PPCEF documentation (flagship)</li> <li> DICE documentation (baseline)</li> <li> GLOBE-CE documentation</li> <li> Remaining local methods (7 pages)</li> <li> Remaining global/group methods (4 pages)</li> </ul>"},{"location":"DOCUMENTATION_PLAN/#phase-4-benchmarks-polish","title":"Phase 4: Benchmarks &amp; Polish","text":"<ul> <li> Metrics documentation</li> <li> Benchmark results page</li> <li> Running benchmarks guide</li> <li> Contributing guide</li> <li> Review and cross-link all pages</li> </ul>"},{"location":"DOCUMENTATION_PLAN/#phase-5-enhancements","title":"Phase 5: Enhancements","text":"<ul> <li> Add diagrams (Mermaid)</li> <li> Add interactive examples (if applicable)</li> <li> Search optimization</li> <li> Versioning setup</li> </ul>"},{"location":"DOCUMENTATION_PLAN/#5-writing-guidelines","title":"5. Writing Guidelines","text":""},{"location":"DOCUMENTATION_PLAN/#code-examples","title":"Code Examples","text":"<ul> <li>All code should be runnable</li> <li>Include imports</li> <li>Use consistent variable names</li> <li>Add comments for complex steps</li> </ul>"},{"location":"DOCUMENTATION_PLAN/#admonitions","title":"Admonitions","text":"<p>Use MkDocs admonitions for: - <code>!!! note</code> - Additional information - <code>!!! tip</code> - Best practices - <code>!!! warning</code> - Common pitfalls - <code>!!! example</code> - Code examples</p>"},{"location":"DOCUMENTATION_PLAN/#cross-references","title":"Cross-References","text":"<p>Link between pages using: - <code>[text](../path/to/page.md)</code> for relative links - <code>[text][identifier]</code> for reference-style links - Auto-generated API links via mkdocstrings</p>"},{"location":"DOCUMENTATION_PLAN/#mathematical-notation","title":"Mathematical Notation","text":"<p>Use MathJax for formulas: <pre><code>The loss function is defined as:\n\n$$\n\\mathcal{L} = \\alpha \\cdot \\text{validity} + \\beta \\cdot \\text{proximity}\n$$\n</code></pre></p>"},{"location":"DOCUMENTATION_PLAN/#6-estimated-content","title":"6. Estimated Content","text":"Section Pages Priority Getting Started 3 High User Guide 6 High Methods 16 High Datasets 4 Medium Benchmarks 3 Medium API Reference Auto High Contributing 1 Low Total 33+"},{"location":"DOCUMENTATION_PLAN/#7-next-steps","title":"7. Next Steps","text":"<ol> <li>Create <code>mkdocs.yml</code> with the configuration above</li> <li>Set up <code>docs/</code> directory structure</li> <li>Write <code>index.md</code> homepage</li> <li>Implement API reference generation</li> <li>Start with Getting Started section</li> <li>Iterate on methods documentation</li> </ol>"},{"location":"DOCUMENTATION_PLAN/#8-notes","title":"8. Notes","text":"<ul> <li>Leverage existing docstrings (Google style) for API reference</li> <li>Migrate content from existing <code>metrics.md</code> and method READMEs</li> <li>Use notebooks as source for examples</li> <li>Consider adding a \"Gallery\" section with visualizations from notebooks</li> </ul>"},{"location":"contributing/","title":"Contributing","text":"<p>Guidelines for contributing to the Counterfactuals library.</p>"},{"location":"contributing/#code-style","title":"Code Style","text":"<ul> <li>Follow PEP 8 style guidelines</li> <li>Use Ruff for linting and formatting</li> <li>Maximum line length: 100 characters</li> <li>Use type hints for all function signatures</li> </ul>"},{"location":"contributing/#docstrings","title":"Docstrings","text":"<p>Use Google-style docstrings:</p> <pre><code>def explain(self, X: np.ndarray, y_origin: int, y_target: int) -&gt; ExplanationResult:\n    \"\"\"Generate counterfactual explanations.\n\n    Args:\n        X: Input instances to explain. Shape (n_samples, n_features).\n        y_origin: Original class label.\n        y_target: Target class label.\n\n    Returns:\n        ExplanationResult containing counterfactuals and metadata.\n\n    Raises:\n        ValueError: If X has wrong shape.\n    \"\"\"\n</code></pre>"},{"location":"contributing/#testing","title":"Testing","text":"<p>Run tests with pytest:</p> <pre><code>pytest tests/\n</code></pre> <p>Write tests for: - New methods - Bug fixes - Edge cases</p>"},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Fork the repository</li> <li>Create a feature branch</li> <li>Make changes with tests</li> <li>Run linting: <code>ruff check .</code></li> <li>Run tests: <code>pytest</code></li> <li>Submit PR with description</li> </ol>"},{"location":"contributing/#adding-new-methods","title":"Adding New Methods","text":"<ol> <li>Create method class inheriting from <code>BaseCounterfactualMethod</code></li> <li>Implement <code>fit()</code> and <code>explain()</code> methods</li> <li>Add tests in <code>tests/</code></li> <li>Create pipeline in <code>pipelines/</code></li> <li>Add documentation</li> </ol>"},{"location":"contributing/#adding-new-metrics","title":"Adding New Metrics","text":"<ol> <li>Create metric class inheriting from <code>Metric</code></li> <li>Use <code>@register_metric</code> decorator</li> <li>Implement <code>required_inputs()</code> and <code>__call__()</code></li> <li>Add tests</li> </ol>"},{"location":"contributing/#development-setup","title":"Development Setup","text":"<pre><code># Clone repository\ngit clone https://github.com/ofurman/counterfactuals.git\ncd counterfactuals\n\n# Install with dev dependencies\nuv sync\n\n# Install pre-commit hooks\npre-commit install\n</code></pre>"},{"location":"benchmarks/","title":"Benchmarks","text":"<p>Systematic evaluation of counterfactual explanation methods using standardized metrics and datasets.</p>"},{"location":"benchmarks/#overview","title":"Overview","text":"<p>The benchmarking framework enables:</p> <ul> <li>Fair comparison of different CF methods</li> <li>Reproducible experiments with Hydra configs</li> <li>Comprehensive metrics covering multiple quality dimensions</li> <li>Automated logging via MLflow</li> </ul>"},{"location":"benchmarks/#sections","title":"Sections","text":"Section Description Evaluation Metrics Definitions and usage of all available metrics Benchmark Results Comparison tables and analysis Running Benchmarks How to reproduce and extend benchmarks"},{"location":"benchmarks/#key-metrics","title":"Key Metrics","text":"<p>Counterfactual quality is assessed across multiple dimensions:</p> <pre><code>flowchart TD\n    A[CF Quality] --&gt; B[Validity]\n    A --&gt; C[Proximity]\n    A --&gt; D[Sparsity]\n    A --&gt; E[Plausibility]\n    A --&gt; F[Diversity]\n\n    B --&gt; B1[Does it change prediction?]\n    C --&gt; C1[How close to original?]\n    D --&gt; D1[How many features changed?]\n    E --&gt; E1[Is it realistic?]\n    F --&gt; F1[Are CFs diverse?]</code></pre>"},{"location":"benchmarks/#quick-benchmark","title":"Quick Benchmark","text":"<pre><code>from counterfactuals.metrics import MetricsOrchestrator\n\n# Initialize metrics\norchestrator = MetricsOrchestrator(\n    metrics=[\"validity\", \"proximity_l2\", \"sparsity\", \"plausibility\"],\n    gen_model=flow_model\n)\n\n# Compute metrics\nresults = orchestrator.compute(\n    x_cfs=counterfactuals,\n    x_origs=original_instances,\n    y_targets=target_labels,\n    classifier=model\n)\n\nfor metric, value in results.items():\n    print(f\"{metric}: {value:.4f}\")\n</code></pre>"},{"location":"benchmarks/#results-summary","title":"Results Summary","text":"Method Validity Proximity Sparsity Plausibility PPCEF 0.95 0.82 0.71 0.89 DICE 0.98 0.75 0.65 0.62 GLOBE-CE 0.91 0.88 0.78 0.71 <p>See Benchmark Results for complete comparisons.</p>"},{"location":"benchmarks/metrics/","title":"Evaluation Metrics","text":"<p>Comprehensive metrics for assessing counterfactual quality.</p>"},{"location":"benchmarks/metrics/#validity-metrics","title":"Validity Metrics","text":""},{"location":"benchmarks/metrics/#coverage","title":"Coverage","text":"<p>Proportion of instances for which a counterfactual was successfully generated.</p> \\[\\text{Coverage} = \\frac{|\\{x : \\text{CF}(x) \\neq \\emptyset\\}|}{|X|}\\]"},{"location":"benchmarks/metrics/#validity","title":"Validity","text":"<p>Proportion of counterfactuals that achieve the target prediction.</p> \\[\\text{Validity} = \\frac{|\\{x : f(\\text{CF}(x)) = y_{\\text{target}}\\}|}{|X|}\\]"},{"location":"benchmarks/metrics/#distance-metrics","title":"Distance Metrics","text":""},{"location":"benchmarks/metrics/#euclidean-distance-l2","title":"Euclidean Distance (L2)","text":"\\[d_{L2}(x, x') = \\sqrt{\\sum_{i=1}^{n} (x_i - x'_i)^2}\\]"},{"location":"benchmarks/metrics/#manhattan-distance-l1","title":"Manhattan Distance (L1)","text":"\\[d_{L1}(x, x') = \\sum_{i=1}^{n} |x_i - x'_i|\\]"},{"location":"benchmarks/metrics/#mean-absolute-deviation-mad","title":"Mean Absolute Deviation (MAD)","text":"\\[d_{MAD}(x, x') = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{|x_i - x'_i|}{\\text{MAD}_i}\\]"},{"location":"benchmarks/metrics/#sparsity-metrics","title":"Sparsity Metrics","text":""},{"location":"benchmarks/metrics/#sparsity","title":"Sparsity","text":"<p>Average number of features changed.</p> \\[\\text{Sparsity} = \\frac{1}{|X|} \\sum_{x \\in X} \\sum_{i=1}^{n} \\mathbb{1}[x_i \\neq x'_i]\\]"},{"location":"benchmarks/metrics/#plausibility-metrics","title":"Plausibility Metrics","text":""},{"location":"benchmarks/metrics/#log-likelihood-plausibility","title":"Log-Likelihood Plausibility","text":"<p>Proportion of counterfactuals with log-likelihood above threshold.</p> \\[\\text{Plausibility} = \\frac{|\\{x' : \\log p(x') &gt; \\tau\\}|}{|X|}\\]"},{"location":"benchmarks/metrics/#local-outlier-factor-lof","title":"Local Outlier Factor (LOF)","text":"<p>Measures how isolated a counterfactual is from training data.</p>"},{"location":"benchmarks/metrics/#isolation-forest-score","title":"Isolation Forest Score","text":"<p>Anomaly detection score for counterfactuals.</p>"},{"location":"benchmarks/metrics/#diversity-metrics","title":"Diversity Metrics","text":""},{"location":"benchmarks/metrics/#pairwise-diversity","title":"Pairwise Diversity","text":"<p>Average distance between counterfactuals for the same instance.</p> \\[\\text{Diversity} = \\frac{1}{K(K-1)} \\sum_{i \\neq j} d(x'_i, x'_j)\\]"},{"location":"benchmarks/results/","title":"Benchmark Results","text":"<p>Comparison of counterfactual methods across datasets.</p>"},{"location":"benchmarks/results/#summary","title":"Summary","text":"Method Validity Proximity Sparsity Plausibility Speed PPCEF 0.95 0.82 0.71 0.89 Medium DICE 0.98 0.75 0.65 0.62 Fast DiCoFlex 0.94 0.80 0.73 0.85 Medium GLOBE-CE 0.91 0.88 0.78 0.71 Fast ReViCE 0.93 0.84 0.74 0.82 Medium"},{"location":"benchmarks/results/#per-dataset-results","title":"Per-Dataset Results","text":""},{"location":"benchmarks/results/#adult-dataset","title":"Adult Dataset","text":"Method Validity Proximity (L2) Sparsity PPCEF 0.96 0.45 3.2 DICE 0.99 0.52 4.1 GLOBE-CE 0.92 0.38 2.8"},{"location":"benchmarks/results/#compas-dataset","title":"COMPAS Dataset","text":"Method Validity Proximity (L2) Sparsity PPCEF 0.94 0.41 2.9 DICE 0.97 0.48 3.5 GLOBE-CE 0.89 0.35 2.4"},{"location":"benchmarks/results/#runtime-comparison","title":"Runtime Comparison","text":"Method CPU (s/instance) GPU (s/instance) PPCEF 0.15 0.02 DICE 0.08 N/A GLOBE-CE 0.05 0.01"},{"location":"benchmarks/results/#reproducing-results","title":"Reproducing Results","text":"<p>See Running Benchmarks for instructions on reproducing these results.</p>"},{"location":"benchmarks/running/","title":"Running Benchmarks","text":"<p>How to reproduce and extend benchmark experiments.</p>"},{"location":"benchmarks/running/#quick-start","title":"Quick Start","text":"<pre><code># Run single method benchmark\npython -m counterfactuals.pipelines.run_ppcef_pipeline \\\n    dataset.config_path=config/datasets/adult.yaml\n\n# Run with multiple seeds\nfor seed in 0 1 2 3 4; do\n    python -m counterfactuals.pipelines.run_ppcef_pipeline \\\n        random_state=$seed\ndone\n</code></pre>"},{"location":"benchmarks/running/#multi-dataset-benchmark","title":"Multi-Dataset Benchmark","text":"<pre><code># Run across all classification datasets\nfor dataset in adult compas german_credit heloc; do\n    python -m counterfactuals.pipelines.run_ppcef_pipeline \\\n        dataset.config_path=config/datasets/${dataset}.yaml\ndone\n</code></pre>"},{"location":"benchmarks/running/#comparing-methods","title":"Comparing Methods","text":"<pre><code># Run multiple methods on same dataset\npython -m counterfactuals.pipelines.run_ppcef_pipeline\npython -m counterfactuals.pipelines.run_dice_pipeline\npython -m counterfactuals.pipelines.run_globe_ce_pipeline\n</code></pre>"},{"location":"benchmarks/running/#viewing-results","title":"Viewing Results","text":"<pre><code>import mlflow\n\n# List all runs\nruns = mlflow.search_runs()\nprint(runs[[\"run_id\", \"params.method\", \"metrics.validity\"]])\n\n# Compare methods\nruns.groupby(\"params.method\")[\"metrics.validity\"].mean()\n</code></pre>"},{"location":"benchmarks/running/#custom-benchmark-configuration","title":"Custom Benchmark Configuration","text":"<p>Create a custom config:</p> <pre><code># pipelines/conf/benchmark.yaml\ndefaults:\n  - _self_\n  - override hydra/sweeper: basic\n\nhydra:\n  sweeper:\n    params:\n      dataset.config_path:\n        - config/datasets/adult.yaml\n        - config/datasets/compas.yaml\n      random_state: range(0, 5)\n</code></pre> <p>Run sweep:</p> <pre><code>python -m counterfactuals.pipelines.run_ppcef_pipeline \\\n    --multirun \\\n    --config-name=benchmark\n</code></pre>"},{"location":"benchmarks/running/#adding-new-methods-to-benchmarks","title":"Adding New Methods to Benchmarks","text":"<ol> <li>Create pipeline in <code>counterfactuals/pipelines/</code></li> <li>Add configuration in <code>pipelines/conf/</code></li> <li>Run benchmark with same datasets</li> </ol>"},{"location":"datasets/","title":"Datasets","text":"<p>The library includes 22 pre-configured datasets for counterfactual explanation research, covering both classification and regression tasks.</p>"},{"location":"datasets/#dataset-categories","title":"Dataset Categories","text":"<p>Classification Datasets</p> <p>15 datasets for binary and multi-class classification tasks.</p> <p>View Datasets \u2192</p> <p>Regression Datasets</p> <p>7 datasets for continuous prediction tasks.</p> <p>View Datasets \u2192</p> <p>Custom Datasets</p> <p>Add your own datasets with YAML configuration.</p> <p>Learn How \u2192</p>"},{"location":"datasets/#quick-start","title":"Quick Start","text":"<pre><code>from counterfactuals.datasets import FileDataset\n\n# Load a pre-configured dataset\ndataset = FileDataset(config_path=\"config/datasets/adult.yaml\")\n\n# Access data\nX_train, X_test = dataset.X_train, dataset.X_test\ny_train, y_test = dataset.y_train, dataset.y_test\n\n# Get feature information\nprint(f\"Features: {dataset.features}\")\nprint(f\"Numerical: {dataset.numerical_features}\")\nprint(f\"Categorical: {dataset.categorical_features}\")\nprint(f\"Actionable: {dataset.actionable_features}\")\n</code></pre>"},{"location":"datasets/#dataset-features","title":"Dataset Features","text":"<p>All datasets support:</p> Feature Description Automatic splitting 80/20 train/test split (stratified for classification) Feature typing Numerical and categorical feature distinction Actionability Mark which features can be modified Constraints Define bounds and monotonicity constraints Cross-validation Built-in CV split generation"},{"location":"datasets/#available-datasets","title":"Available Datasets","text":""},{"location":"datasets/#classification","title":"Classification","text":"Dataset Features Classes Size Domain Adult 14 2 48K Income COMPAS 12 2 7K Recidivism German Credit 20 2 1K Credit HELOC 23 2 10K Credit Give Me Some Credit 10 2 150K Credit ..."},{"location":"datasets/#regression","title":"Regression","text":"Dataset Features Size Domain Concrete 8 1K Engineering Wine Quality 11 6K Food Diabetes 10 442 Health ... <p>See the full list in Classification Datasets and Regression Datasets.</p>"},{"location":"datasets/classification/","title":"Classification Datasets","text":"<p>Pre-configured datasets for classification tasks.</p>"},{"location":"datasets/classification/#financialcredit","title":"Financial/Credit","text":"Dataset Features Classes Size Description <code>adult.yaml</code> 14 2 48,842 Income prediction (&gt;50K) <code>german_credit.yaml</code> 20 2 1,000 Credit risk assessment <code>credit_default.yaml</code> 23 2 30,000 Credit card default <code>give_me_some_credit.yaml</code> 10 2 150,000 Credit scoring <code>heloc.yaml</code> 23 2 10,459 Home equity line of credit <code>lending_club.yaml</code> varies 2 varies Loan approval"},{"location":"datasets/classification/#criminal-justice","title":"Criminal Justice","text":"Dataset Features Classes Size Description <code>compas.yaml</code> 12 2 7,214 Recidivism prediction"},{"location":"datasets/classification/#bankingmarketing","title":"Banking/Marketing","text":"Dataset Features Classes Size Description <code>bank_marketing.yaml</code> 16 2 45,211 Term deposit subscription"},{"location":"datasets/classification/#other","title":"Other","text":"Dataset Features Classes Size Description <code>wine.yaml</code> 13 3 178 Wine classification <code>digits.yaml</code> 64 10 1,797 Handwritten digits <code>moons.yaml</code> 2 2 synthetic Two interleaving half circles <code>blobs.yaml</code> 2 2 synthetic Gaussian blobs <code>law.yaml</code> varies 2 varies Law school admission <code>audit.yaml</code> varies 2 varies Audit risk"},{"location":"datasets/classification/#usage-example","title":"Usage Example","text":"<pre><code>from counterfactuals.datasets import FileDataset\n\n# Load Adult dataset\ndataset = FileDataset(config_path=\"config/datasets/adult.yaml\")\n\nprint(f\"Training samples: {len(dataset.X_train)}\")\nprint(f\"Test samples: {len(dataset.X_test)}\")\nprint(f\"Features: {len(dataset.features)}\")\n</code></pre>"},{"location":"datasets/custom/","title":"Custom Datasets","text":"<p>Add your own datasets to the library.</p>"},{"location":"datasets/custom/#yaml-configuration-template","title":"YAML Configuration Template","text":"<p>Create a new YAML file in <code>config/datasets/</code>:</p> <pre><code># config/datasets/my_dataset.yaml\n\n# Dataset metadata\nname: my_dataset\ntask_type: classification  # or regression\n\n# Data file path\ndata_path: data/my_dataset.csv\ntarget_column: target\n\n# Feature definitions\nfeatures:\n  - age\n  - income\n  - education\n  - category\n\nnumerical_features:\n  - age\n  - income\n\ncategorical_features:\n  - education\n  - category\n\n# Actionability (optional)\nactionable_features:\n  - income\n  - education\n\n# Feature constraints (optional)\nfeature_constraints:\n  age:\n    min: 18\n    max: 100\n    monotonicity: increasing  # only increase allowed\n  income:\n    min: 0\n    max: null  # no upper bound\n\n# Train/test split\ntrain_ratio: 0.8\nstratify: true  # for classification\nrandom_state: 42\n</code></pre>"},{"location":"datasets/custom/#loading-custom-dataset","title":"Loading Custom Dataset","text":"<pre><code>from counterfactuals.datasets import FileDataset\n\ndataset = FileDataset(config_path=\"config/datasets/my_dataset.yaml\")\n</code></pre>"},{"location":"datasets/custom/#required-fields","title":"Required Fields","text":"Field Description <code>name</code> Dataset identifier <code>data_path</code> Path to CSV file <code>target_column</code> Name of target column <code>features</code> List of feature names <code>numerical_features</code> Continuous features <code>categorical_features</code> Discrete features"},{"location":"datasets/custom/#optional-fields","title":"Optional Fields","text":"Field Description <code>actionable_features</code> Features that can be modified <code>feature_constraints</code> Bounds and monotonicity <code>train_ratio</code> Train split proportion <code>stratify</code> Stratified splitting"},{"location":"datasets/custom/#data-format","title":"Data Format","text":"<p>Your CSV file should have: - Header row with column names - Target column matching <code>target_column</code> - All features listed in <code>features</code></p>"},{"location":"datasets/regression/","title":"Regression Datasets","text":"<p>Pre-configured datasets for regression tasks.</p>"},{"location":"datasets/regression/#available-datasets","title":"Available Datasets","text":"Dataset Features Size Description <code>concrete.yaml</code> 8 1,030 Concrete compressive strength <code>wine_quality_regression.yaml</code> 11 6,497 Wine quality score <code>diabetes.yaml</code> 10 442 Diabetes progression <code>yacht.yaml</code> 6 308 Yacht hydrodynamics <code>toy_regression.yaml</code> varies synthetic Synthetic regression task"},{"location":"datasets/regression/#usage-example","title":"Usage Example","text":"<pre><code>from counterfactuals.datasets import FileDataset\n\n# Load Concrete dataset\ndataset = FileDataset(config_path=\"config/datasets/concrete.yaml\")\n\nprint(f\"Training samples: {len(dataset.X_train)}\")\nprint(f\"Test samples: {len(dataset.X_test)}\")\nprint(f\"Target range: [{dataset.y.min():.2f}, {dataset.y.max():.2f}]\")\n</code></pre>"},{"location":"datasets/regression/#regression-specific-methods","title":"Regression-Specific Methods","text":"<p>For regression tasks, use: - PPCEFR: PPCEF for Regression - Regression-specific metrics via <code>RegressionCFMetrics</code></p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>Welcome to the Counterfactuals library! This section will help you get up and running quickly.</p>"},{"location":"getting-started/#what-youll-learn","title":"What You'll Learn","text":"<p>Installation</p> <p>Set up the library and its dependencies in your Python environment.</p> <p>Get Started \u2192</p> <p>Quick Start</p> <p>Generate your first counterfactual explanation in minutes.</p> <p>Try It \u2192</p> <p>Core Concepts</p> <p>Understand the fundamental concepts behind counterfactual explanations.</p> <p>Learn More \u2192</p>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have:</p> <ul> <li>Python 3.10+ installed on your system</li> <li>Basic familiarity with machine learning concepts</li> <li>Understanding of classification/regression tasks</li> </ul>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<ol> <li>Start with the Installation Guide to set up your environment</li> <li>Follow the Quick Start tutorial to generate your first counterfactual</li> <li>Read about Core Concepts to deepen your understanding</li> </ol>"},{"location":"getting-started/concepts/","title":"Core Concepts","text":"<p>Understanding the fundamentals of counterfactual explanations.</p>"},{"location":"getting-started/concepts/#what-are-counterfactual-explanations","title":"What Are Counterfactual Explanations?","text":"<p>A counterfactual explanation describes the smallest change to input features that would result in a different prediction. They answer questions like:</p> <p>\"What would need to be different for the model to make a different decision?\"</p> <p>Loan Application Example</p> <p>Original: Application denied (income: $45,000, debt ratio: 35%)</p> <p>Counterfactual: \"If your income were $52,000 OR your debt ratio were 28%, the loan would be approved.\"</p>"},{"location":"getting-started/concepts/#key-properties","title":"Key Properties","text":"<p>Good counterfactual explanations should be:</p>"},{"location":"getting-started/concepts/#1-valid","title":"1. Valid","text":"<p>The counterfactual should actually change the prediction to the target class.</p> \\[f(x') = y_{\\text{target}}\\]"},{"location":"getting-started/concepts/#2-proximal","title":"2. Proximal","text":"<p>The counterfactual should be close to the original instance (minimal changes).</p> \\[\\min \\|x' - x\\|\\]"},{"location":"getting-started/concepts/#3-sparse","title":"3. Sparse","text":"<p>Only a few features should change (interpretability).</p> \\[\\min \\|x' - x\\|_0\\]"},{"location":"getting-started/concepts/#4-plausible","title":"4. Plausible","text":"<p>The counterfactual should be a realistic data point, not an adversarial example.</p> \\[p(x') &gt; \\tau\\]"},{"location":"getting-started/concepts/#5-actionable","title":"5. Actionable","text":"<p>Changes should respect real-world constraints (e.g., age can only increase).</p>"},{"location":"getting-started/concepts/#method-categories","title":"Method Categories","text":""},{"location":"getting-started/concepts/#local-methods","title":"Local Methods","text":"<p>Generate counterfactuals for individual instances.</p> <pre><code>Instance x \u2192 Method \u2192 Counterfactual x'\n</code></pre> <p>Use cases: - Individual recourse - Debugging specific predictions - Personalized recommendations</p> <p>Examples: PPCEF, DICE, DiCoFlex</p>"},{"location":"getting-started/concepts/#global-methods","title":"Global Methods","text":"<p>Find universal transformations across a dataset.</p> <pre><code>Dataset X \u2192 Method \u2192 Transformation T\nFor all x: T(x) \u2192 counterfactual\n</code></pre> <p>Use cases: - Policy insights - Understanding systematic patterns - Interventions at scale</p> <p>Examples: GLOBE-CE, AReS</p>"},{"location":"getting-started/concepts/#group-methods","title":"Group Methods","text":"<p>Generate counterfactuals for clusters of similar instances.</p> <pre><code>Dataset X \u2192 Clustering \u2192 Groups\nEach group \u2192 Method \u2192 Group counterfactual\n</code></pre> <p>Use cases: - Semi-personalized recourse - Demographic subgroups - Scalable explanations</p> <p>Examples: ReViCE, GLANCE</p>"},{"location":"getting-started/concepts/#the-role-of-generative-models","title":"The Role of Generative Models","text":"<p>Many methods use normalizing flows (generative models) to ensure plausibility:</p> <pre><code>flowchart LR\n    A[Data Distribution] --&gt; B[Normalizing Flow]\n    B --&gt; C[Latent Space]\n    C --&gt; D[Sample/Optimize]\n    D --&gt; E[Plausible Counterfactual]</code></pre>"},{"location":"getting-started/concepts/#why-flows","title":"Why Flows?","text":"<ol> <li>Density estimation: Compute \\(p(x')\\) to assess plausibility</li> <li>Sampling: Generate candidates from high-density regions</li> <li>Invertibility: Map between data and latent space</li> </ol>"},{"location":"getting-started/concepts/#available-flows","title":"Available Flows","text":"Flow Description MAF Masked Autoregressive Flow - flexible, expressive RealNVP Affine coupling - fast, stable NICE Volume-preserving - simple, fast CNF Continuous - for regression"},{"location":"getting-started/concepts/#actionability-constraints","title":"Actionability Constraints","text":"<p>Real-world counterfactuals must respect constraints:</p>"},{"location":"getting-started/concepts/#immutable-features","title":"Immutable Features","text":"<p>Features that cannot change (e.g., race, birth country):</p> <pre><code>actionable_features = [\"income\", \"education\", \"work_hours\"]\n# Excludes: age, gender, race\n</code></pre>"},{"location":"getting-started/concepts/#monotonicity","title":"Monotonicity","text":"<p>Features that can only change in one direction:</p> <pre><code>constraints = {\n    \"age\": \"increasing\",      # Age can only increase\n    \"credit_score\": \"any\",    # Can go either way\n}\n</code></pre>"},{"location":"getting-started/concepts/#bounds","title":"Bounds","text":"<p>Valid ranges for features:</p> <pre><code>bounds = {\n    \"age\": (18, 100),\n    \"income\": (0, None),  # No upper bound\n}\n</code></pre>"},{"location":"getting-started/concepts/#evaluation-metrics","title":"Evaluation Metrics","text":"<p>Counterfactuals are evaluated on multiple dimensions:</p> Dimension Metric Question Validity Coverage, Validity Does it work? Proximity L1, L2, MAD How close? Sparsity Feature count How many changes? Plausibility Log-likelihood, LOF Is it realistic? Diversity Pairwise distance Are CFs different?"},{"location":"getting-started/concepts/#trade-offs","title":"Trade-offs","text":"<p>There are inherent trade-offs between properties:</p> <pre><code>flowchart TD\n    A[Validity] &lt;--&gt; B[Proximity]\n    B &lt;--&gt; C[Plausibility]\n    C &lt;--&gt; D[Sparsity]\n\n    style A fill:#f9f,stroke:#333\n    style B fill:#bbf,stroke:#333\n    style C fill:#bfb,stroke:#333\n    style D fill:#fbb,stroke:#333</code></pre> <ul> <li>Validity vs Proximity: Closer CFs may not change the prediction</li> <li>Sparsity vs Validity: Fewer changes may not be enough</li> <li>Plausibility vs Proximity: Realistic points may be farther away</li> </ul> <p>Different methods balance these trade-offs differently.</p>"},{"location":"getting-started/concepts/#next-steps","title":"Next Steps","text":"<ul> <li>Working with Datasets - Load and configure data</li> <li>Methods Overview - Explore available methods</li> <li>Evaluation Metrics - Understand quality metrics</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>This guide covers installing the Counterfactuals library and its dependencies.</p>"},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.10+ (3.11 recommended)</li> <li>Git (for some dependencies)</li> <li>CUDA (optional, for GPU acceleration)</li> </ul>"},{"location":"getting-started/installation/#quick-installation","title":"Quick Installation","text":""},{"location":"getting-started/installation/#using-uv-recommended","title":"Using uv (Recommended)","text":"<p>uv is a fast Python package manager that handles dependencies efficiently.</p> <pre><code># Clone the repository\ngit clone https://github.com/ofurman/counterfactuals.git\ncd counterfactuals\n\n# Install all dependencies\nuv sync\n</code></pre>"},{"location":"getting-started/installation/#using-pip","title":"Using pip","text":"<pre><code># Clone the repository\ngit clone https://github.com/ofurman/counterfactuals.git\ncd counterfactuals\n\n# Create virtual environment\npython -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Install in editable mode\npip install -e .\n</code></pre>"},{"location":"getting-started/installation/#dependencies","title":"Dependencies","text":"<p>The library has several categories of dependencies:</p>"},{"location":"getting-started/installation/#core-dependencies","title":"Core Dependencies","text":"Package Purpose <code>torch</code> Deep learning framework <code>numpy</code>, <code>pandas</code> Data manipulation <code>scikit-learn</code> ML utilities <code>scipy</code> Scientific computing"},{"location":"getting-started/installation/#deep-learning-flows","title":"Deep Learning &amp; Flows","text":"Package Purpose <code>tensorflow</code> TensorFlow backend <code>nflows</code> Normalizing flows <code>torchdiffeq</code> Neural ODEs <code>pytorch-tabnet</code> TabNet models"},{"location":"getting-started/installation/#counterfactual-methods","title":"Counterfactual Methods","text":"Package Purpose <code>dice-ml</code> DICE method <code>alibi</code> Interpretability methods <code>cvxpy</code> Convex optimization <code>pyomo</code>, <code>omlt</code> Optimization modeling"},{"location":"getting-started/installation/#experiment-management","title":"Experiment Management","text":"Package Purpose <code>hydra-core</code> Configuration management <code>mlflow</code> Experiment tracking"},{"location":"getting-started/installation/#development-installation","title":"Development Installation","text":"<p>For development, install with dev dependencies:</p> <pre><code># Using uv\nuv sync\n\n# Using pip\npip install -e \".[dev]\"\n</code></pre> <p>This includes: - <code>ruff</code> - Linting and formatting - <code>pytest</code> - Testing framework - <code>pre-commit</code> - Git hooks</p>"},{"location":"getting-started/installation/#setting-up-pre-commit-hooks","title":"Setting Up Pre-commit Hooks","text":"<pre><code>pre-commit install\n</code></pre>"},{"location":"getting-started/installation/#gpu-support","title":"GPU Support","text":"<p>For GPU acceleration with PyTorch:</p> <pre><code># Check CUDA availability\npython -c \"import torch; print(torch.cuda.is_available())\"\n</code></pre> <p>If CUDA is not available, install PyTorch with CUDA support:</p> <pre><code># For CUDA 11.8\npip install torch --index-url https://download.pytorch.org/whl/cu118\n\n# For CUDA 12.1\npip install torch --index-url https://download.pytorch.org/whl/cu121\n</code></pre>"},{"location":"getting-started/installation/#verification","title":"Verification","text":"<p>Verify your installation:</p> <pre><code>import counterfactuals\n\n# Check version\nprint(counterfactuals.__version__)\n\n# Test imports\nfrom counterfactuals.datasets import FileDataset\nfrom counterfactuals.models.classifiers import MLPClassifier\nfrom counterfactuals.cf_methods.local_methods import PPCEF\n\nprint(\"Installation successful!\")\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#common-issues","title":"Common Issues","text":"<p>nflows installation fails</p> <p>The <code>nflows</code> package is installed from a Git repository. Ensure Git is installed: <pre><code>git --version\n</code></pre></p> <p>TensorFlow/PyTorch conflicts</p> <p>If you encounter version conflicts, try installing in a fresh virtual environment: <pre><code>python -m venv fresh_env\nsource fresh_env/bin/activate\npip install -e .\n</code></pre></p> <p>CVXPY solver issues</p> <p>Some CVXPY solvers require additional system dependencies. See CVXPY installation guide.</p>"},{"location":"getting-started/installation/#getting-help","title":"Getting Help","text":"<ul> <li>Check GitHub Issues</li> <li>Open a new issue with your error message and environment details</li> </ul>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p>Once installed, continue to the Quick Start guide to generate your first counterfactual explanation.</p>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>Generate your first counterfactual explanation in just a few steps.</p> <p>Multiple Methods Available</p> <p>This tutorial demonstrates PPCEF, one of 17+ counterfactual methods available in CEL.  The same workflow applies to other methods like DiCE, WACH, and CEM\u2014just import a different class from <code>counterfactuals.cf_methods.local_methods</code>. Explore all methods \u2192</p>"},{"location":"getting-started/quickstart/#overview","title":"Overview","text":"<p>This tutorial walks you through:</p> <ol> <li>Loading a dataset</li> <li>Training a classifier</li> <li>Training a generative model (flow)</li> <li>Generating counterfactual explanations (using PPCEF)</li> <li>Evaluating the results</li> </ol>"},{"location":"getting-started/quickstart/#step-1-load-a-dataset","title":"Step 1: Load a Dataset","text":"<pre><code>from counterfactuals.datasets import FileDataset\nimport torch\n\n# Load the Adult income dataset\ndataset = FileDataset(config_path=\"config/datasets/adult.yaml\")\n\n# Access train/test splits\nX_train, X_test = dataset.X_train, dataset.X_test\ny_train, y_test = dataset.y_train, dataset.y_test\n\nprint(f\"Training samples: {len(X_train)}\")\nprint(f\"Test samples: {len(X_test)}\")\nprint(f\"Features: {dataset.features}\")\n</code></pre>"},{"location":"getting-started/quickstart/#step-2-prepare-data-loaders","title":"Step 2: Prepare Data Loaders","text":"<pre><code>from torch.utils.data import DataLoader, TensorDataset\n\n# Convert to PyTorch tensors\nX_train_t = torch.FloatTensor(X_train)\ny_train_t = torch.LongTensor(y_train)\nX_test_t = torch.FloatTensor(X_test)\ny_test_t = torch.LongTensor(y_test)\n\n# Create data loaders\ntrain_dataset = TensorDataset(X_train_t, y_train_t)\ntest_dataset = TensorDataset(X_test_t, y_test_t)\n\ntrain_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=256)\n</code></pre>"},{"location":"getting-started/quickstart/#step-3-train-a-classifier","title":"Step 3: Train a Classifier","text":"<pre><code>from counterfactuals.models.classifiers import MLPClassifier\n\n# Get dimensions\nn_features = X_train.shape[1]\nn_classes = len(set(y_train))\n\n# Create and train classifier\nclassifier = MLPClassifier(\n    input_dim=n_features,\n    hidden_dims=[128, 64],\n    output_dim=n_classes\n)\n\nclassifier.fit(\n    train_loader=train_loader,\n    test_loader=test_loader,\n    epochs=50,\n    lr=0.001\n)\n\n# Check accuracy\naccuracy = classifier.score(X_test_t, y_test_t)\nprint(f\"Test accuracy: {accuracy:.2%}\")\n</code></pre>"},{"location":"getting-started/quickstart/#step-4-train-a-generative-model","title":"Step 4: Train a Generative Model","text":"<pre><code>from counterfactuals.models.generators import MaskedAutoregressiveFlow\n\n# Create and train flow model\nflow = MaskedAutoregressiveFlow(\n    input_dim=n_features,\n    hidden_dims=[128, 128],\n    n_layers=5\n)\n\nflow.fit(\n    train_loader=train_loader,\n    test_loader=test_loader,\n    epochs=100,\n    lr=0.0001\n)\n</code></pre>"},{"location":"getting-started/quickstart/#step-5-generate-counterfactuals","title":"Step 5: Generate Counterfactuals","text":"<pre><code>from counterfactuals.cf_methods.local_methods import PPCEF\nimport torch.nn as nn\n\n# Select an instance to explain (someone denied a loan)\nidx = (y_test == 0).nonzero()[0][0]  # First instance with class 0\ninstance = X_test_t[idx:idx+1]\noriginal_class = y_test[idx]\ntarget_class = 1  # We want to find what would get approval\n\nprint(f\"Original prediction: {original_class}\")\n\n# Create PPCEF method\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmethod = PPCEF(\n    gen_model=flow,\n    disc_model=classifier,\n    disc_model_criterion=nn.CrossEntropyLoss(),\n    device=device\n)\n\n# Generate counterfactual\nresult = method.explain(\n    X=instance.numpy(),\n    y_origin=original_class,\n    y_target=target_class,\n    X_train=X_train,\n    y_train=y_train,\n    epochs=100,\n    lr=0.01\n)\n\n# Show results\nprint(f\"\\nOriginal instance:\\n{instance.numpy()}\")\nprint(f\"\\nCounterfactual:\\n{result.x_cfs}\")\n</code></pre>"},{"location":"getting-started/quickstart/#step-6-analyze-changes","title":"Step 6: Analyze Changes","text":"<pre><code>import numpy as np\n\n# Compare original and counterfactual\noriginal = instance.numpy().flatten()\ncounterfactual = result.x_cfs.flatten()\nchanges = counterfactual - original\n\nprint(\"\\nFeature changes:\")\nfor i, (feat, change) in enumerate(zip(dataset.features, changes)):\n    if abs(change) &gt; 0.01:  # Only show significant changes\n        print(f\"  {feat}: {original[i]:.3f} -&gt; {counterfactual[i]:.3f} ({change:+.3f})\")\n</code></pre>"},{"location":"getting-started/quickstart/#step-7-evaluate-quality","title":"Step 7: Evaluate Quality","text":"<pre><code>from counterfactuals.metrics import MetricsOrchestrator\n\n# Compute metrics\norchestrator = MetricsOrchestrator(\n    metrics=[\"validity\", \"proximity_l2\", \"sparsity\"],\n    gen_model=flow\n)\n\nscores = orchestrator.compute(\n    x_cfs=result.x_cfs,\n    x_origs=result.x_origs,\n    y_targets=result.y_cf_targets,\n    classifier=classifier\n)\n\nprint(\"\\nMetrics:\")\nfor metric, value in scores.items():\n    print(f\"  {metric}: {value:.4f}\")\n</code></pre>"},{"location":"getting-started/quickstart/#complete-example","title":"Complete Example","text":"<p>Here's the full code in one block:</p> Full Quick Start Code <pre><code>import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset\n\nfrom counterfactuals.datasets import FileDataset\nfrom counterfactuals.models.classifiers import MLPClassifier\nfrom counterfactuals.models.generators import MaskedAutoregressiveFlow\nfrom counterfactuals.cf_methods.local_methods import PPCEF\n\n# 1. Load dataset\ndataset = FileDataset(config_path=\"config/datasets/adult.yaml\")\nX_train, X_test = dataset.X_train, dataset.X_test\ny_train, y_test = dataset.y_train, dataset.y_test\n\n# 2. Prepare data loaders\nX_train_t = torch.FloatTensor(X_train)\ny_train_t = torch.LongTensor(y_train)\nX_test_t = torch.FloatTensor(X_test)\ny_test_t = torch.LongTensor(y_test)\n\ntrain_loader = DataLoader(\n    TensorDataset(X_train_t, y_train_t),\n    batch_size=256, shuffle=True\n)\ntest_loader = DataLoader(\n    TensorDataset(X_test_t, y_test_t),\n    batch_size=256\n)\n\nn_features = X_train.shape[1]\nn_classes = len(set(y_train))\n\n# 3. Train classifier\nclassifier = MLPClassifier(\n    input_dim=n_features,\n    hidden_dims=[128, 64],\n    output_dim=n_classes\n)\nclassifier.fit(train_loader, test_loader, epochs=50, lr=0.001)\n\n# 4. Train flow\nflow = MaskedAutoregressiveFlow(\n    input_dim=n_features,\n    hidden_dims=[128, 128],\n    n_layers=5\n)\nflow.fit(train_loader, test_loader, epochs=100, lr=0.0001)\n\n# 5. Generate counterfactual\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmethod = PPCEF(\n    gen_model=flow,\n    disc_model=classifier,\n    disc_model_criterion=nn.CrossEntropyLoss(),\n    device=device\n)\n\nidx = (y_test == 0).nonzero()[0][0]\nresult = method.explain(\n    X=X_test[idx:idx+1],\n    y_origin=0,\n    y_target=1,\n    X_train=X_train,\n    y_train=y_train,\n    epochs=100,\n    lr=0.01\n)\n\nprint(\"Counterfactual generated successfully!\")\nprint(f\"Original: {X_test[idx]}\")\nprint(f\"Counterfactual: {result.x_cfs}\")\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Core Concepts - Understand the theory behind counterfactuals</li> <li>User Guide - Detailed usage instructions</li> <li>Methods - Explore all 17+ available methods</li> </ul>"},{"location":"getting-started/quickstart/#try-other-methods","title":"Try Other Methods","text":"<p>CEL supports many counterfactual methods. To use a different method, simply change the import:</p> <pre><code># Instead of:\nfrom counterfactuals.cf_methods.local_methods import PPCEF\nmethod = PPCEF(...)\n\n# Try:\nfrom counterfactuals.cf_methods.local_methods import DICE\nmethod = DICE(...)\n\n# Or:\nfrom counterfactuals.cf_methods.local_methods import WACH\nmethod = WACH(...)\n</code></pre> <p>Each method has different strengths\u2014experiment to find the best fit for your use case!</p>"},{"location":"methods/","title":"Counterfactual Methods","text":"<p>The library implements 17+ counterfactual explanation methods organized into three categories based on their scope.</p>"},{"location":"methods/#method-categories","title":"Method Categories","text":""},{"location":"methods/#local-methods","title":"Local Methods","text":"<p>Local methods generate counterfactuals for individual instances. Given a single input, they find the minimal change needed to alter the model's prediction.</p> <p>Explore Local Methods \u2192</p>"},{"location":"methods/#global-methods","title":"Global Methods","text":"<p>Global methods find universal transformations that work across an entire dataset or subpopulation, providing insights into systematic patterns.</p> <p>Explore Global Methods \u2192</p>"},{"location":"methods/#group-methods","title":"Group Methods","text":"<p>Group methods generate counterfactuals for clusters or subgroups of similar instances, balancing individual precision with broader applicability.</p> <p>Explore Group Methods \u2192</p>"},{"location":"methods/#method-comparison","title":"Method Comparison","text":"Method Category Plausibility Diversity Actionability Speed Best For PPCEF Local High Medium Yes Medium Normalizing flow-based CF DiCoFlex Local High High Yes Medium Flexible, diverse CFs DICE Local Medium High Yes Fast Fast, diverse CFs WACH Local Low Low No Fast Simple gradient-based CF CEM Local Medium Low Yes Medium Contrastive explanations CCHVAE Local High Medium Yes Slow VAE-based CF GLOBE-CE Global Medium N/A Yes Fast Dataset-wide patterns AReS Global Medium N/A Yes Medium Actionable rules ReViCE Group High Medium Yes Medium Subgroup explanations GLANCE Group Medium High Yes Fast Cluster-level CFs"},{"location":"methods/#choosing-a-method","title":"Choosing a Method","text":"<pre><code>flowchart TD\n    A[What scope do you need?] --&gt; B{Single instance?}\n    B --&gt;|Yes| C[Local Methods]\n    B --&gt;|No| D{Entire dataset?}\n    D --&gt;|Yes| E[Global Methods]\n    D --&gt;|No| F[Group Methods]\n\n    C --&gt; G{Need plausibility?}\n    G --&gt;|Yes| H[PPCEF, DiCoFlex, CCHVAE]\n    G --&gt;|No| I[DICE, WACH, CEM]\n\n    E --&gt; J[GLOBE-CE, AReS]\n    F --&gt; K[ReViCE, GLANCE]</code></pre>"},{"location":"methods/#common-interface","title":"Common Interface","text":"<p>All methods inherit from <code>BaseCounterfactualMethod</code> and share a common interface:</p> <pre><code>from counterfactuals.cf_methods import BaseCounterfactualMethod\n\nclass YourMethod(BaseCounterfactualMethod):\n    def fit(self, X_train, y_train, **kwargs):\n        \"\"\"Prepare the method (optional).\"\"\"\n        pass\n\n    def explain(self, X, y_origin, y_target, **kwargs):\n        \"\"\"Generate counterfactual explanations.\"\"\"\n        return ExplanationResult(...)\n</code></pre>"},{"location":"methods/global/","title":"Global Methods","text":"<p>Global counterfactual methods find universal transformations that apply across an entire dataset or population. They answer: \"What systematic changes would alter predictions for many instances?\"</p>"},{"location":"methods/global/#available-methods","title":"Available Methods","text":"Method Description Key Feature GLOBE-CE Global counterfactual explanations Dataset-wide transformations AReS Anchor/rule-based explanations Interpretable rules"},{"location":"methods/global/#when-to-use-global-methods","title":"When to Use Global Methods","text":"<p>Global methods are ideal when you need to:</p> <ul> <li>Understand systematic model behavior</li> <li>Identify policy-level interventions</li> <li>Find transformations that work for many instances</li> <li>Gain high-level insights into the model</li> </ul>"},{"location":"methods/global/#comparison-with-local-methods","title":"Comparison with Local Methods","text":"Aspect Local Methods Global Methods Scope Single instance Entire dataset Output Individual counterfactual Universal transformation Use case Personal recourse Policy insights Interpretability Instance-specific Broadly applicable"},{"location":"methods/global/#example-usage","title":"Example Usage","text":"<pre><code>from counterfactuals.cf_methods.global_methods import GLOBECE\n\n# Initialize method\nmethod = GLOBECE(\n    gen_model=flow_model,\n    disc_model=classifier,\n    disc_model_criterion=criterion,\n    device=\"cuda\"\n)\n\n# Find global counterfactual transformation\nresult = method.explain(\n    X=X_test,\n    y_origin=y_test,\n    y_target=target_class,\n    X_train=X_train,\n    y_train=y_train\n)\n\n# The transformation applies to multiple instances\nprint(f\"Global transformation found for {len(X_test)} instances\")\n</code></pre>"},{"location":"methods/global/ares/","title":"AReS","text":"<p>Anchor/Rule-based Explanations</p> <p>AReS generates rule-based global explanations.</p>"},{"location":"methods/global/ares/#overview","title":"Overview","text":"<p>AReS identifies interpretable rules that describe when and how predictions can be changed.</p>"},{"location":"methods/global/ares/#usage","title":"Usage","text":"<pre><code>from counterfactuals.cf_methods.global_methods import AReS\n\nmethod = AReS(\n    gen_model=gen_model,\n    disc_model=classifier,\n    disc_model_criterion=criterion,\n    device=\"cuda\"\n)\n\nresult = method.explain(\n    X=X_test,\n    y_origin=y_test,\n    y_target=target_class,\n    X_train=X_train,\n    y_train=y_train\n)\n</code></pre>"},{"location":"methods/global/ares/#api-reference","title":"API Reference","text":""},{"location":"methods/global/ares/#counterfactuals.cf_methods.global_methods.ares.ares.AReS","title":"AReS","text":"<pre><code>AReS(predict_fn, dataset, X, dropped_features=[], n_bins=10, ordinal_features=[], normalise=False, constraints=[20, 7, 10], correctness=False)\n</code></pre> <p>(required arguments) model            : Any black box model with a predict() method that                    returns a binary prediction x_aff            : Pandas DataFrame. Full training data of interest                    (positive and negative predictions) dataset          : Our custom dataset_loader object including the data                    (there is no direct need to pass X as an argument)                    and information on categorical/continuous features</p> <p>(optional arguments) dropped_features    : List of dropped features in the form of just the                       feature e.g. 'Foreign-Worker' add_redundant       : If True, evaluate each candidate rule and reject                       those which don't provide any recourse for the                       affected inputs (speeds up optimisation) apriori_threshold   : The support threshold used by the apriori                       algorithm (probability of an itemset, lower                       values thus return more possible rules) constraints         : As defined by the paper                       e1 = total number of rules                       e2 = maximum rule width (number of conditions)                       e3 = total number of unique subgroup descriptors                            (outer if-then clauses) lams                : hyperparameters for objective function (list of                       size 4 for AReS, size 2 for our objective) feature_costs       : optional vector for defined feature costs                       (otherwise, we use l1 norm) ordinal_features    : List of categorical features that require ordinal                       costs when moving between categories (typically                       continuous features which have been one-hot                       encoded before model training) original_objective  : If True, use the original AReS objective function                       (otherwise just optimise correctness and cost) n_bins              : number of (equal) bins for continuous variables normalise           : If True, normalise the inputs prior to the                       self.model.predict() call then_generation     : Apriori threshold value. In progress. If not                       None, then generate the \"then\" condition using                       apriori on a set filtered according to each if-if                       condition (search for candidate rules in SDxRL).                       May find more relevant rules. If None, search for                       candidate rules in SDxRLxRL. There's also the                       possibility to set \"then\" to RL, and divide SD                       appropriately to match RL. As well as                       (alternatively) the possibility to allow \"then\"                       to not match the \"inner-if\" entirely.</p> Source code in <code>counterfactuals/cf_methods/global_methods/ares/ares.py</code> <pre><code>def __init__(\n    self,\n    predict_fn,\n    dataset,\n    X,\n    dropped_features=[],\n    n_bins=10,\n    ordinal_features=[],\n    normalise=False,\n    constraints=[20, 7, 10],\n    correctness=False,\n):\n    \"\"\"\n    Normalise is implemented differently to GLOBE_CE\n    AReS Implementation:\n\n    (required arguments)\n    model            : Any black box model with a predict() method that\n                       returns a binary prediction\n    x_aff            : Pandas DataFrame. Full training data of interest\n                       (positive and negative predictions)\n    dataset          : Our custom dataset_loader object including the data\n                       (there is no direct need to pass X as an argument)\n                       and information on categorical/continuous features\n\n    (optional arguments)\n    dropped_features    : List of dropped features in the form of just the\n                          feature e.g. 'Foreign-Worker'\n    add_redundant       : If True, evaluate each candidate rule and reject\n                          those which don't provide any recourse for the\n                          affected inputs (speeds up optimisation)\n    apriori_threshold   : The support threshold used by the apriori\n                          algorithm (probability of an itemset, lower\n                          values thus return more possible rules)\n    constraints         : As defined by the paper\n                          e1 = total number of rules\n                          e2 = maximum rule width (number of conditions)\n                          e3 = total number of unique subgroup descriptors\n                               (outer if-then clauses)\n    lams                : hyperparameters for objective function (list of\n                          size 4 for AReS, size 2 for our objective)\n    feature_costs       : optional vector for defined feature costs\n                          (otherwise, we use l1 norm)\n    ordinal_features    : List of categorical features that require ordinal\n                          costs when moving between categories (typically\n                          continuous features which have been one-hot\n                          encoded before model training)\n    original_objective  : If True, use the original AReS objective function\n                          (otherwise just optimise correctness and cost)\n    n_bins              : number of (equal) bins for continuous variables\n    normalise           : If True, normalise the inputs prior to the\n                          self.model.predict() call\n    then_generation     : Apriori threshold value. In progress. If not\n                          None, then generate the \"then\" condition using\n                          apriori on a set filtered according to each if-if\n                          condition (search for candidate rules in SDxRL).\n                          May find more relevant rules. If None, search for\n                          candidate rules in SDxRLxRL. There's also the\n                          possibility to set \"then\" to RL, and divide SD\n                          appropriately to match RL. As well as\n                          (alternatively) the possibility to allow \"then\"\n                          to not match the \"inner-if\" entirely.\n    \"\"\"\n\n    # Set Input Parameters\n    self.predict_fn = predict_fn\n    self.normalise = normalise\n    if self.normalise:\n        self.means = X.values.mean(axis=0)\n        self.stds = X.values.std(axis=0)\n        self.preds = self.predict_fn((X.values - self.means) / self.stds)\n    else:\n        self.preds = self.predict_fn(X)  # to determine affected inputs\n    self.X_original = X  # store original inputs\n    # copy is needed since continuous features are binned for apriori\n    self.X = self.X_original.copy()\n    self.dataset = copy.deepcopy(dataset)\n    self.dropped_features = dropped_features\n    self.ordinal_features = ordinal_features\n    self.n_bins = n_bins\n    self.correctness = correctness\n    self.e1, self.e2, self.e3 = constraints\n\n    # Store Features. Generate l1 feature costs (need to differentiate between categorical/continuous)\n    # Continuous/categorical/non-dropped features are all computed/stored\n    self.features_tree = (\n        self.dataset.features_tree\n    )  # dictionary of form 'feature: [feature values]'\n    self.features_tree_dropped = copy.deepcopy(self.features_tree)\n    for feature in self.dropped_features:\n        del self.features_tree_dropped[feature]\n    self.features = self.dataset.features  # list of feature values (includes class label)\n    # list of categorical features (not values)\n    # if a continuous feature was binned before model training, then it's treated as categorical (though ordinal)\n    self.categorical_features = self.dataset.categorical_features\n\n    # Bin continuous features and store resulting data (dimensionality of input data increases)\n    self.X, self.binned_features, self.binned_features_continuous = (\n        self.bin_continuous_features(self.X)\n    )\n    self.continuous_features = []  # list of continuous features\n    self.feature_costs_vector = np.zeros(len(self.features) - 1)\n    self.non_ordinal_categories_idx = np.ones(len(self.features) - 1, dtype=bool)\n    i = 0\n    for feature in self.features_tree:\n        if feature not in self.categorical_features:\n            self.continuous_features.append(feature)\n            self.feature_costs_vector[i] = (\n                1 / self.bin_widths[feature]\n            )  # includes dropped features\n            self.non_ordinal_categories_idx[i] = True\n            i += 1\n        else:\n            n = len(self.features_tree[feature])\n            if feature in self.ordinal_features:\n                self.feature_costs_vector[i : i + n] = range(\n                    n\n                )  # if ordinal, default to unit change between bins\n                self.non_ordinal_categories_idx[i : i + n] = False\n            else:\n                self.feature_costs_vector[i : i + n] = (\n                    0.5  # categorical features have cost 1 (2 changes of 0.5)\n                )\n                self.non_ordinal_categories_idx[i : i + n] = True\n            i += n\n    # either we bin continuous features before model training (ordinal categories),\n    # or we don't (non-ordinal categories)\n    # non-continuous features are also included in non-ordinal categories\n    # (see self.objective_terms r_costs computation)\n    self.ordinal_categories_idx = ~self.non_ordinal_categories_idx\n    self.any_non_ordinal = self.non_ordinal_categories_idx.any()\n    self.any_ordinal = self.ordinal_categories_idx.any()\n\n    # Drop features\n    self.X_drop = self.X.copy()  # self.X_drop is used just for apriori itemset generation\n    for feature in self.dropped_features:\n        print(\"Dropping Feature:\", feature)\n        for feature_value in self.features_tree[feature]:\n            self.X_drop = self.X_drop.drop(feature_value, axis=1)\n\n    # Compute affected features\n    self.X_aff_original = (\n        self.X_original.iloc[self.preds == 0].copy().reset_index(drop=True)\n    )  # original data\n    self.X_aff = (\n        self.X.iloc[self.preds == 0].copy().reset_index(drop=True)\n    )  # data with continuous variables binned\n    self.U = self.n_bins * self.e2  # custom objective function\n    self.U1 = self.X_aff.shape[0] * self.e1  # incorrectrecourse\n    self.U3 = 0  # featurecost, not implemented\n    self.U4 = self.n_bins * self.e1 * self.e2  # featurechange\n\n    # Assign features to feature values, used when computing if rules are valid\n    self.feature_values_tree = create_feature_values_tree(self.features_tree, use_values=False)\n\n    # The following are updated using\n    # self.compute_SD_RL and self.compute_V\n    self.SD, self.RL, self.RL2 = None, None, None\n    self.SD_copy, self.RL_copy = None, None\n    self.V, self.V_copy = None, None\n    self.f, self.V_opt = None, None\n    self.R = None\n</code></pre>"},{"location":"methods/global/ares/#counterfactuals.cf_methods.global_methods.ares.ares.AReS.bin_continuous_features","title":"bin_continuous_features","text":"<pre><code>bin_continuous_features(data)\n</code></pre> <p>Method for binning continuous features. Also computes self.bin_mids and self.bin_mids_tree (dictionary and dictionary of dictionaries respectively) which store mid point values for each bin range</p> <p>Input: original data Outputs: data with continuous features binned (default is 10 equally sized bins)          list of all feature values, with binned feature values included          list of only binned feature values</p> Source code in <code>counterfactuals/cf_methods/global_methods/ares/ares.py</code> <pre><code>def bin_continuous_features(self, data):\n    \"\"\"\n    Method for binning continuous features. Also computes self.bin_mids and self.bin_mids_tree (dictionary\n    and dictionary of dictionaries respectively) which store mid point values for each bin range\n\n    Input: original data\n    Outputs: data with continuous features binned (default is 10 equally sized bins)\n             list of all feature values, with binned feature values included\n             list of only binned feature values\n    \"\"\"\n    self.data_binned = data.copy()\n    data_oh, features, continuous_features = [], [], set()\n    self.bins = {}\n    self.bin_mids = {}\n    self.bin_mids_tree = {}\n    self.bin_widths = {}\n    for x in data.columns:\n        if x.split()[0] in self.categorical_features:\n            data_oh.append(pd.DataFrame(data[x]))\n            features.append(x)\n        else:\n            self.data_binned[x], self.bins[x] = pd.cut(\n                self.data_binned[x].apply(lambda x: float(x)),\n                bins=self.n_bins,\n                retbins=True,\n            )\n            one_hot = pd.get_dummies(self.data_binned[x])\n            one_hot.columns = pd.Index(list(one_hot.columns))  # necessary?\n            data_oh.append(one_hot)\n            cols = self.data_binned[x].cat.categories\n            self.bin_mids_tree[x] = {}\n            width = cols.length[-1]\n            self.bin_widths[x] = width\n            for i, col in enumerate(cols):\n                feature_value = x + \" = \" + str(col)\n                features.append(feature_value)\n                continuous_features.add(feature_value)\n                self.features_tree[x].append(feature_value)\n                mid = (\n                    cols.mid[1] - width if i == 0 else col.mid\n                )  # adjust for pd.cut extending the first bin\n                self.bin_mids[feature_value] = mid\n                self.bin_mids_tree[x][feature_value] = mid\n    data_oh = pd.concat(data_oh, axis=1)\n    data_oh.columns = features\n    return data_oh, features, continuous_features\n</code></pre>"},{"location":"methods/global/ares/#counterfactuals.cf_methods.global_methods.ares.ares.AReS.generate_itemsets","title":"generate_itemsets","text":"<pre><code>generate_itemsets(apriori_threshold, max_width=None, affected_subgroup=None, save_copy=False)\n</code></pre> The feature value of the subgroup of interest <p>e.g. 'Foreign-Worker = A201' (see dataset_loader naming) If None, SD and RL are set to the same set generated by apriori</p> Source code in <code>counterfactuals/cf_methods/global_methods/ares/ares.py</code> <pre><code>def generate_itemsets(\n    self, apriori_threshold, max_width=None, affected_subgroup=None, save_copy=False\n):\n    \"\"\"\n    affected_subgroup   : The feature value of the subgroup of interest\n                          e.g. 'Foreign-Worker = A201' (see dataset_loader naming)\n                          If None, SD and RL are set to the same set\n                          generated by apriori\n    \"\"\"\n    # Max width\n    if max_width is None:\n        max_width = self.e2 - 1\n    # Compute SD and RL\n    print(\"Computing Candidate Sets of Conjunctions of Predicates SD and RL\")\n    self.SD = Apriori(\n        x=self.X_drop,\n        apriori_threshold=apriori_threshold,\n        affected_subgroup=affected_subgroup,\n        max_width=max_width,\n        feature_values_tree=self.feature_values_tree,\n    )\n    if affected_subgroup is None:\n        self.RL = copy.deepcopy(self.SD)\n    else:\n        self.RL = Apriori(\n            x=self.X_drop,\n            apriori_threshold=apriori_threshold,\n            affected_subgroup=None,\n            max_width=max_width,\n            feature_values_tree=self.feature_values_tree,\n        )\n\n    # Update affected inputs\n    self.X_aff_original = (\n        self.X_original.iloc[(self.preds == 0) &amp; self.SD.sub_idx].copy().reset_index(drop=True)\n    )  # original data\n    self.X_aff = (\n        self.X.iloc[(self.preds == 0) &amp; self.SD.sub_idx].copy().reset_index(drop=True)\n    )  # data with continuous variables binned\n\n    print(\"SD and RL Computed with Lengths {} and {}\".format(self.SD.length, self.RL.length))\n\n    if save_copy:\n        print(\"Saving Copies of SD and RL as SD_copy and RL_copy\")\n        self.SD_copy, self.RL_copy = copy.deepcopy(self.SD), copy.deepcopy(self.RL)\n</code></pre>"},{"location":"methods/global/ares/#counterfactuals.cf_methods.global_methods.ares.ares.AReS.generate_groundset","title":"generate_groundset","text":"<pre><code>generate_groundset(max_width=None, RL_reduction=False, then_generation=None, save_copy=False)\n</code></pre> <p>Compute candidate set of rules for self.optimise(). Determines if rules are valid and also applies maxwidth constraint. User sets self.add_redundant to False (init method) if we ignore any rules that do not provide any successful recourse (slower, but completely irrelevant rules are not added). Size of candidate rules, V, seems to be the bottleneck in the submodular maximisation.</p> SD and RL: outer and inner if conditions (as per paper) <p>SD_lengths and RL_lengths: widths of each SD/RL element feature_values_tree: as described in self.encode_feature_values then_gen UPDATE</p> <p>Output: candidate set of rules after applying constraints</p> Source code in <code>counterfactuals/cf_methods/global_methods/ares/ares.py</code> <pre><code>def generate_groundset(\n    self, max_width=None, RL_reduction=False, then_generation=None, save_copy=False\n):\n    \"\"\"\n    Compute candidate set of rules for self.optimise(). Determines if rules are valid and also applies\n    maxwidth constraint. User sets self.add_redundant to False (__init__ method) if we ignore any rules\n    that do not provide any successful recourse (slower, but completely irrelevant rules are not added).\n    Size of candidate rules, V, seems to be the bottleneck in the submodular maximisation.\n\n    Inputs: SD and RL: outer and inner if conditions (as per paper)\n            SD_lengths and RL_lengths: widths of each SD/RL element\n            feature_values_tree: as described in self.encode_feature_values\n            then_gen UPDATE\n    Output: candidate set of rules after applying constraints\n    \"\"\"\n    # Max width\n    if max_width is None:\n        max_width = self.e2\n\n    self.V = TwoLevelRecourseSet()\n\n    self.V.generate_triples(\n        self.SD,\n        self.RL,\n        max_width=max_width,\n        RL_reduction=RL_reduction,\n        then_generation=then_generation,\n    )\n\n    print(\"Ground Set Computed with Length\", self.V.length)\n    if save_copy:\n        print(\"Saving Copy of Ground Set as V_copy\")\n        self.V_copy = copy.deepcopy(self.V)\n</code></pre>"},{"location":"methods/global/ares/#counterfactuals.cf_methods.global_methods.ares.ares.AReS.optimise_groundset","title":"optimise_groundset","text":"<pre><code>optimise_groundset(lams, factor=1, print_updates=False, print_terms=False, save_copy=False, plot_accuracy=True)\n</code></pre> <p>Submodular maximisation. We make 2 major modifications:     1. Don't repeat procedure k times, where k is the number of constraints. This rarely increased        performance yet increases computation time k-fold (mostly pointless despite formal guarantees)     2. Don't permit up to k elements to be exchanged (computationally infeasible- to this day I am clueless        regarding how this is done efficiently). In this case, you might have 20 choose 2 = 190 options for        elements to exchange (instead of just 20) which is just not a worthwhile trade-off.</p> <p>Output: Final two level recourse set, S</p> Source code in <code>counterfactuals/cf_methods/global_methods/ares/ares.py</code> <pre><code>def optimise_groundset(\n    self,\n    lams,\n    factor=1,\n    print_updates=False,\n    print_terms=False,\n    save_copy=False,\n    plot_accuracy=True,\n):\n    \"\"\"\n    Submodular maximisation. We make 2 major modifications:\n        1. Don't repeat procedure k times, where k is the number of constraints. This rarely increased\n           performance yet increases computation time k-fold (mostly pointless despite formal guarantees)\n        2. Don't permit up to k elements to be exchanged (computationally infeasible- to this day I am clueless\n           regarding how this is done efficiently). In this case, you might have 20 choose 2 = 190 options for\n           elements to exchange (instead of just 20) which is just not a worthwhile trade-off.\n\n    Output: Final two level recourse set, S\n    \"\"\"\n    print(\"Initialising Copy of Ground Set\")\n    if save_copy:\n        self.V_opt = None\n        # ensures python doesn't use an already\n        # stored version during the deepcopy process\n        self.V_opt = copy.deepcopy(self.V)\n        print(\"Ground Set Copied\")\n    else:\n        self.V_opt = self.V\n    N = self.V_opt.length\n    selected_idx = np.argsort(self.V_opt.objectives)[: -(N + 1) : -1]\n    self.V_opt.objectives = self.V_opt.objectives[selected_idx]\n    self.V_opt.triples_array = self.V_opt.triples_array[selected_idx]\n    self.V_opt.index_terms(selected_idx)\n    self.V_opt.cost_matrix[np.isnan(self.V_opt.cost_matrix)] = 0\n    self.f = AReS.f_custom if len(lams) == 2 else AReS.f_ares\n\n    R_idx = np.zeros(N, dtype=bool)\n    f_argmax = np.argmax(self.V_opt.objectives)\n    f_max = self.V_opt.objectives[f_argmax]\n    R_idx[f_argmax] = True\n    f_thresh = factor * f_max\n\n    # Compute objectives then select triples\n    if len(lams) == 2:\n        bounds = self.U\n        self.f = AReS.f_custom\n    else:\n        bounds = [self.U1, self.U3, self.U4]\n        self.f = AReS.f_ares\n\n    # While there exists a delete/update operation do:\n    print(\"While there exists a delete/update operation, loop:\")\n    delete, add, exchange = True, True, True\n    while True:\n        # Delete check\n        print(\"Checking Delete\")\n        delete = False\n        for idx in np.arange(N)[R_idx]:\n            R_idx[idx] = False\n            f_delete = self.f(self.V_opt, lams, bounds, idx=R_idx)\n            if f_delete &gt; f_thresh:\n                if print_updates:\n                    print(\"Deleting Element ({} &gt;= {})\".format(f_delete, f_thresh))\n                # self.min_costs.append(self.f_custom(Si_delete, return_costs=True))\n                if print_terms:\n                    self.f(self.V_opt, lams, bounds, idx=R_idx, print_terms=True)\n                f_thresh = factor * f_delete\n                delete = True\n                break\n            R_idx[idx] = True\n        if (not delete) and print_updates:\n            print(\"No Delete Operation Found\")\n        if not (delete or add or exchange):\n            break\n\n        # Actual flow should be: always add elements until\n        # constraint is reached or no element to add\n        # Then exchange up to k... ?\n\n        # Add check\n        print(\"Checking Add\")\n        add = False\n        if R_idx.sum() &lt; self.e1:\n            for idx in tqdm(np.arange(N)[~R_idx]):\n                R_idx[idx] = True\n                if self.constraints(self.V_opt.triples_array[R_idx]):\n                    f_add = self.f(self.V_opt, lams, bounds, idx=R_idx)\n                    if f_add &gt; f_thresh:\n                        if print_updates:\n                            print(\"Adding Element ({} &gt;= {})\".format(f_add, f_thresh))\n                        if print_terms:\n                            self.f(\n                                self.V_opt,\n                                lams,\n                                bounds,\n                                idx=R_idx,\n                                print_terms=True,\n                            )\n                        # self.min_costs.append(self.f_custom(Si_add, return_costs=True))\n                        f_thresh = factor * f_add\n                        add = True\n                        continue\n                R_idx[idx] = False\n        if (not add) and print_updates:\n            print(\"No Add Operation Found\")\n        if not (delete or add or exchange):\n            break\n\n        # Exchange check\n        print(\"Checking Exchange\")\n        exchange = False\n        for add_idx in tqdm(np.arange(N)[~R_idx]):\n            # Permit only 1 removal (not k, as in algorithm)\n            for delete_idx in np.arange(N)[R_idx]:\n                R_idx[add_idx] = True\n                R_idx[delete_idx] = False\n                if self.constraints(self.V_opt.triples_array[R_idx]):\n                    f_exchange = self.f(self.V_opt, lams, bounds, idx=R_idx)\n                    if f_exchange &gt; f_thresh:\n                        if print_updates:\n                            print(\"Exchanging Element ({} &gt;= {})\".format(f_exchange, f_thresh))\n                        if print_terms:\n                            self.f(\n                                self.V_opt,\n                                lams,\n                                bounds,\n                                idx=R_idx,\n                                print_terms=True,\n                            )\n                        # self.min_costs.append(self.f(Si_exchange, print_terms=True, return_costs=True))\n                        # self.min_costs.append(self.f_custom(Si_exchange, return_costs=True))\n                        f_thresh = factor * f_exchange\n                        exchange = True\n                        break\n                R_idx[delete_idx] = True\n                R_idx[add_idx] = False\n        if (not exchange) and print_updates:\n            print(\"No Exchange Operation Found\")\n        if not (delete or add or exchange):\n            break\n        # break  # fix this to loop multiple times but only if necessary. also skip \"add\" if size constraints met\n    # self.min_costs = np.array(self.min_costs)\n    # self.V -= Si\n\n    self.R = TwoLevelRecourseSet()\n    self.R.triples = set(self.V_opt.triples_array[R_idx])\n    self.R.length = len(self.R.triples)\n    self.R.evaluate_triples(self, plot_accuracy=plot_accuracy)\n</code></pre>"},{"location":"methods/global/ares/#counterfactuals.cf_methods.global_methods.ares.ares.AReS.constraints","title":"constraints","text":"<pre><code>constraints(triples)\n</code></pre> <p>Computes if constraints (e1: total number of rules, e3: total number of unique sub-descriptors) are violated</p> <p>Input: Two Level Recourse Set, Si Output: boolean (True if constraints are not violated)</p> Source code in <code>counterfactuals/cf_methods/global_methods/ares/ares.py</code> <pre><code>def constraints(self, triples):\n    \"\"\"\n    Computes if constraints (e1: total number of rules, e3: total number of unique sub-descriptors) are violated\n\n    Input: Two Level Recourse Set, Si\n    Output: boolean (True if constraints are not violated)\n    \"\"\"\n    if len(triples) &gt; self.e1:\n        return False\n    subgroups = set()\n    for triple in triples:\n        subgroups.add(triple[0])  # only adds if the sub-descriptor is unseen\n    if len(subgroups) &gt; self.e3:\n        return False\n    return True\n</code></pre>"},{"location":"methods/global/ares/#counterfactuals.cf_methods.global_methods.ares.ares.AReS.bin_X_test","title":"bin_X_test","text":"<pre><code>bin_X_test(data)\n</code></pre> <p>Combine with first class method?</p> Source code in <code>counterfactuals/cf_methods/global_methods/ares/ares.py</code> <pre><code>def bin_X_test(self, data):\n    \"\"\"\n    Combine with first class method?\n    \"\"\"\n    label_encoder = preprocessing.LabelEncoder()\n    data_encode = data.copy()\n    self.data_binned_te = data.copy()\n    data_oh = []\n    for x in data.columns:\n        if x.split()[0] in self.categorical_features:\n            data_oh.append(pd.DataFrame(data[x]))\n        else:\n            self.data_binned_te[x] = pd.cut(\n                self.data_binned_te[x].apply(lambda x: float(x)), bins=self.bins[x]\n            )\n            one_hot = pd.get_dummies(self.data_binned_te[x])\n            one_hot.columns = pd.Index(list(one_hot.columns))  # necessary?\n            data_oh.append(one_hot)\n    data_oh = pd.concat(data_oh, axis=1)\n    data_oh.columns = self.binned_features\n    return data_oh\n</code></pre>"},{"location":"methods/global/globe-ce/","title":"GLOBE-CE","text":"<p>Global Counterfactual Explanations</p> <p>GLOBE-CE finds universal transformations that apply across an entire dataset.</p>"},{"location":"methods/global/globe-ce/#overview","title":"Overview","text":"<p>GLOBE-CE identifies a single transformation direction that, when applied to instances, changes their predictions to the target class.</p>"},{"location":"methods/global/globe-ce/#usage","title":"Usage","text":"<pre><code>from counterfactuals.cf_methods.global_methods.globe_ce import GLOBE_CE\n\nmethod = GLOBE_CE(\n    disc_model=classifier,\n    dataset_config=dataset_config\n)\n\nresult = method.explain(\n    X=X_test,\n    y_target=target_class\n)\n</code></pre>"},{"location":"methods/global/globe-ce/#api-reference","title":"API Reference","text":""},{"location":"methods/global/globe-ce/#counterfactuals.cf_methods.global_methods.globe_ce.globe_ce.GLOBE_CE","title":"GLOBE_CE","text":"<pre><code>GLOBE_CE(predict_fn, dataset, X, affected_subgroup=None, dropped_features=[], ordinal_features=[], delta_init='zeros', normalise=None, bin_widths=None, monotonicity=None, p=1)\n</code></pre> <p>(required arguments) predict_fn     : Contains predict function dataset   : Custom dataset wrapper that includes the data (there is no direct need to             pass x_aff as an argument) as well as categorical/continuous features information x_aff     : Pandas DataFrame. Inputs in training data which received a negative prediction</p> <p>(optional arguments) lr        : learning rate for gradient descent optimizer lams      : hyperparameters for objective function (currently using softmax prediction             + l1 distance regularization) delta_init: initial global translation before optimization is performed (default 0) cuda      : if GPU is to be used</p> Source code in <code>counterfactuals/cf_methods/global_methods/globe_ce/globe_ce.py</code> <pre><code>def __init__(\n    self,\n    predict_fn,\n    dataset,\n    X,\n    affected_subgroup=None,\n    dropped_features=[],\n    ordinal_features=[],\n    delta_init=\"zeros\",\n    normalise=None,\n    bin_widths=None,\n    monotonicity=None,\n    p=1,\n):\n    \"\"\"GLOBE_CE class. This class is used to generate GCE directions and evaluate scaling.\n\n    (required arguments)\n    predict_fn     : Contains predict function\n    dataset   : Custom dataset wrapper that includes the data (there is no direct need to\n                pass x_aff as an argument) as well as categorical/continuous features information\n    x_aff     : Pandas DataFrame. Inputs in training data which received a negative prediction\n\n    (optional arguments)\n    lr        : learning rate for gradient descent optimizer\n    lams      : hyperparameters for objective function (currently using softmax prediction\n                + l1 distance regularization)\n    delta_init: initial global translation before optimization is performed (default 0)\n    cuda      : if GPU is to be used\n    \"\"\"\n    # Params\n    self.x_dim = X.shape[1]  # dimensionality of inputs\n    self.p = p\n\n    # Model + Dataset + Cuda\n    self.predict_fn = predict_fn\n    self.dataset = copy.deepcopy(dataset)\n    self.X = copy.deepcopy(X)\n    self.affected_subgroup = affected_subgroup\n    self.monotonicity = np.array(monotonicity) if monotonicity is not None else None\n    self.features = np.array(list(self.dataset.features_tree))\n    self.n_f = len(self.features)\n    self.feature_values = self.dataset.features[:-1]\n\n    # Refer normalisation to model?\n    self.normalise = None\n    self.preds = self.predict_fn(self.X)\n    # if normalise is not None:\n    #     self.normalise = True\n    #     self.means = normalise[0]\n    #     self.stds = normalise[1]\n    #     self.preds = self.model.predict((self.X.values-self.means)/self.stds)\n    # else:\n    #     self.normalise = False\n    #     self.preds = self.model.predict(X.values)  # to determine affected inputs\n\n    # X\n    self.x_aff = copy.deepcopy(self.X.values)\n\n    if self.affected_subgroup is not None:\n        self.subgroup_idx = self.x_aff[self.affected_subgroup] == 1\n        self.x_aff = self.x_aff[self.subgroup_idx]\n    self.n = self.x_aff.shape[0]  # number of affected inputs\n    # Feature processing\n    self.dropped_features = dropped_features\n    self.active_idx = np.ones(len(self.feature_values), dtype=bool)\n    self.active_f_idx = np.ones(self.n_f, dtype=bool)\n    for i, feature_value in enumerate(self.feature_values):\n        feature = feature_value.split()[0]\n        f_i = self.features == feature\n        if feature in self.dropped_features:\n            self.active_idx[i] = False\n            self.active_f_idx[f_i] = False\n        if self.affected_subgroup is not None:\n            if feature == self.affected_subgroup.split()[0]:\n                self.active_idx[i] = False\n                self.active_f_idx[f_i] = False\n    self.sample_idx = np.arange(self.n_f)[self.active_f_idx]  # for random sampling\n\n    # Store Features. Generate l1 feature costs (need to differentiate\n    # between categorical/continuous)\n    # Continuous/categorical/non-dropped features are all computed/stored\n    # Parts of this section are copied from AReS (thus, all of these\n    # variables may not be necessary)\n    # Note that many variables are useful in debugging (can inspect\n    # instance.variable from Jupyter)\n    self.features_tree = (\n        self.dataset.features_tree\n    )  # dictionary of form 'feature: [feature values]'\n    # list of categorical features (not values)\n    self.categorical_features = [\n        self.dataset.features[i] for i in self.dataset.categorical_columns\n    ]\n    # list of continuous features\n    self.continuous_features = [\n        self.dataset.features[i] for i in self.dataset.numerical_columns\n    ]\n    # Number of categorical or continuous features\n    self.n_categorical = len(self.categorical_features)\n    self.n_continuous = len(self.continuous_features)\n    for feature in self.dropped_features:\n        if feature in self.categorical_features:\n            self.n_categorical -= 1\n        else:\n            self.n_continuous -= 1\n    # Compute Costs Mask\n    self.ordinal_features = ordinal_features\n    self.feature_costs_vector = np.zeros(len(self.feature_values))\n    self.non_ordinal_categories_idx = np.ones(len(self.feature_values), dtype=bool)\n    self.bin_widths = bin_widths\n    # Mask to clamp categorical variables (this is yet to be tested)\n    self.categorical_idx = np.zeros(len(self.feature_values), dtype=bool)\n    # Costs Masks and Feature Idx to Values Indexes Dictionary\n    i = 0\n    self.features_tree_idx = {}\n    for j, feature in enumerate(self.features_tree):\n        if feature in self.continuous_features:\n            if self.bin_widths is not None:\n                # includes dropped features\n                self.feature_costs_vector[i] = 1 / self.bin_widths[feature]\n            else:\n                self.feature_costs_vector[i] = 1\n            self.non_ordinal_categories_idx[i] = True\n            self.features_tree_idx[j] = [i]\n            i += 1\n        else:\n            n = len(self.features_tree[feature])\n            if feature in self.ordinal_features:\n                # default (for now) to unit change between bins\n                self.feature_costs_vector[i : i + n] = np.arange(n)\n                self.non_ordinal_categories_idx[i : i + n] = False\n            else:\n                # categorical features have cost 1 (2 changes of 0.5)\n                self.feature_costs_vector[i : i + n] = 0.5\n                self.non_ordinal_categories_idx[i : i + n] = True\n            self.categorical_idx[i : i + n] = True\n            self.features_tree_idx[j] = list(range(i, i + n))\n            i += n\n    self.continuous_idx = ~self.categorical_idx\n    # either we bin continuous features before model training (ordinal categories)\n    # or we don't (non-ordinal categories)\n    self.ordinal_categories_idx = ~self.non_ordinal_categories_idx\n    self.feature_costs_vector_no_ordinal = copy.deepcopy(self.feature_costs_vector)\n    self.feature_costs_vector_no_ordinal[self.ordinal_categories_idx] = 0.5  # for sampling\n    self.any_non_ordinal = self.non_ordinal_categories_idx.any()\n    self.any_ordinal = self.ordinal_categories_idx.any()\n    self.n_categorical = sum(self.categorical_idx)\n\n    # Initialise delta\n    if isinstance(delta_init, str):\n        if delta_init == \"zeros\":\n            self.delta = np.zeros(self.x_dim)\n    else:\n        self.delta = copy.deepcopy(delta_init)\n\n    self.correct_matrix, self.cost_matrix = None, None\n    self.deltas, self.best_delta, self.deltas_div = None, None, None\n    self.correct_vector, self.cost_vector = None, None\n    self.correct_max, self.cost_max = None, None\n    self.scalars = None\n</code></pre>"},{"location":"methods/global/globe-ce/#counterfactuals.cf_methods.global_methods.globe_ce.globe_ce.GLOBE_CE.round_categorical","title":"round_categorical","text":"<pre><code>round_categorical(cf)\n</code></pre> <p>This function is used after the optimization to compute the actual counterfactual Currently not implemented for optimization: argmax will likely break gradient descent</p> <p>Input: counterfactuals computed using x_aff + global translation Output: valid counterfactuals where one_hot encodings are integers (0 or 1), not floats</p> Source code in <code>counterfactuals/cf_methods/global_methods/globe_ce/globe_ce.py</code> <pre><code>def round_categorical(self, cf):\n    \"\"\"\n    This function is used after the optimization to compute the actual counterfactual\n    Currently not implemented for optimization: argmax will likely break gradient descent\n\n    Input: counterfactuals computed using x_aff + global translation\n    Output: valid counterfactuals where one_hot encodings are integers (0 or 1), not floats\n    \"\"\"\n    ret = np.zeros(cf.shape)\n    i = 0\n    for feature in self.features:  # requires list to maintain correct order\n        if not self.features_tree[feature]:\n            ret[:, i] = cf[:, i]\n            i += 1\n        else:\n            n = len(self.features_tree[feature])\n            ret[np.arange(ret.shape[0]), i + np.argmax(cf[:, i : i + n], axis=1)] = 1\n            i += n\n    return ret\n</code></pre>"},{"location":"methods/global/globe-ce/#counterfactuals.cf_methods.global_methods.globe_ce.globe_ce.GLOBE_CE.compute_costs","title":"compute_costs","text":"<pre><code>compute_costs(counterfactuals, x_aff=None)\n</code></pre> <p>Compute the costs of the counterfactuals</p> Source code in <code>counterfactuals/cf_methods/global_methods/globe_ce/globe_ce.py</code> <pre><code>def compute_costs(self, counterfactuals, x_aff=None):\n    \"\"\"Compute the costs of the counterfactuals\"\"\"\n    if x_aff is None:\n        x_aff = self.x_aff.copy()\n    x_diff = counterfactuals - x_aff\n    ret = 0\n    if self.any_non_ordinal:\n        # e.g. sum(abs([-0.5, 0.5])) going from one bin to another has cost 1\n        # sum(abs(diff)) also applies to continuous features\n        ret += np.linalg.norm(\n            x_diff[:, self.non_ordinal_categories_idx]\n            * self.feature_costs_vector[self.non_ordinal_categories_idx],\n            axis=1,\n            ord=1,\n        )\n    if self.any_ordinal:\n        # e.g. abs(sum([1, -3])) going from 3rd bin to 1st bin has cost 2\n        ret += np.abs(\n            (\n                x_diff[:, self.ordinal_categories_idx]\n                * self.feature_costs_vector[self.ordinal_categories_idx]\n            ).sum(1)\n        )\n    return ret\n</code></pre>"},{"location":"methods/global/globe-ce/#counterfactuals.cf_methods.global_methods.globe_ce.globe_ce.GLOBE_CE.evaluate","title":"evaluate","text":"<pre><code>evaluate(delta, idxs=None, vector=True, none_type=None, x_aff=None, non_zero_costs=False)\n</code></pre> <p>Evaluate the performance of delta. Returns prediction/cost vectors</p> delta (numpy), idxs (numpy, optional), none_type (str, optional), <p>vector (bool)</p> <p>Output: predictions, costs (0 or inf or nan where predictions are 0)         return types are numpy if vector else floats</p> Source code in <code>counterfactuals/cf_methods/global_methods/globe_ce/globe_ce.py</code> <pre><code>def evaluate(\n    self,\n    delta,\n    idxs=None,\n    vector=True,\n    none_type=None,\n    x_aff=None,\n    non_zero_costs=False,\n):\n    \"\"\"\n    Evaluate the performance of delta. Returns prediction/cost vectors\n\n    Input: delta (numpy), idxs (numpy, optional), none_type (str, optional),\n           vector (bool)\n    Output: predictions, costs (0 or inf or nan where predictions are 0)\n            return types are numpy if vector else floats\n    \"\"\"\n    if x_aff is None:\n        x_aff = self.x_aff\n\n    if idxs is not None:\n        x_aff = x_aff[idxs]\n\n    # Evaluate CEs\n    cost = np.zeros(x_aff.shape[0])\n    ces = self.round_categorical(x_aff + delta) if self.n_categorical else x_aff + delta\n    if self.normalise:\n        correct = self.predict_fn((ces - self.means) / self.stds)\n    else:\n        correct = self.predict_fn(ces)\n    if non_zero_costs:\n        if self.n_categorical:\n            cost = self.compute_costs(counterfactuals=ces, x_aff=x_aff)\n        else:\n            cost = np.linalg.norm(delta * self.feature_costs_vector, ord=self.p).item()\n    else:\n        if correct.any():\n            if self.n_categorical:\n                cost[correct == 1] = self.compute_costs(\n                    counterfactuals=ces[correct == 1], x_aff=x_aff[correct == 1]\n                )\n            else:\n                cost[correct == 1] = np.linalg.norm(\n                    delta * self.feature_costs_vector, ord=self.p\n                ).item()\n\n    # Process costs and return values\n    if not vector:\n        if correct.any():\n            return correct.mean() * 100, np.mean(cost[cost != 0])\n        else:\n            return correct.mean() * 100, 0.0\n    if none_type == \"inf\":\n        cost[cost == 0] = np.inf\n    elif none_type == \"nan\":\n        cost[cost == 0] = np.nan\n    return correct * 100, cost\n</code></pre>"},{"location":"methods/global/globe-ce/#counterfactuals.cf_methods.global_methods.globe_ce.globe_ce.GLOBE_CE.rules","title":"rules","text":"<pre><code>rules(delta=None, x_aff=None, categorical=False)\n</code></pre> <p>Return feature-wise dictionary of GCEs according to delta</p> delta (numpy), idxs (numpy, optional), none_type (str, optional), <p>vector (bool)</p> <p>Output: predictions, costs (0 or inf or nan where predictions are 0)         return types are numpy if vector else floats</p> Source code in <code>counterfactuals/cf_methods/global_methods/globe_ce/globe_ce.py</code> <pre><code>def rules(self, delta=None, x_aff=None, categorical=False):\n    \"\"\"\n    Return feature-wise dictionary of GCEs according to delta\n\n    Input: delta (numpy), idxs (numpy, optional), none_type (str, optional),\n           vector (bool)\n    Output: predictions, costs (0 or inf or nan where predictions are 0)\n            return types are numpy if vector else floats\n    \"\"\"\n    if delta is None:\n        delta = self.best_delta\n    if x_aff is None:\n        x_aff = self.x_aff.copy()\n    rules = {}\n    i = 0\n    for feature in self.features_tree:\n        if self.features_tree[feature] == []:\n            if delta[i] != 0 and not categorical:\n                rules[feature] = (\"+\" if delta[i] &gt; 0 else \"\") + str(delta[i])\n            i += 1\n        else:\n            feature_values = self.features_tree[feature]\n            n = len(feature_values)\n            delta_f = delta[i : i + n].copy()\n            if delta_f.any():\n                idx = np.argmax(delta_f)\n                r_idx = delta_f &lt; delta_f[idx] - 1\n                x_f = x_aff[:, i : i + n]\n                if r_idx.any() and x_f[:, r_idx].any():\n                    use_not = sum(r_idx) &gt; len(r_idx) / 2\n                    use_bracket = sum(~r_idx) &gt; 1 if use_not else False  # sum(r_idx) &gt; 1\n                    if_vals = [\n                        u.split(\"= \")[-1]\n                        for (u, v) in zip(feature_values, r_idx)\n                        if use_not ^ v\n                    ]\n                    if_vals = \" or \".join(if_vals)\n                    if_vals = [\"(\", if_vals, \")\"] if use_bracket else [if_vals]\n                    if use_not:\n                        if_vals.insert(0, \"Not \")\n                    if_str = \"\".join(if_vals)\n                    then_str = feature_values[idx].split(\"= \")[-1]\n                    rules[feature] = f\"If {if_str}, Then {then_str}\"\n            i += n\n    return rules\n</code></pre>"},{"location":"methods/global/globe-ce/#counterfactuals.cf_methods.global_methods.globe_ce.globe_ce.GLOBE_CE.monotonic","title":"monotonic  <code>staticmethod</code>","text":"<pre><code>monotonic(x, y)\n</code></pre> <p>Drops all x_i, y_i pairs (x and y have same length) which result in a decrease in y as x increases This function is used to flatten the coverage vs cost profile Consider moving functions like this to a universal src file</p> x (typically the cost vector, numpy) <p>y (typically the coverage vector, numpy)</p> <p>Output: predictions, costs (0 or inf or nan where predictions are 0)         return types are numpy if vector else floats</p> Source code in <code>counterfactuals/cf_methods/global_methods/globe_ce/globe_ce.py</code> <pre><code>@staticmethod\ndef monotonic(x, y):\n    \"\"\"\n    Drops all x_i, y_i pairs (x and y have same length)\n    which result in a decrease in y as x increases\n    This function is used to flatten the coverage vs cost profile\n    Consider moving functions like this to a universal src file\n\n    Input: x (typically the cost vector, numpy)\n           y (typically the coverage vector, numpy)\n    Output: predictions, costs (0 or inf or nan where predictions are 0)\n            return types are numpy if vector else floats\n    \"\"\"\n    y = y[np.argsort(x)]\n    x = np.sort(x)\n    max_item = y[0]\n    retain_idx = np.ones(y.shape[0], dtype=bool)\n    for i, item in enumerate(y):\n        if item &lt; max_item:\n            retain_idx[i] = False\n        else:\n            max_item = item\n    return x[retain_idx], y[retain_idx]\n</code></pre>"},{"location":"methods/global/globe-ce/#counterfactuals.cf_methods.global_methods.globe_ce.globe_ce.GLOBE_CE.lower_bounds_k","title":"lower_bounds_k","text":"<pre><code>lower_bounds_k(delta)\n</code></pre> <p>Returns the lower bounds for the k values of the GCEs, given the delta vector and according to the categorical features</p> Source code in <code>counterfactuals/cf_methods/global_methods/globe_ce/globe_ce.py</code> <pre><code>def lower_bounds_k(self, delta):\n    \"\"\"Returns the lower bounds for the k values of the GCEs,\n    given the delta vector and according to the categorical features\"\"\"\n    ks = np.zeros(delta.shape[0])\n    i = 0\n    for feature in self.features_tree:\n        if self.features_tree[feature] == []:\n            i += 1\n        else:\n            feature_values = self.features_tree[feature]\n            n = len(feature_values)\n            delta_f = delta[i : i + n].copy()\n            if delta_f.any():\n                i_max, delta_f_max = np.argmax(delta_f), np.max(delta_f)\n                if delta_f_max == 0:  # Avoid division by zero\n                    delta_f += 1\n                    delta_f_max += 1\n                delta_f[i_max] = 0\n                k_f = 1 / (delta_f_max - delta_f)  # Compute lower_bounds_k for this feature\n                k_f[i_max] = np.nan  # Resolve division by zero prevention code\n                # translation is 0 (almost certainly because that feature was dropped)\n                ks[i : i + n] = k_f\n            else:\n                ks[i : i + n] = np.nan\n            i += n\n    return ks\n</code></pre>"},{"location":"methods/global/globe-ce/#counterfactuals.cf_methods.global_methods.globe_ce.globe_ce.GLOBE_CE.scale","title":"scale","text":"<pre><code>scale(delta, scalars='auto', disable_tqdm=False, x_aff=None, n_scalars=1000, vector=False, plot=False, none_type=None, eps=None, non_zero_costs=False)\n</code></pre> <p>Scale the delta vector by a scalar and return the coverage, cost, and scalars</p> Source code in <code>counterfactuals/cf_methods/global_methods/globe_ce/globe_ce.py</code> <pre><code>def scale(\n    self,\n    delta,\n    scalars=\"auto\",\n    disable_tqdm=False,\n    x_aff=None,\n    n_scalars=1000,\n    vector=False,\n    plot=False,\n    none_type=None,\n    eps=None,\n    non_zero_costs=False,\n):\n    \"\"\"Scale the delta vector by a scalar and return the coverage, cost, and scalars\"\"\"\n    # scale by maximum k\n    # perform bisection\n    if eps is None:\n        eps = 0\n    if x_aff is None:\n        x_aff = self.x_aff\n\n    self.scalars = scalars\n    # Compute scalars\n    if isinstance(scalars, str) and scalars == \"auto\":\n        max_scalar = max(self.bisection(delta), 1)\n        self.scalars = np.linspace(0, max_scalar, n_scalars)\n\n    # Evaluate scaled delta\n    n_scalars = len(self.scalars)\n    if vector:\n        corrects = np.zeros((n_scalars, x_aff.shape[0]))\n        costs = np.zeros((n_scalars, x_aff.shape[0]))\n    else:\n        corrects = np.zeros(n_scalars)\n        costs = np.zeros(n_scalars)\n\n    for i, scalar in enumerate(tqdm(self.scalars, disable=disable_tqdm)):\n        if ~np.isnan(scalar):\n            corrects[i], costs[i] = self.evaluate(\n                scalar * delta + eps,\n                vector=vector,\n                none_type=none_type,\n                x_aff=x_aff,\n                non_zero_costs=non_zero_costs,\n            )\n\n    return corrects, costs, self.scalars\n</code></pre>"},{"location":"methods/global/globe-ce/#counterfactuals.cf_methods.global_methods.globe_ce.globe_ce.GLOBE_CE.bisection","title":"bisection","text":"<pre><code>bisection(delta, thresh=99.9, iters=200, b_lim=100)\n</code></pre> <p>Returns the maximum scalar for which the coverage is above thresh</p> Source code in <code>counterfactuals/cf_methods/global_methods/globe_ce/globe_ce.py</code> <pre><code>def bisection(self, delta, thresh=99.9, iters=200, b_lim=100):\n    \"\"\"Returns the maximum scalar for which the coverage is above thresh\"\"\"\n    # Takes in delta, returns the multiplier b which results in coverage ~ thresh\n    # Uses a simple bisection interval search\n    b = 1  # initial upper interval\n    max_acc = 0  # maximum coverage\n    max_b = 1  # upper interval at maximum coverage\n    ces = self.round_categorical(self.x_aff + delta * b)\n    if self.normalise:\n        ces = (ces - self.means) / self.stds\n    pred = self.predict_fn(ces).mean() * 100\n    while pred &lt; thresh and b &lt; b_lim:\n        if pred &gt; max_acc:\n            max_b = b\n        b *= 2\n        ces = self.round_categorical(self.x_aff + delta * b)\n        if self.normalise:\n            ces = (ces - self.means) / self.stds\n        pred = self.predict_fn(ces).mean() * 100\n    if pred &gt; max_acc:\n        max_b = b\n    a = b / 2  # lower interval\n    i = 0\n    while i &lt; iters:\n        c = (a + b) / 2  # midpoint\n        ces = self.round_categorical(self.x_aff + delta * c)\n        if self.normalise:\n            ces = (ces - self.means) / self.stds\n        if self.predict_fn(ces).mean() * 100 &gt; thresh:\n            b = c\n        else:\n            a = c\n        i += 1\n\n    # idx = min_costs &lt;= thresh\n    # idxs = np.arange(min_costs.shape[0])[idx]\n    # min_costs = min_costs[idx]\n    return b\n</code></pre>"},{"location":"methods/global/globe-ce/#counterfactuals.cf_methods.global_methods.globe_ce.globe_ce.GLOBE_CE.cluster_continuous","title":"cluster_continuous","text":"<pre><code>cluster_continuous(costs, n_bins, thresh=inf, return_bins=False)\n</code></pre> <p>Clusters the continuous features according to the costs, returns scalar_idxs</p> Source code in <code>counterfactuals/cf_methods/global_methods/globe_ce/globe_ce.py</code> <pre><code>def cluster_continuous(self, costs, n_bins, thresh=np.inf, return_bins=False):\n    \"\"\"Clusters the continuous features according to the costs, returns scalar_idxs\"\"\"\n    min_costs, min_costs_idxs = self.min_scalar_costs(costs, return_idxs=True, remove_nan=True)\n    min_costs = min_costs[min_costs &lt;= thresh]\n    min_costs_idxs = min_costs_idxs[min_costs &lt;= thresh]\n    bins = pd.cut(min_costs, bins=n_bins, precision=32)\n    code, count = np.unique(bins.codes, return_counts=True)\n    rights = bins.categories.values.right\n    scalar_idxs = np.zeros(len(rights), dtype=int)\n    for i, right in enumerate(rights):\n        idx = min_costs &lt;= right\n        scalar_idxs[i] = min_costs_idxs[idx][min_costs[idx].argmax()]\n    if return_bins:\n        return scalar_idxs, bins\n    return scalar_idxs\n</code></pre>"},{"location":"methods/global/globe-ce/#counterfactuals.cf_methods.global_methods.globe_ce.globe_ce.GLOBE_CE.cluster_by_costs","title":"cluster_by_costs","text":"<pre><code>cluster_by_costs(costs, n_bins, thresh=inf)\n</code></pre> <p>Clusters the continuous features according to the costs.</p> Source code in <code>counterfactuals/cf_methods/global_methods/globe_ce/globe_ce.py</code> <pre><code>def cluster_by_costs(self, costs, n_bins, thresh=np.inf):\n    \"\"\"Clusters the continuous features according to the costs.\"\"\"\n    # combine with self.group()\n    max_costs = np.zeros(costs.shape[0])\n    for i in range(costs.shape[0]):\n        if (costs[i] &lt;= thresh).any():\n            max_costs[i] = costs[i][costs[i] &lt;= thresh].max()\n\n    bins = pd.cut(max_costs, bins=n_bins, precision=32)  # can replace with min_costs\n    code, count = np.unique(bins.codes, return_counts=True)\n    rights = bins.categories.values.right\n\n    max_costs = costs.max(1)\n    max_scalar_idxs = np.zeros(len(code))\n    cost = costs.copy()\n\n    for i, c in enumerate(code):\n        idxs = np.arange(max_costs.shape[0])[max_costs &lt;= rights[c]]\n        if idxs.any():\n            max_scalar_idxs[i] = idxs[-1]\n            x_idxs = costs[idxs[-1]] == 0\n            if x_idxs.any():\n                cost[: idxs[-1] + 1] = 0\n                max_costs = cost[:, x_idxs].max(1)\n                max_costs[: idxs[-1] + 1] = np.inf\n        else:\n            max_scalar_idxs[i] = np.inf\n    return max_scalar_idxs\n</code></pre>"},{"location":"methods/global/globe-ce/#counterfactuals.cf_methods.global_methods.globe_ce.globe_ce.GLOBE_CE.evaluate_clustering","title":"evaluate_clustering","text":"<pre><code>evaluate_clustering(delta, scalars, max_scalar_idxs, costs=None, x_aff=None, print_outputs=True, vector=False, eps=0, latex_table=False)\n</code></pre> <p>Evaluates the clustering by computing the coverage and cost for each cluster</p> Source code in <code>counterfactuals/cf_methods/global_methods/globe_ce/globe_ce.py</code> <pre><code>def evaluate_clustering(\n    self,\n    delta,\n    scalars,\n    max_scalar_idxs,\n    costs=None,\n    x_aff=None,\n    print_outputs=True,\n    vector=False,\n    eps=0,\n    latex_table=False,\n):\n    \"\"\"Evaluates the clustering by computing the coverage and cost for each cluster\"\"\"\n    if x_aff is not None:\n        x_aff = x_aff.copy()\n    else:\n        x_aff = self.x_aff.copy()\n    if costs is None:\n        costs = np.zeros((scalars.shape[0], x_aff.shape[0]))\n        for i, scalar_idx in enumerate(max_scalar_idxs):\n            if scalar_idx != np.inf:\n                scalar_idx = int(scalar_idx)\n                costs[scalar_idx] = self.evaluate(\n                    delta * (scalars[scalar_idx] + eps), x_aff=x_aff, vector=True\n                )[1]\n    max_scalar_costs = np.zeros(x_aff.shape[0])\n    max_scalar_costs[:] = np.inf\n    costs_c = np.zeros(max_scalar_idxs.shape[0])\n    corrects_c = np.zeros(max_scalar_idxs.shape[0])\n    cor, avg_cost = 0, 0\n    last_rules = {}\n    for i, scalar_idx in enumerate(max_scalar_idxs):\n        # print(i)\n        if scalar_idx != np.inf:\n            scalar_idx = int(scalar_idx)\n            scalar_cost = costs[scalar_idx].copy()\n            scalar_cost[scalar_cost == 0] = np.inf\n            x_idx = scalar_cost &lt; max_scalar_costs  # input indexes of new or better costs\n            if x_idx.any():\n                max_scalar_costs[x_idx] = scalar_cost[x_idx]\n                new_cor = cor + x_idx.sum()\n                new_avg_cost = max_scalar_costs[x_idx].mean()\n                costs_c[i], corrects_c[i] = (\n                    max_scalar_costs[max_scalar_costs != np.inf].mean(),\n                    new_cor / x_aff.shape[0] * 100,\n                )\n                if print_outputs:\n                    # if i!=0: print()\n                    new_accs, new_costs = (\n                        round((new_cor - cor) / x_aff.shape[0] * 100, 2),\n                        round(new_avg_cost, 2),\n                    )\n                    total_accs, total_costs = (\n                        round(new_cor / x_aff.shape[0] * 100, 2),\n                        round(costs_c[i], 2),\n                    )\n                    if not latex_table:  # latex_table assumes purely categorical delta\n                        print(\n                            \"\\033[1m\\n New Inputs:\\t+{}%\\t\".format(new_accs)\n                            + \"New Inputs Cost:\\t{})\".format(new_costs)\n                        )\n                    x = x_aff[x_idx]\n                    rules = self.rules(delta * (scalars[scalar_idx] + eps), x_aff=x)\n                    j = 0\n                    prefix = \"\" if latex_table else \" Rules:\\033[0m\\t\"\n                    multiple_rules = False\n                    for rule in rules:\n                        r = rules[rule]\n                        if self.features_tree[rule] == []:\n                            c = self.feature_costs_vector[~self.categorical_idx]\n                            c = c[self.continuous_features.index(rule)].item()\n                            print(\n                                prefix,\n                                rule\n                                + \": {} ({})\".format(\n                                    round(float(r), 6), round(float(r) * c, 2)\n                                ),\n                            )\n                        else:\n                            if latex_table:\n                                if (rule not in last_rules) or (r not in last_rules[rule]):\n                                    prefix = \"\\n\" if multiple_rules else \"\"\n                                    multiple_rules = True\n                                    print(prefix + rule + \": \" + r)\n                            else:\n                                print(prefix, rule + \":\", r)\n                        prefix = \"\\n\" if latex_table else \"\\t\"\n                    if latex_table:\n                        print(\n                            \"&amp; {}\\% &amp; {} &amp; {}\\% &amp; {}\\\\\\\\\\\\midrule\".format(\n                                new_accs, new_costs, total_accs, total_costs\n                            )\n                        )\n                    else:\n                        print(\n                            \"\\033[1m(Coverage:\\t{}%\\t\".format(total_accs)\n                            + \"Average Cost:\\t\\t{})\\033[0m\".format(total_costs)\n                        )\n                    last_rules = rules\n                cor = new_cor\n                avg_cost = max_scalar_costs[max_scalar_costs != np.inf].mean()\n    if print_outputs and not latex_table:\n        print(\"\\n\\033[1mCoverage:\\t{}%\".format(round(cor / x_aff.shape[0] * 100, 4)))\n        print(\"Average Cost:\\t{}\".format(round(avg_cost, 4)))\n    if vector:\n        return costs_c, corrects_c\n    else:\n        return avg_cost, cor\n</code></pre>"},{"location":"methods/group/","title":"Group Methods","text":"<p>Group counterfactual methods generate explanations for clusters or subgroups of similar instances. They balance individual precision with broader applicability.</p>"},{"location":"methods/group/#available-methods","title":"Available Methods","text":"Method Description Key Feature ReViCE Regional variant of PPCEF Flow-based group CFs GLANCE Group-level anchor counterfactuals Anchor-based grouping Group GLOBE-CE Group variant of GLOBE-CE Subpopulation focus"},{"location":"methods/group/#when-to-use-group-methods","title":"When to Use Group Methods","text":"<p>Group methods are ideal when you need to:</p> <ul> <li>Provide explanations for similar users</li> <li>Balance personalization with scalability</li> <li>Identify subpopulation-specific patterns</li> <li>Generate semi-personalized recourse</li> </ul>"},{"location":"methods/group/#how-groups-are-formed","title":"How Groups Are Formed","text":"<p>Group methods typically cluster instances based on:</p> <ul> <li>Feature similarity</li> <li>Prediction confidence</li> <li>Demographic attributes</li> <li>Custom grouping criteria</li> </ul> <pre><code>flowchart LR\n    A[Dataset] --&gt; B[Clustering]\n    B --&gt; C[Group 1]\n    B --&gt; D[Group 2]\n    B --&gt; E[Group N]\n    C --&gt; F[Group CF 1]\n    D --&gt; G[Group CF 2]\n    E --&gt; H[Group CF N]</code></pre>"},{"location":"methods/group/#example-usage","title":"Example Usage","text":"<pre><code>from counterfactuals.cf_methods.group_methods import GroupPPCEF\n\n# Initialize method\nmethod = GroupPPCEF(\n    gen_model=flow_model,\n    disc_model=classifier,\n    disc_model_criterion=criterion,\n    device=\"cuda\"\n)\n\n# Generate group counterfactuals\nresult = method.explain(\n    X=X_test,\n    y_origin=y_test,\n    y_target=target_class,\n    X_train=X_train,\n    y_train=y_train,\n    n_groups=5  # Number of groups\n)\n\n# Each instance is assigned to a group\nprint(f\"Group assignments: {result.cf_group_ids}\")\n</code></pre>"},{"location":"methods/group/glance/","title":"GLANCE","text":"<p>Group-Level Anchor-based Counterfactual Explanations</p> <p>GLANCE uses anchors to define groups and generate group-level counterfactuals.</p>"},{"location":"methods/group/glance/#overview","title":"Overview","text":"<p>GLANCE identifies anchor points that define natural groupings and generates counterfactuals applicable to each group.</p>"},{"location":"methods/group/glance/#usage","title":"Usage","text":"<pre><code>from counterfactuals.cf_methods.group_methods import GLANCE\n\nmethod = GLANCE(\n    gen_model=gen_model,\n    disc_model=classifier,\n    disc_model_criterion=criterion,\n    device=\"cuda\"\n)\n\nresult = method.explain(\n    X=X_test,\n    y_origin=y_test,\n    y_target=target_class,\n    X_train=X_train,\n    y_train=y_train\n)\n</code></pre>"},{"location":"methods/group/glance/#api-reference","title":"API Reference","text":""},{"location":"methods/group/glance/#counterfactuals.cf_methods.group_methods.glance.glance.GLANCE","title":"GLANCE","text":"<pre><code>GLANCE(X_test, y_test, model, features, k=-1, s=4, m=1)\n</code></pre> Source code in <code>counterfactuals/cf_methods/group_methods/glance/glance.py</code> <pre><code>def __init__(\n    self,\n    X_test,\n    y_test,\n    model,\n    features,\n    k: int = -1,\n    s: int = 4,\n    m: int = 1,\n) -&gt; None:\n    self.features = features\n    self.model = model\n    self.X = X_test[y_test != 1]\n    self.Y = y_test[y_test != 1]\n    self.n = len(self.X)\n\n    self.k = k if k &gt; 0 else self.n  # starting number of a groups\n    self.s = s\n    self.m = m\n</code></pre>"},{"location":"methods/group/glance/#counterfactuals.cf_methods.group_methods.glance.glance.GLANCE.__merge_clusters","title":"__merge_clusters","text":"<pre><code>__merge_clusters(_c1, _c2, actions)\n</code></pre> <p>Merge two clusters by averaging their centroids, updating the labels and merging the actions</p> Source code in <code>counterfactuals/cf_methods/group_methods/glance/glance.py</code> <pre><code>def __merge_clusters(self, _c1: tuple, _c2: tuple, actions: dict) -&gt; dict:\n    \"\"\"Merge two clusters by averaging their centroids, updating the labels and merging the actions\"\"\"\n    new_centroid = (np.array(_c1[0]) + np.array(_c2[0])) / 2\n    new_label = self.k\n    self.k += 1\n\n    c1_action_set = actions[_c1]\n    c2_action_set = actions[_c2]\n\n    assert isinstance(c1_action_set, set)\n    assert isinstance(c2_action_set, set)\n\n    merged_actions = c1_action_set.union(c2_action_set)\n\n    # Remove from actions\n    del actions[_c1]\n    del actions[_c2]\n\n    # Add the new cluster\n    actions[(tuple(new_centroid), new_label)] = merged_actions\n</code></pre>"},{"location":"methods/group/glance/#counterfactuals.cf_methods.group_methods.glance.glance.GLANCE.get_clusters","title":"get_clusters","text":"<pre><code>get_clusters()\n</code></pre> <p>Returns the final clusters as a list of tuples</p> <p>Each tuple contains: - The centroid of the cluster - The label of the cluster - The average action of the cluster (centroid + action = counterfactual)</p> Source code in <code>counterfactuals/cf_methods/group_methods/glance/glance.py</code> <pre><code>def get_clusters(self) -&gt; list[tuple]:\n    \"\"\"\n    Returns the final clusters as a list of tuples\n\n    Each tuple contains:\n    - The centroid of the cluster\n    - The label of the cluster\n    - The average action of the cluster (centroid + action = counterfactual)\n    \"\"\"\n    return self.final_clusters\n</code></pre>"},{"location":"methods/group/group-globe-ce/","title":"Group GLOBE-CE","text":"<p>Group Variant of GLOBE-CE</p> <p>Group GLOBE-CE extends GLOBE-CE to find transformations for subpopulations.</p>"},{"location":"methods/group/group-globe-ce/#overview","title":"Overview","text":"<p>Group GLOBE-CE partitions the data into groups and finds optimal transformations for each group.</p>"},{"location":"methods/group/group-globe-ce/#usage","title":"Usage","text":"<p>Group GLOBE-CE can be implemented by combining GLOBE-CE with group clustering. See the RPPCEF implementation for a similar approach.</p> <pre><code># Group GLOBE-CE is implemented via the RPPCEF method\n# with GLOBE-CE style deltas\nfrom counterfactuals.cf_methods.group_methods.group_ppcef import RPPCEF\n\nmethod = RPPCEF(\n    gen_model=gen_model,\n    disc_model=classifier,\n    disc_model_criterion=criterion,\n    device=\"cuda\"\n)\n\nresult = method.explain(\n    X=X_test,\n    y_origin=y_test,\n    y_target=target_class,\n    X_train=X_train,\n    y_train=y_train,\n    n_groups=5\n)\n</code></pre>"},{"location":"methods/group/group-globe-ce/#related-methods","title":"Related Methods","text":"<ul> <li>GLOBE-CE - Global counterfactual explanations</li> <li>ReViCE - Regional/group PPCEF</li> </ul>"},{"location":"methods/group/revice/","title":"ReViCE","text":"<p>Regional Variant of PPCEF (Group PPCEF)</p> <p>ReViCE generates counterfactuals for groups of similar instances.</p>"},{"location":"methods/group/revice/#overview","title":"Overview","text":"<p>ReViCE extends PPCEF to handle groups, finding counterfactual transformations that work for clusters of similar instances.</p>"},{"location":"methods/group/revice/#usage","title":"Usage","text":"<pre><code>from counterfactuals.cf_methods.group_methods.group_ppcef import RPPCEF\n\nmethod = RPPCEF(\n    gen_model=gen_model,\n    disc_model=classifier,\n    disc_model_criterion=criterion,\n    device=\"cuda\"\n)\n\nresult = method.explain(\n    X=X_test,\n    y_origin=y_test,\n    y_target=target_class,\n    X_train=X_train,\n    y_train=y_train,\n    n_groups=5\n)\n</code></pre>"},{"location":"methods/group/revice/#api-reference","title":"API Reference","text":""},{"location":"methods/group/revice/#counterfactuals.cf_methods.group_methods.group_ppcef.rppcef.RPPCEF","title":"RPPCEF","text":"<pre><code>RPPCEF(cf_method_type, gen_model, disc_model, disc_model_criterion, init_cf_method_from_kmeans=False, K=None, X=None, device=None, actionable_features=None, **kwargs)\n</code></pre> <p>               Bases: <code>BaseCounterfactualMethod</code>, <code>GroupCounterfactualMixin</code></p> Source code in <code>counterfactuals/cf_methods/group_methods/group_ppcef/rppcef.py</code> <pre><code>def __init__(\n    self,\n    cf_method_type: str,\n    gen_model: GenerativePytorchMixin,\n    disc_model: PytorchBase,\n    disc_model_criterion: torch.nn.modules.loss._Loss,\n    init_cf_method_from_kmeans: bool = False,\n    K: int = None,\n    X: np.ndarray = None,\n    device: str = None,\n    # TODO: poprawa nazewnictwa\n    actionable_features: list = None,\n    **kwargs,\n):\n    # Initialize mixins / base to set up models and device\n    super().__init__(\n        gen_model=gen_model,\n        disc_model=disc_model,\n        disc_model_criterion=disc_model_criterion,\n        device=device,\n        **kwargs,\n    )\n\n    self.actionable_features = actionable_features\n    # initialize delta after we know device/other attributes\n    self.delta = self._init_cf_method(cf_method_type, K, init_cf_method_from_kmeans, X)\n    self.loss_components_logs = {}\n</code></pre>"},{"location":"methods/group/revice/#counterfactuals.cf_methods.group_methods.group_ppcef.rppcef.RPPCEF.explain","title":"explain","text":"<pre><code>explain(X, y_origin, y_target, X_train, y_train)\n</code></pre> <p>Explains the model's prediction for a given input.</p> Source code in <code>counterfactuals/cf_methods/group_methods/group_ppcef/rppcef.py</code> <pre><code>def explain(\n    self,\n    X: np.ndarray,\n    y_origin: np.ndarray,\n    y_target: np.ndarray,\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n):\n    \"\"\"\n    Explains the model's prediction for a given input.\n    \"\"\"\n    raise NotImplementedError(\"This method is not implemented for this class.\")\n</code></pre>"},{"location":"methods/group/revice/#counterfactuals.cf_methods.group_methods.group_ppcef.rppcef.RPPCEF.explain_dataloader","title":"explain_dataloader","text":"<pre><code>explain_dataloader(dataloader, alpha, alpha_s, alpha_k, log_prob_threshold, epochs=1000, lr=0.0005, patience=100, patience_eps=0.001)\n</code></pre> <p>Trains the model for a specified number of epochs.</p> Source code in <code>counterfactuals/cf_methods/group_methods/group_ppcef/rppcef.py</code> <pre><code>def explain_dataloader(\n    self,\n    dataloader: DataLoader,\n    alpha: int,\n    alpha_s: int,\n    alpha_k: int,\n    log_prob_threshold: float,\n    epochs: int = 1000,\n    lr: float = 0.0005,\n    patience: int = 100,\n    patience_eps: int = 1e-3,\n):\n    \"\"\"\n    Trains the model for a specified number of epochs.\n    \"\"\"\n    self.loss_components_logs = {}\n    self.gen_model.eval()\n    for param in self.gen_model.parameters():\n        param.requires_grad = False\n\n    if self.disc_model:\n        self.disc_model.eval()\n        for param in self.disc_model.parameters():\n            param.requires_grad = False\n\n    target_class = []\n    original = []\n    original_class = []\n    for xs_origin, contexts_origin in dataloader:\n        xs_origin = xs_origin.to(self.device)\n        contexts_origin = contexts_origin.to(self.device)\n\n        contexts_origin = contexts_origin.reshape(-1, 1)\n        contexts_target = torch.abs(1 - contexts_origin)\n\n        xs_origin = torch.as_tensor(xs_origin)\n        xs_origin.requires_grad = False\n\n        optimizer = torch.optim.Adam(self.delta.parameters(), lr=lr)\n\n        min_loss = float(\"inf\")\n        dist_flag = False\n\n        for epoch in (epoch_pbar := tqdm(range(epochs), dynamic_ncols=True)):\n            optimizer.zero_grad()\n            loss_components = self._search_step(\n                self.delta,\n                xs_origin,\n                contexts_origin,\n                contexts_target,\n                alpha=alpha,\n                alpha_s=alpha_s,\n                alpha_k=alpha_k,\n                log_prob_threshold=log_prob_threshold,\n            )\n            mean_loss = loss_components[\"loss\"].mean()\n            mean_loss.backward()\n            optimizer.step()\n\n            self._log_loss_components(loss_components)\n\n            loss = loss_components[\"loss\"].detach().cpu().mean().item()\n            # Progress bar description\n            epoch_pbar.set_description(\n                \", \".join(\n                    [\n                        f\"{k}: {v.detach().cpu().mean().item():.4f}\"\n                        for k, v in loss_components.items()\n                    ]\n                )\n            )\n            # Early stopping handling\n            if (loss &lt; (min_loss - patience_eps)) or (epoch &lt; 1000):\n                min_loss = loss\n                patience_counter = 0\n            else:\n                patience_counter += 1\n                if patience_counter &gt; patience:\n                    if not dist_flag:\n                        patience_counter = 0\n                        dist_flag = True\n                    else:\n                        break\n\n        original.append(xs_origin.detach().cpu().numpy())\n        original_class.append(contexts_origin.detach().cpu().numpy())\n        target_class.append(contexts_target.detach().cpu().numpy())\n\n    x_origs = np.concatenate(original, axis=0)\n    y_origs = np.concatenate(original_class, axis=0)\n    y_target = np.concatenate(target_class, axis=0)\n    # x_cfs = x_origs + self.delta().detach().numpy()\n    return self.delta, x_origs, y_origs, y_target\n</code></pre>"},{"location":"methods/local/","title":"Local Methods","text":"<p>Local counterfactual methods generate explanations for individual instances. They answer the question: \"What minimal changes to this specific input would change the model's prediction?\"</p>"},{"location":"methods/local/#available-methods","title":"Available Methods","text":"Method Description Key Feature Best For Artelt Heuristic-based method Fast computation Speed-critical applications CCHVAE Conditional hierarchical VAE Latent space VAE-based explanations CEGP Genetic programming approach Evolutionary search Non-differentiable models CEM Contrastive explanation method Pertinent negatives Contrastive explanations CET Counterfactual explanation trees Tree-based Interpretable rules DiCoFlex Diverse counterfactuals with flexible constraints Diversity + constraints Balanced quality DICE Diverse counterfactual explanations Multiple diverse CFs Diversity-focused PPCEF Probabilistic counterfactuals with normalizing flows High plausibility Flow-based density SACE Several SACE variants Multiple strategies Strategy comparison WACH Weighted actionable counterfactuals Actionability focus Simple gradient CFs"},{"location":"methods/local/#when-to-use-local-methods","title":"When to Use Local Methods","text":"<p>Local methods are ideal when you need to:</p> <ul> <li>Explain a specific prediction to a user</li> <li>Provide actionable recourse for an individual</li> <li>Debug model behavior on particular instances</li> <li>Generate personalized recommendations</li> </ul>"},{"location":"methods/local/#example-usage","title":"Example Usage","text":"<p>This example demonstrates PPCEF, but the same pattern applies to all local methods:</p> <pre><code>from counterfactuals.cf_methods.local_methods import PPCEF\n\n# Initialize method\nmethod = PPCEF(\n    gen_model=flow_model,\n    disc_model=classifier,\n    disc_model_criterion=criterion,\n    device=\"cuda\"\n)\n\n# Generate counterfactual for a single instance\nresult = method.explain(\n    X=instance,           # Shape: (1, n_features)\n    y_origin=0,           # Current prediction\n    y_target=1,           # Desired prediction\n    X_train=X_train,\n    y_train=y_train,\n    epochs=100,\n    lr=0.01\n)\n\nprint(f\"Original: {instance}\")\nprint(f\"Counterfactual: {result.x_cfs}\")\n</code></pre> <p>Using Other Methods</p> <p>To use a different method, simply change the import: <pre><code>from counterfactuals.cf_methods.local_methods import DICE  # or WACH, CEM, etc.\nmethod = DICE(...)  # Each method has different parameters\n</code></pre></p>"},{"location":"methods/local/artelt/","title":"Artelt","text":"<p>Artelt's Heuristic Counterfactual Method</p> <p>Artelt's method uses heuristics for fast counterfactual generation.</p>"},{"location":"methods/local/artelt/#variants","title":"Variants","text":"<ul> <li>Artelt: Base implementation</li> <li>Heuristic20: Optimized variant with 20 heuristics</li> </ul>"},{"location":"methods/local/artelt/#usage","title":"Usage","text":"<pre><code>from counterfactuals.cf_methods.local_methods import Artelt\n\nmethod = Artelt(\n    gen_model=gen_model,\n    disc_model=classifier,\n    disc_model_criterion=criterion,\n    device=\"cuda\"\n)\n\nresult = method.explain(\n    X=instance,\n    y_origin=0,\n    y_target=1,\n    X_train=X_train,\n    y_train=y_train\n)\n</code></pre>"},{"location":"methods/local/artelt/#api-reference","title":"API Reference","text":""},{"location":"methods/local/artelt/#counterfactuals.cf_methods.local_methods.artelt.artelt.Artelt","title":"Artelt","text":"<pre><code>Artelt(disc_model, **kwargs)\n</code></pre> <p>               Bases: <code>BaseCounterfactualMethod</code>, <code>LocalCounterfactualMixin</code></p> Source code in <code>counterfactuals/cf_methods/local_methods/artelt/artelt.py</code> <pre><code>def __init__(self, disc_model: PytorchBase, **kwargs) -&gt; None:\n    self.disc_model = disc_model\n    self.density_estimators = {}\n    self.kernel_density_estimators = {}\n    self.ellipsoids = {}\n    self.cf = {}\n</code></pre>"},{"location":"methods/local/cchvae/","title":"CCHVAE","text":"<p>Conditional Counterfactual Hierarchical VAE</p> <p>CCHVAE uses a hierarchical variational autoencoder for counterfactual generation.</p>"},{"location":"methods/local/cchvae/#overview","title":"Overview","text":"<p>CCHVAE learns a latent representation that enables generating plausible counterfactuals through latent space traversal.</p>"},{"location":"methods/local/cchvae/#usage","title":"Usage","text":"<pre><code>from counterfactuals.cf_methods.local_methods.c_chvae import CCHVAE\n\nmethod = CCHVAE(\n    mlmodel=ml_model,\n    hyperparams=hyperparams\n)\n\nresult = method.get_counterfactuals(\n    factuals=factuals\n)\n</code></pre>"},{"location":"methods/local/cchvae/#api-reference","title":"API Reference","text":""},{"location":"methods/local/cchvae/#counterfactuals.cf_methods.local_methods.c_chvae.c_chvae.CCHVAE","title":"CCHVAE","text":"<pre><code>CCHVAE(mlmodel, hyperparams=None)\n</code></pre> <p>Implementation of CCHVAE.</p> <p>This class implements the Counterfactuals via Conditional Variational Autoencoders (CCHVAE) method for generating model-agnostic counterfactual explanations for tabular data, following Pawelczyk et al. (2020).</p> <p>Parameters:</p> Name Type Description Default <code>mlmodel</code> <code>MLModel</code> <p>Black-box model wrapper used for prediction and data access.</p> required <code>hyperparams</code> <code>Dict</code> <p>Dictionary of hyperparameters. See Notes for details.</p> <code>None</code> Notes <p>Hyperparameters (<code>hyperparams</code>) control initialization and search behavior:</p> <ul> <li><code>\"data_name\"</code> (str): Name of the dataset.</li> <li><code>\"n_search_samples\"</code> (int, default: 300): Number of candidate counterfactuals sampled per iteration.</li> <li><code>\"p_norm\"</code> (int in {1, 2}): L_p norm used for distance calculation.</li> <li><code>\"step\"</code> (float, default: 0.1): Step size for expanding the search radius.</li> <li><code>\"max_iter\"</code> (int, default: 2000): Maximum iterations per factual instance.</li> <li><code>\"clamp\"</code> (bool, default: True): If True, feature values are clamped to [0, 1].</li> <li><code>\"binary_cat_features\"</code> (bool, default: True): If True, categorical encoding uses drop-if-binary.</li> <li><code>\"vae_params\"</code> (Dict): Parameters for the VAE:</li> <li><code>\"layers\"</code> (List[int]): Number of neurons per layer.</li> <li><code>\"train\"</code> (bool, default: True): Whether to train a new VAE.</li> <li><code>\"kl_weight\"</code> (float, default: 0.3): KL divergence weight for the VAE loss.</li> <li><code>\"lambda_reg\"</code> (float, default: 1e-6): Regularization weight for VAE.</li> <li><code>\"epochs\"</code> (int, default: 5): Training epochs for the VAE.</li> <li><code>\"lr\"</code> (float, default: 1e-3): Learning rate for the VAE optimizer.</li> <li><code>\"batch_size\"</code> (int, default: 32): Batch size for VAE training.</li> </ul> References <p>Pawelczyk, M., Broelemann, K., &amp; Kasneci, G. (2020). Learning Model-Agnostic Counterfactual Explanations for Tabular Data. In Proceedings of The Web Conference 2020.</p> <p>Parameters:</p> Name Type Description Default <code>mlmodel</code> <code>MLModel</code> <p>Model wrapper providing prediction and dataset utilities.</p> required <code>hyperparams</code> <code>Dict</code> <p>Hyperparameter dictionary</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provided model backend is unsupported.</p> <code>FileNotFoundError</code> <p>If VAE loading is requested but the model file is missing.</p> Source code in <code>counterfactuals/cf_methods/local_methods/c_chvae/c_chvae.py</code> <pre><code>def __init__(self, mlmodel: MLModel, hyperparams: Dict = None) -&gt; None:\n    \"\"\"Initializes the CCHVAE method.\n\n    Args:\n      mlmodel: Model wrapper providing prediction and dataset utilities.\n      hyperparams: Hyperparameter dictionary\n\n    Raises:\n      ValueError: If the provided model backend is unsupported.\n      FileNotFoundError: If VAE loading is requested but the model file is missing.\n    \"\"\"\n    supported_backends = [\"pytorch\"]\n    if mlmodel.backend not in supported_backends:\n        raise ValueError(f\"{mlmodel.backend} is not in supported backends {supported_backends}\")\n\n    self._mlmodel = mlmodel\n    self._params = hyperparams\n\n    self._n_search_samples = self._params[\"n_search_samples\"]\n    self._p_norm = self._params[\"p_norm\"]\n    self._step = self._params[\"step\"]\n    self._max_iter = self._params[\"max_iter\"]\n    self._clamp = self._params[\"clamp\"]\n\n    vae_params = self._params[\"vae_params\"]\n    self._generative_model = self._load_vae(\n        self._mlmodel.data.df, vae_params, self._mlmodel, self._params[\"data_name\"]\n    )\n</code></pre>"},{"location":"methods/local/cchvae/#counterfactuals.cf_methods.local_methods.c_chvae.c_chvae.CCHVAE.get_counterfactuals","title":"get_counterfactuals","text":"<pre><code>get_counterfactuals(factuals)\n</code></pre> <p>Generates counterfactuals for the given factual instances with validation.</p> <p>This method applies the internal search for each factual row, checks the validity of found counterfactuals, and returns them in the original feature order of the model.</p> <p>Parameters:</p> Name Type Description Default <code>factuals</code> <code>DataFrame</code> <p>DataFrame of factual instances.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame containing validated counterfactual instances aligned to <code>factuals</code>.</p> Source code in <code>counterfactuals/cf_methods/local_methods/c_chvae/c_chvae.py</code> <pre><code>def get_counterfactuals(self, factuals: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Generates counterfactuals for the given factual instances with validation.\n\n    This method applies the internal search for each factual row, checks the\n    validity of found counterfactuals, and returns them in the original feature\n    order of the model.\n\n    Args:\n      factuals: DataFrame of factual instances.\n\n    Returns:\n      DataFrame containing validated counterfactual instances aligned to `factuals`.\n    \"\"\"\n    factuals = self._mlmodel.get_ordered_features(factuals)\n\n    encoded_feature_names = self._mlmodel.data.categorical\n    cat_features_indices = [\n        factuals.columns.get_loc(feature) for feature in encoded_feature_names\n    ]\n\n    df_cfs = factuals.apply(\n        lambda x: self._counterfactual_search(\n            self._step, x.reshape((1, -1)), cat_features_indices\n        ),\n        raw=True,\n        axis=1,\n    )\n\n    df_cfs = check_counterfactuals(self._mlmodel, df_cfs, factuals.index)\n    df_cfs = self._mlmodel.get_ordered_features(df_cfs)\n    return df_cfs\n</code></pre>"},{"location":"methods/local/cchvae/#counterfactuals.cf_methods.local_methods.c_chvae.c_chvae.CCHVAE.get_counterfactuals_without_check","title":"get_counterfactuals_without_check","text":"<pre><code>get_counterfactuals_without_check(factuals)\n</code></pre> <p>Generates counterfactuals without running the post-hoc validity checks.</p> <p>This is similar to <code>get_counterfactuals</code> but skips <code>check_counterfactuals</code>, returning the raw counterfactual outputs projected back to the model's original feature order.</p> <p>Parameters:</p> Name Type Description Default <code>factuals</code> <code>DataFrame</code> <p>DataFrame of factual instances.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame containing counterfactual instances aligned to <code>factuals</code>.</p> Source code in <code>counterfactuals/cf_methods/local_methods/c_chvae/c_chvae.py</code> <pre><code>def get_counterfactuals_without_check(self, factuals: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Generates counterfactuals without running the post-hoc validity checks.\n\n    This is similar to `get_counterfactuals` but skips `check_counterfactuals`,\n    returning the raw counterfactual outputs projected back to the model's\n    original feature order.\n\n    Args:\n      factuals: DataFrame of factual instances.\n\n    Returns:\n      DataFrame containing counterfactual instances aligned to `factuals`.\n    \"\"\"\n    factuals = self._mlmodel.get_ordered_features(factuals)\n\n    encoded_feature_names = self._mlmodel.data.categorical\n    cat_features_indices = [\n        factuals.columns.get_loc(feature) for feature in encoded_feature_names\n    ]\n\n    df_cfs = factuals.apply(\n        lambda x: self._counterfactual_search(\n            self._step, x.reshape((1, -1)), cat_features_indices\n        ),\n        raw=True,\n        axis=1,\n    )\n\n    df_cfs = self._mlmodel.get_ordered_features(df_cfs)\n    return df_cfs\n</code></pre>"},{"location":"methods/local/cegp/","title":"CEGP","text":"<p>Counterfactual Explanations via Genetic Programming</p> <p>CEGP uses evolutionary algorithms to search for counterfactual explanations.</p>"},{"location":"methods/local/cegp/#overview","title":"Overview","text":"<p>CEGP applies genetic programming to evolve counterfactual candidates through mutation and crossover operations.</p>"},{"location":"methods/local/cegp/#usage","title":"Usage","text":"<pre><code>from counterfactuals.cf_methods.local_methods import CEGP\n\nmethod = CEGP(\n    gen_model=gen_model,\n    disc_model=classifier,\n    disc_model_criterion=criterion,\n    device=\"cuda\"\n)\n\nresult = method.explain(\n    X=instance,\n    y_origin=0,\n    y_target=1,\n    X_train=X_train,\n    y_train=y_train\n)\n</code></pre>"},{"location":"methods/local/cegp/#api-reference","title":"API Reference","text":""},{"location":"methods/local/cegp/#counterfactuals.cf_methods.local_methods.cegp.cegp.CEGP","title":"CEGP","text":"<pre><code>CEGP(disc_model, beta=0.01, c_init=1.0, c_steps=5, max_iterations=500, device=None, **kwargs)\n</code></pre> <p>               Bases: <code>BaseCounterfactualMethod</code>, <code>LocalCounterfactualMixin</code></p> <p>Parameters:</p> Name Type Description Default <code>disc_model</code> <code>PytorchBase</code> <p>Discriminative model to use for counterfactual generation</p> required <code>beta</code> <code>float</code> <p>Trade-off parameter for distance computation</p> <code>0.01</code> <code>c_init</code> <code>float</code> <p>Initial value of c for the attack loss term</p> <code>1.0</code> <code>c_steps</code> <code>int</code> <p>Number of steps to adjust c</p> <code>5</code> <code>max_iterations</code> <code>int</code> <p>Maximum number of iterations to run optimization</p> <code>500</code> Source code in <code>counterfactuals/cf_methods/local_methods/cegp/cegp.py</code> <pre><code>def __init__(\n    self,\n    disc_model: PytorchBase,\n    beta: float = 0.01,\n    c_init: float = 1.0,\n    c_steps: int = 5,\n    max_iterations: int = 500,\n    device: str | None = None,\n    **kwargs,  # ignore other arguments\n) -&gt; None:\n    \"\"\"Initialize CEGP counterfactual method.\n\n    Args:\n        disc_model: Discriminative model to use for counterfactual generation\n        beta: Trade-off parameter for distance computation\n        c_init: Initial value of c for the attack loss term\n        c_steps: Number of steps to adjust c\n        max_iterations: Maximum number of iterations to run optimization\n    \"\"\"\n    # Initialize base/mixin (moves model to device if applicable)\n    super().__init__(disc_model=disc_model, device=device)\n\n    tf.compat.v1.disable_eager_execution()\n    predict_proba = lambda x: disc_model.predict_proba(x).numpy()  # noqa: E731\n    num_features = disc_model.input_size\n    shape = (1, num_features)\n\n    # Get feature ranges from training data\n    feature_range = (0, 1)  # Default range, should be adjusted based on data\n\n    self.cf = CounterFactualProto(\n        predict_proba,\n        shape,\n        beta=beta,\n        max_iterations=max_iterations,\n        feature_range=feature_range,\n        c_init=c_init,\n        c_steps=c_steps,\n    )\n\n    self.is_fitted = False\n</code></pre>"},{"location":"methods/local/cegp/#counterfactuals.cf_methods.local_methods.cegp.cegp.CEGP.fit","title":"fit","text":"<pre><code>fit(X_train)\n</code></pre> <p>Fit the CEGP model on training data.</p> <p>Parameters:</p> Name Type Description Default <code>X_train</code> <code>ndarray</code> <p>Training data to fit the model on</p> required Source code in <code>counterfactuals/cf_methods/local_methods/cegp/cegp.py</code> <pre><code>def fit(self, X_train: np.ndarray) -&gt; None:\n    \"\"\"Fit the CEGP model on training data.\n\n    Args:\n        X_train: Training data to fit the model on\n    \"\"\"\n    self.cf.fit(X_train.astype(np.float32), d_type=\"abdm\", disc_perc=[25, 50, 75])\n    self.is_fitted = True\n</code></pre>"},{"location":"methods/local/cegp/#counterfactuals.cf_methods.local_methods.cegp.cegp.CEGP.explain","title":"explain","text":"<pre><code>explain(X, y_origin, y_target, X_train=None, y_train=None, **kwargs)\n</code></pre> <p>Generate counterfactual explanations for given samples.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Samples to explain</p> required <code>y_origin</code> <code>ndarray</code> <p>Original labels</p> required <code>y_target</code> <code>ndarray</code> <p>Target labels</p> required <code>X_train</code> <code>ndarray | None</code> <p>Training data (used for fitting if not already fitted)</p> <code>None</code> <code>y_train</code> <code>ndarray | None</code> <p>Training labels</p> <code>None</code> Source code in <code>counterfactuals/cf_methods/local_methods/cegp/cegp.py</code> <pre><code>def explain(\n    self,\n    X: np.ndarray,\n    y_origin: np.ndarray,\n    y_target: np.ndarray,\n    X_train: np.ndarray | None = None,\n    y_train: np.ndarray | None = None,\n    **kwargs,\n) -&gt; ExplanationResult:\n    \"\"\"Generate counterfactual explanations for given samples.\n\n    Args:\n        X: Samples to explain\n        y_origin: Original labels\n        y_target: Target labels\n        X_train: Training data (used for fitting if not already fitted)\n        y_train: Training labels\n    \"\"\"\n    if X_train is not None and not self.is_fitted:\n        self.fit(X_train)\n\n    try:\n        X_in = X.copy()\n        X_proc = X.reshape((1,) + X.shape)\n        explanation = self.cf.explain(X_proc).cf\n        if explanation is None:\n            raise ValueError(\"No counterfactual found\")\n        x_cfs = np.array(explanation.get(\"X\"))\n    except Exception as e:\n        print(f\"Error in CEGP explanation: {e}\")\n        x_cfs = np.full_like(X.reshape(1, -1), np.nan).squeeze()\n\n    # Wrap results in ExplanationResult\n    return ExplanationResult(\n        x_cfs=np.array(x_cfs),\n        y_cf_targets=np.array(y_target),\n        x_origs=np.array(X_in),\n        y_origs=np.array(y_origin),\n        logs=None,\n    )\n</code></pre>"},{"location":"methods/local/cegp/#counterfactuals.cf_methods.local_methods.cegp.cegp.CEGP.explain_dataloader","title":"explain_dataloader","text":"<pre><code>explain_dataloader(dataloader, target_class, *args, **kwargs)\n</code></pre> <p>Generate counterfactual explanations for all samples in dataloader.</p> <p>Parameters:</p> Name Type Description Default <code>dataloader</code> <code>DataLoader</code> <p>DataLoader containing samples to explain</p> required <code>target_class</code> <code>int</code> <p>Target class for counterfactuals</p> required Source code in <code>counterfactuals/cf_methods/local_methods/cegp/cegp.py</code> <pre><code>def explain_dataloader(\n    self, dataloader: DataLoader, target_class: int, *args, **kwargs\n) -&gt; ExplanationResult:\n    \"\"\"Generate counterfactual explanations for all samples in dataloader.\n\n    Args:\n        dataloader: DataLoader containing samples to explain\n        target_class: Target class for counterfactuals\n    \"\"\"\n    Xs, ys = dataloader.dataset.tensors\n\n    # Fit on first batch if not already fitted\n    if not self.is_fitted:\n        self.fit(Xs.numpy())\n\n    # create ys_target numpy array same shape as ys but with target class\n    ys_target = np.full(ys.shape, target_class)\n    Xs_cfs = []\n    model_returned = []\n\n    for X, y in tqdm(zip(Xs, ys), total=len(Xs)):\n        try:\n            X = X.reshape((1,) + X.shape)\n            explanation = self.cf.explain(X).cf\n            if explanation is None:\n                raise ValueError(\"No counterfactual found\")\n            Xs_cfs.append(explanation[\"X\"])\n            model_returned.append(True)\n        except Exception as e:\n            print(f\"Error in CEGP explanation: {e}\")\n            explanation = np.empty_like(X.reshape(1, -1))\n            explanation[:] = np.nan\n            Xs_cfs.append(explanation)\n            model_returned.append(False)\n\n    Xs_cfs = np.array(Xs_cfs).squeeze()\n    Xs = np.array(Xs)\n    ys = np.array(ys)\n    ys_target = np.array(ys_target)\n\n    return ExplanationResult(\n        x_cfs=Xs_cfs,\n        y_cf_targets=ys_target,\n        x_origs=Xs,\n        y_origs=ys,\n        logs={\"model_returned\": model_returned},\n    )\n</code></pre>"},{"location":"methods/local/cem/","title":"CEM","text":"<p>Contrastive Explanation Method</p> <p>CEM generates counterfactuals by finding contrastive perturbations.</p>"},{"location":"methods/local/cem/#overview","title":"Overview","text":"<p>CEM identifies both pertinent positives (features that must be present) and pertinent negatives (features that must be absent) for a prediction.</p>"},{"location":"methods/local/cem/#usage","title":"Usage","text":"<pre><code>from counterfactuals.cf_methods.local_methods.cem import CEM_CF\n\nmethod = CEM_CF(\n    gen_model=gen_model,\n    disc_model=classifier,\n    disc_model_criterion=criterion,\n    device=\"cuda\"\n)\n\nresult = method.explain(\n    X=instance,\n    y_origin=0,\n    y_target=1,\n    X_train=X_train,\n    y_train=y_train\n)\n</code></pre>"},{"location":"methods/local/cem/#api-reference","title":"API Reference","text":""},{"location":"methods/local/cem/#counterfactuals.cf_methods.local_methods.cem.cem.CEM_CF","title":"CEM_CF","text":"<pre><code>CEM_CF(disc_model, mode='PN', kappa=0.2, beta=0.1, c_init=10.0, c_steps=5, max_iterations=200, learning_rate_init=0.01, device=None, **kwargs)\n</code></pre> <p>               Bases: <code>BaseCounterfactualMethod</code>, <code>LocalCounterfactualMixin</code></p> Source code in <code>counterfactuals/cf_methods/local_methods/cem/cem.py</code> <pre><code>def __init__(\n    self,\n    disc_model: PytorchBase,\n    mode: str = \"PN\",\n    kappa: float = 0.2,\n    beta: float = 0.1,\n    c_init: float = 10.0,\n    c_steps: int = 5,\n    max_iterations: int = 200,\n    learning_rate_init: float = 1e-2,\n    device: str | None = None,\n    **kwargs,  # ignore other arguments\n) -&gt; None:\n    # Initialize base/mixin (moves model to device if applicable)\n    super().__init__(disc_model=disc_model, device=device)\n\n    tf.compat.v1.disable_eager_execution()\n    predict_proba = lambda x: disc_model.predict_proba(x).numpy()  # noqa: E731\n    num_features = disc_model.input_size\n    shape = (1, num_features)\n\n    # Set gradient clipping\n    clip = (-1000.0, 1000.0)\n\n    # Get feature ranges from model\n    feature_range = (0, 1)  # Default range, should be adjusted based on data\n\n    self.cf = CEM(\n        predict_proba,\n        mode=mode,\n        shape=shape,\n        kappa=kappa,\n        beta=beta,\n        feature_range=feature_range,\n        max_iterations=max_iterations,\n        c_init=c_init,\n        c_steps=c_steps,\n        learning_rate_init=learning_rate_init,\n        clip=clip,\n    )\n</code></pre>"},{"location":"methods/local/cem/#counterfactuals.cf_methods.local_methods.cem.cem.CEM_CF.fit","title":"fit","text":"<pre><code>fit(X_train)\n</code></pre> <p>Fit the CEM model on training data</p> Source code in <code>counterfactuals/cf_methods/local_methods/cem/cem.py</code> <pre><code>def fit(self, X_train: np.ndarray) -&gt; None:\n    \"\"\"Fit the CEM model on training data\"\"\"\n    self.cf.fit(X_train, no_info_type=\"median\")\n</code></pre>"},{"location":"methods/local/cet/","title":"CET","text":"<p>Counterfactual Explanation Trees</p> <p>CET uses tree structures to generate interpretable counterfactual explanations.</p>"},{"location":"methods/local/cet/#overview","title":"Overview","text":"<p>CET builds decision trees that guide the counterfactual generation process.</p>"},{"location":"methods/local/cet/#usage","title":"Usage","text":"<pre><code>from counterfactuals.cf_methods.local_methods.cet import CounterfactualExplanationTree\n\nmethod = CounterfactualExplanationTree(\n    gen_model=gen_model,\n    disc_model=classifier,\n    disc_model_criterion=criterion,\n    device=\"cuda\"\n)\n\nresult = method.explain(\n    X=instance,\n    y_origin=0,\n    y_target=1,\n    X_train=X_train,\n    y_train=y_train\n)\n</code></pre>"},{"location":"methods/local/cet/#api-reference","title":"API Reference","text":""},{"location":"methods/local/cet/#counterfactuals.cf_methods.local_methods.cet.cet.CounterfactualExplanationTree","title":"CounterfactualExplanationTree","text":"<pre><code>CounterfactualExplanationTree(mdl, X, Y=[], max_iteration=1000, max_depth=3, min_samples_leaf=1, remain_redundant_leaf=False, max_candidates=50, tol=1e-06, use_mined_rules=False, minsup=0.5, discretization_bins=5, lime_approximation=False, n_samples=10000, alpha=1.0, feature_names=[], feature_types=[], feature_categories=[], feature_constraints=[], target_name='Output', target_labels=['Good', 'Bad'], device=None, **kwargs)\n</code></pre> <p>               Bases: <code>BaseCounterfactualMethod</code>, <code>LocalCounterfactualMixin</code></p> Source code in <code>counterfactuals/cf_methods/local_methods/cet/cet.py</code> <pre><code>def __init__(\n    self,\n    mdl,\n    X,\n    Y=[],\n    max_iteration=1000,\n    max_depth=3,\n    min_samples_leaf=1,\n    remain_redundant_leaf=False,\n    max_candidates=50,\n    tol=1e-6,\n    use_mined_rules=False,\n    minsup=0.5,\n    discretization_bins=5,\n    lime_approximation=False,\n    n_samples=10000,\n    alpha=1.0,\n    feature_names=[],\n    feature_types=[],\n    feature_categories=[],\n    feature_constraints=[],\n    target_name=\"Output\",\n    target_labels=[\"Good\", \"Bad\"],\n    device: str | None = None,\n    **kwargs,\n):\n    # Initialize base/mixin behavior\n    super().__init__(disc_model=mdl, device=device)\n    self.mdl_ = mdl\n    self.extractor_ = ActionExtractor(\n        mdl,\n        X,\n        Y=Y,\n        feature_names=feature_names,\n        feature_types=feature_types,\n        feature_categories=feature_categories,\n        feature_constraints=feature_constraints,\n        max_candidates=max_candidates,\n        tol=tol,\n        target_name=target_name,\n        target_labels=target_labels,\n        lime_approximation=lime_approximation,\n        n_samples=n_samples,\n        alpha=alpha,\n    )\n    self.cost_ = Cost(\n        X,\n        Y,\n        feature_types=feature_types,\n        feature_categories=feature_categories,\n        feature_constraints=feature_constraints,\n        max_candidates=max_candidates,\n        tol=tol,\n    )\n\n    self.lime_approximation_ = lime_approximation\n    if lime_approximation:\n        self.lime_ = LimeEstimator(\n            mdl,\n            X,\n            n_samples=n_samples,\n            feature_types=feature_types,\n            feature_categories=feature_categories,\n            alpha=alpha,\n        )\n\n    self.max_iteration_ = max_iteration\n    self.max_depth_ = max_depth\n    self.min_samples_leaf_ = min_samples_leaf\n    self.remain_redundant_leaf_ = remain_redundant_leaf\n    self.feature_names_ = (\n        feature_names\n        if len(feature_names) == X.shape[1]\n        else [\"x_{}\".format(d) for d in range(X.shape[1])]\n    )\n    self.feature_types_ = (\n        feature_types if len(feature_types) == X.shape[1] else [\"C\" for d in range(X.shape[1])]\n    )\n    self.feature_categories_ = feature_categories\n    self.feature_categories_flatten_ = flatten(feature_categories)\n    self.feature_constraints_ = (\n        feature_constraints\n        if len(feature_constraints) == X.shape[1]\n        else [\"\" for d in range(X.shape[1])]\n    )\n    self.target_name_ = target_name\n    self.target_labels_ = target_labels\n    self.tol_ = tol\n    self.infeasible_ = False\n    self.feature_categories_inv_ = []\n    for d in range(X.shape[1]):\n        g = -1\n        if self.feature_types_[d] == \"B\":\n            for i, cat in enumerate(self.feature_categories_):\n                if d in cat:\n                    g = i\n                    break\n        self.feature_categories_inv_.append(g)\n\n    if use_mined_rules:\n        self.discretizer_ = FrequentRuleMiner(minsup=minsup, discretization=True)\n        self.discretizer_ = self.discretizer_.fit(\n            X,\n            feature_names=feature_names,\n            feature_types=feature_types,\n            discretization_bins=discretization_bins,\n        )\n        self.rule_names_ = self.discretizer_.rule_names_\n        self.R_ = len(self.rule_names_)\n        self.rule_length_ = self.discretizer_.L_\n    else:\n        self.discretizer_ = FeatureDiscretizer(bins=discretization_bins, onehot=False)\n        self.discretizer_ = self.discretizer_.fit(\n            X, feature_names=feature_names, feature_types=feature_types\n        )\n        self.rule_names_ = self.discretizer_.feature_names\n        self.R_ = len(self.rule_names_)\n        self.rule_length_ = np.ones(self.R_)\n    self.rule_probability_ = (1 / self.rule_length_) / (1 / self.rule_length_).sum()\n</code></pre>"},{"location":"methods/local/cet/#counterfactuals.cf_methods.local_methods.cet.cet.CounterfactualExplanationTree.explain","title":"explain","text":"<pre><code>explain(X, y_origin=None, y_target=None, X_train=None, **kwargs)\n</code></pre> <p>Wrapper to produce ExplanationResult for compatibility.</p> <p>This uses the existing get_counterfactuals method and returns an ExplanationResult dataclass.</p> Source code in <code>counterfactuals/cf_methods/local_methods/cet/cet.py</code> <pre><code>def explain(\n    self,\n    X: np.ndarray,\n    y_origin: np.ndarray | None = None,\n    y_target: np.ndarray | None = None,\n    X_train: np.ndarray | None = None,\n    **kwargs,\n) -&gt; ExplanationResult:\n    \"\"\"Wrapper to produce ExplanationResult for compatibility.\n\n    This uses the existing get_counterfactuals method and returns an\n    ExplanationResult dataclass.\n    \"\"\"\n    # If training data provided, fit (keeps backward compatibility)\n    if X_train is not None:\n        self.fit(X_train)\n\n    x_cfs = self.get_counterfactuals(X)\n    y_target_arr = (\n        np.array(y_target) if y_target is not None else np.zeros((X.shape[0],), dtype=int)\n    )\n    y_origin_arr = (\n        np.array(y_origin) if y_origin is not None else np.zeros((X.shape[0],), dtype=int)\n    )\n\n    return ExplanationResult(\n        x_cfs=np.array(x_cfs),\n        y_cf_targets=y_target_arr,\n        x_origs=np.array(X),\n        y_origs=y_origin_arr,\n        logs=None,\n    )\n</code></pre>"},{"location":"methods/local/cet/#counterfactuals.cf_methods.local_methods.cet.cet.CounterfactualExplanationTree.get_counterfactuals","title":"get_counterfactuals","text":"<pre><code>get_counterfactuals(X, return_costs=False)\n</code></pre> <p>Get counterfactual examples for input instances.</p>"},{"location":"methods/local/cet/#counterfactuals.cf_methods.local_methods.cet.cet.CounterfactualExplanationTree.get_counterfactuals--parameters","title":"Parameters","text":"<p>X : array-like of shape (n_samples, n_features)     Input instances to generate counterfactuals for return_costs : bool, default=False     Whether to return the costs associated with each counterfactual</p>"},{"location":"methods/local/cet/#counterfactuals.cf_methods.local_methods.cet.cet.CounterfactualExplanationTree.get_counterfactuals--returns","title":"Returns","text":"<p>counterfactuals : array-like of shape (n_samples, n_features)     Counterfactual examples costs : array-like of shape (n_samples,), optional     Costs associated with each counterfactual if return_costs=True</p> Source code in <code>counterfactuals/cf_methods/local_methods/cet/cet.py</code> <pre><code>def get_counterfactuals(self, X, return_costs=False):\n    \"\"\"\n    Get counterfactual examples for input instances.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Input instances to generate counterfactuals for\n    return_costs : bool, default=False\n        Whether to return the costs associated with each counterfactual\n\n    Returns\n    -------\n    counterfactuals : array-like of shape (n_samples, n_features)\n        Counterfactual examples\n    costs : array-like of shape (n_samples,), optional\n        Costs associated with each counterfactual if return_costs=True\n    \"\"\"\n    actions = self.predict(X)\n    counterfactuals = X + actions\n\n    if return_costs:\n        # Compute costs for each counterfactual\n        costs = np.array(\n            [self.cost_.compute(x, a, cost_type=self.cost_type_) for x, a in zip(X, actions)]\n        )\n        return counterfactuals, costs\n\n    return counterfactuals\n</code></pre>"},{"location":"methods/local/dice/","title":"DICE","text":"<p>Diverse Counterfactual Explanations</p> <p>DICE is a popular method for generating diverse counterfactual explanations, integrated via the <code>dice-ml</code> library.</p>"},{"location":"methods/local/dice/#overview","title":"Overview","text":"<p>DICE generates multiple diverse counterfactuals by optimizing for both validity and diversity simultaneously.</p>"},{"location":"methods/local/dice/#usage","title":"Usage","text":"<pre><code>from counterfactuals.cf_methods.local_methods import DICE\n\nmethod = DICE(\n    gen_model=gen_model,\n    disc_model=classifier,\n    disc_model_criterion=criterion,\n    device=\"cuda\"\n)\n\nresult = method.explain(\n    X=instance,\n    y_origin=0,\n    y_target=1,\n    X_train=X_train,\n    y_train=y_train\n)\n</code></pre>"},{"location":"methods/local/dice/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>disc_model</code> BaseClassifier required Trained classifier <code>n_counterfactuals</code> int 5 Number of CFs to generate"},{"location":"methods/local/dice/#references","title":"References","text":"<ul> <li>Mothilal et al., \"Explaining Machine Learning Classifiers through Diverse Counterfactual Explanations\"</li> </ul>"},{"location":"methods/local/dice/#api-reference","title":"API Reference","text":""},{"location":"methods/local/dice/#counterfactuals.cf_methods.local_methods.dice.dice.DICE","title":"DICE","text":"<pre><code>DICE(X_train, y_train, features, disc_model)\n</code></pre> <p>               Bases: <code>BaseCounterfactualMethod</code>, <code>LocalCounterfactualMixin</code></p> <p>An interface class to different DiCE implementations.</p> Source code in <code>counterfactuals/cf_methods/local_methods/dice/dice.py</code> <pre><code>def __init__(self, X_train, y_train, features, disc_model):\n    self.features = features\n    self.target_feature = \"target\"\n    input_dataframe = pd.DataFrame(\n        np.concatenate((X_train, y_train.reshape(-1, 1)), axis=1),\n        columns=features + [self.target_feature],\n    )\n\n    dice = dice_ml.Data(\n        dataframe=input_dataframe,\n        continuous_features=features[:-1],\n        outcome_name=features[-1],\n    )\n    model = dice_ml.Model(disc_model, backend=\"PYT\")\n\n    self.exp = dice_ml.Dice(dice, model, method=\"gradient\")\n</code></pre>"},{"location":"methods/local/dice/#counterfactuals.cf_methods.local_methods.dice.dice.DICE.explain","title":"explain","text":"<pre><code>explain(Xs, ys, total_CFs=1, desired_class='opposite', desired_range=None, permitted_range=None, features_to_vary='all', stopping_threshold=0.5, posthoc_sparsity_param=0.1, posthoc_sparsity_algorithm='linear', verbose=False, **kwargs)\n</code></pre> <p>General method for generating counterfactuals.</p> <p>:param query_instances: Input point(s) for which counterfactuals are to be generated.                         This can be a dataframe with one or more rows. :param total_CFs: Total number of counterfactuals required. :param desired_class: Desired counterfactual class - can take 0 or 1. Default value                       is \"opposite\" to the outcome class of query_instance for binary classification. :param desired_range: For regression problems. Contains the outcome range to                       generate counterfactuals in. This should be a list of two numbers in                       ascending order. :param permitted_range: Dictionary with feature names as keys and permitted range in list as values.                         Defaults to the range inferred from training data.                         If None, uses the parameters initialized in data_interface. :param features_to_vary: Either a string \"all\" or a list of feature names to vary. :param stopping_threshold: Minimum threshold for counterfactuals target class probability. :param proximity_weight: A positive float. Larger this weight, more close the counterfactuals are to the                          query_instance. Used by ['genetic', 'gradientdescent'],                          ignored by ['random', 'kdtree'] methods. :param sparsity_weight: A positive float. Larger this weight, less features are changed from the query_instance.                         Used by ['genetic', 'kdtree'], ignored by ['random', 'gradientdescent'] methods. :param diversity_weight: A positive float. Larger this weight, more diverse the counterfactuals are.                          Used by ['genetic', 'gradientdescent'], ignored by ['random', 'kdtree'] methods. :param categorical_penalty: A positive float. A weight to ensure that all levels of a categorical variable sums to 1.                          Used by ['genetic', 'gradientdescent'], ignored by ['random', 'kdtree'] methods. :param posthoc_sparsity_param: Parameter for the post-hoc operation on continuous features to enhance sparsity. :param posthoc_sparsity_algorithm: Perform either linear or binary search. Takes \"linear\" or \"binary\".                                    Prefer binary search when a feature range is large (for instance,                                    income varying from 10k to 1000k) and only if the features share a                                    monotonic relationship with predicted outcome in the model. :param verbose: Whether to output detailed messages. :param sample_size: Sampling size :param random_seed: Random seed for reproducibility :param kwargs: Other parameters accepted by specific explanation method</p> <p>:returns: A CounterfactualExplanations object that contains the list of           counterfactual examples per query_instance as one of its attributes.</p> Source code in <code>counterfactuals/cf_methods/local_methods/dice/dice.py</code> <pre><code>def explain(\n    self,\n    Xs,\n    ys,\n    total_CFs=1,\n    desired_class=\"opposite\",\n    desired_range=None,\n    permitted_range=None,\n    features_to_vary=\"all\",\n    stopping_threshold=0.5,\n    posthoc_sparsity_param=0.1,\n    posthoc_sparsity_algorithm=\"linear\",\n    verbose=False,\n    **kwargs,\n):\n    \"\"\"General method for generating counterfactuals.\n\n    :param query_instances: Input point(s) for which counterfactuals are to be generated.\n                            This can be a dataframe with one or more rows.\n    :param total_CFs: Total number of counterfactuals required.\n    :param desired_class: Desired counterfactual class - can take 0 or 1. Default value\n                          is \"opposite\" to the outcome class of query_instance for binary classification.\n    :param desired_range: For regression problems. Contains the outcome range to\n                          generate counterfactuals in. This should be a list of two numbers in\n                          ascending order.\n    :param permitted_range: Dictionary with feature names as keys and permitted range in list as values.\n                            Defaults to the range inferred from training data.\n                            If None, uses the parameters initialized in data_interface.\n    :param features_to_vary: Either a string \"all\" or a list of feature names to vary.\n    :param stopping_threshold: Minimum threshold for counterfactuals target class probability.\n    :param proximity_weight: A positive float. Larger this weight, more close the counterfactuals are to the\n                             query_instance. Used by ['genetic', 'gradientdescent'],\n                             ignored by ['random', 'kdtree'] methods.\n    :param sparsity_weight: A positive float. Larger this weight, less features are changed from the query_instance.\n                            Used by ['genetic', 'kdtree'], ignored by ['random', 'gradientdescent'] methods.\n    :param diversity_weight: A positive float. Larger this weight, more diverse the counterfactuals are.\n                             Used by ['genetic', 'gradientdescent'], ignored by ['random', 'kdtree'] methods.\n    :param categorical_penalty: A positive float. A weight to ensure that all levels of a categorical variable sums to 1.\n                             Used by ['genetic', 'gradientdescent'], ignored by ['random', 'kdtree'] methods.\n    :param posthoc_sparsity_param: Parameter for the post-hoc operation on continuous features to enhance sparsity.\n    :param posthoc_sparsity_algorithm: Perform either linear or binary search. Takes \"linear\" or \"binary\".\n                                       Prefer binary search when a feature range is large (for instance,\n                                       income varying from 10k to 1000k) and only if the features share a\n                                       monotonic relationship with predicted outcome in the model.\n    :param verbose: Whether to output detailed messages.\n    :param sample_size: Sampling size\n    :param random_seed: Random seed for reproducibility\n    :param kwargs: Other parameters accepted by specific explanation method\n\n    :returns: A CounterfactualExplanations object that contains the list of\n              counterfactual examples per query_instance as one of its attributes.\n    \"\"\"\n    query_instances = pd.DataFrame(Xs, columns=self.features)\n    dice_exp = self.exp.generate_counterfactuals(\n        query_instances,\n        total_CFs,\n        desired_class=desired_class,\n        desired_range=desired_range,\n        permitted_range=permitted_range,\n        features_to_vary=features_to_vary,\n        stopping_threshold=stopping_threshold,\n        posthoc_sparsity_param=posthoc_sparsity_param,\n        posthoc_sparsity_algorithm=posthoc_sparsity_algorithm,\n        verbose=verbose,\n        **kwargs,\n    )\n    coverage_mask = np.array([cf.final_cfs_df.shape[0] &gt; 0 for cf in dice_exp.cf_examples_list])\n    Xs_cfs = self.get_counterfactual(dice_exp)\n    Xs_cfs = np.array(Xs_cfs).squeeze()\n    Xs = np.array(Xs)[coverage_mask]\n    ys = np.array(ys)[coverage_mask]\n    ys_target = [desired_class] * len(Xs) if desired_class != \"opposite\" else np.abs(1 - ys)\n    return ExplanationResult(x_cfs=Xs_cfs, y_cf_targets=ys_target, x_origs=Xs, y_origs=ys)\n</code></pre>"},{"location":"methods/local/dice/#counterfactuals.cf_methods.local_methods.dice.dice.DICE.explain_dataloader","title":"explain_dataloader","text":"<pre><code>explain_dataloader(dataloader, total_CFs=1, desired_class='opposite', desired_range=None, permitted_range=None, features_to_vary='all', stopping_threshold=0.5, posthoc_sparsity_param=0.1, posthoc_sparsity_algorithm='linear', verbose=False, **kwargs)\n</code></pre> <p>General method for generating counterfactuals.</p> <p>:param query_instances: Input point(s) for which counterfactuals are to be generated.                         This can be a dataframe with one or more rows. :param total_CFs: Total number of counterfactuals required. :param desired_class: Desired counterfactual class - can take 0 or 1. Default value                       is \"opposite\" to the outcome class of query_instance for binary classification. :param desired_range: For regression problems. Contains the outcome range to                       generate counterfactuals in. This should be a list of two numbers in                       ascending order. :param permitted_range: Dictionary with feature names as keys and permitted range in list as values.                         Defaults to the range inferred from training data.                         If None, uses the parameters initialized in data_interface. :param features_to_vary: Either a string \"all\" or a list of feature names to vary. :param stopping_threshold: Minimum threshold for counterfactuals target class probability. :param proximity_weight: A positive float. Larger this weight, more close the counterfactuals are to the                          query_instance. Used by ['genetic', 'gradientdescent'],                          ignored by ['random', 'kdtree'] methods. :param sparsity_weight: A positive float. Larger this weight, less features are changed from the query_instance.                         Used by ['genetic', 'kdtree'], ignored by ['random', 'gradientdescent'] methods. :param diversity_weight: A positive float. Larger this weight, more diverse the counterfactuals are.                          Used by ['genetic', 'gradientdescent'], ignored by ['random', 'kdtree'] methods. :param categorical_penalty: A positive float. A weight to ensure that all levels of a categorical variable sums to 1.                          Used by ['genetic', 'gradientdescent'], ignored by ['random', 'kdtree'] methods. :param posthoc_sparsity_param: Parameter for the post-hoc operation on continuous features to enhance sparsity. :param posthoc_sparsity_algorithm: Perform either linear or binary search. Takes \"linear\" or \"binary\".                                    Prefer binary search when a feature range is large (for instance,                                    income varying from 10k to 1000k) and only if the features share a                                    monotonic relationship with predicted outcome in the model. :param verbose: Whether to output detailed messages. :param sample_size: Sampling size :param random_seed: Random seed for reproducibility :param kwargs: Other parameters accepted by specific explanation method</p> <p>:returns: A CounterfactualExplanations object that contains the list of           counterfactual examples per query_instance as one of its attributes.</p> Source code in <code>counterfactuals/cf_methods/local_methods/dice/dice.py</code> <pre><code>def explain_dataloader(\n    self,\n    dataloader,\n    total_CFs=1,\n    desired_class=\"opposite\",\n    desired_range=None,\n    permitted_range=None,\n    features_to_vary=\"all\",\n    stopping_threshold=0.5,\n    posthoc_sparsity_param=0.1,\n    posthoc_sparsity_algorithm=\"linear\",\n    verbose=False,\n    **kwargs,\n):\n    \"\"\"General method for generating counterfactuals.\n\n    :param query_instances: Input point(s) for which counterfactuals are to be generated.\n                            This can be a dataframe with one or more rows.\n    :param total_CFs: Total number of counterfactuals required.\n    :param desired_class: Desired counterfactual class - can take 0 or 1. Default value\n                          is \"opposite\" to the outcome class of query_instance for binary classification.\n    :param desired_range: For regression problems. Contains the outcome range to\n                          generate counterfactuals in. This should be a list of two numbers in\n                          ascending order.\n    :param permitted_range: Dictionary with feature names as keys and permitted range in list as values.\n                            Defaults to the range inferred from training data.\n                            If None, uses the parameters initialized in data_interface.\n    :param features_to_vary: Either a string \"all\" or a list of feature names to vary.\n    :param stopping_threshold: Minimum threshold for counterfactuals target class probability.\n    :param proximity_weight: A positive float. Larger this weight, more close the counterfactuals are to the\n                             query_instance. Used by ['genetic', 'gradientdescent'],\n                             ignored by ['random', 'kdtree'] methods.\n    :param sparsity_weight: A positive float. Larger this weight, less features are changed from the query_instance.\n                            Used by ['genetic', 'kdtree'], ignored by ['random', 'gradientdescent'] methods.\n    :param diversity_weight: A positive float. Larger this weight, more diverse the counterfactuals are.\n                             Used by ['genetic', 'gradientdescent'], ignored by ['random', 'kdtree'] methods.\n    :param categorical_penalty: A positive float. A weight to ensure that all levels of a categorical variable sums to 1.\n                             Used by ['genetic', 'gradientdescent'], ignored by ['random', 'kdtree'] methods.\n    :param posthoc_sparsity_param: Parameter for the post-hoc operation on continuous features to enhance sparsity.\n    :param posthoc_sparsity_algorithm: Perform either linear or binary search. Takes \"linear\" or \"binary\".\n                                       Prefer binary search when a feature range is large (for instance,\n                                       income varying from 10k to 1000k) and only if the features share a\n                                       monotonic relationship with predicted outcome in the model.\n    :param verbose: Whether to output detailed messages.\n    :param sample_size: Sampling size\n    :param random_seed: Random seed for reproducibility\n    :param kwargs: Other parameters accepted by specific explanation method\n\n    :returns: A CounterfactualExplanations object that contains the list of\n              counterfactual examples per query_instance as one of its attributes.\n    \"\"\"\n    Xs, ys = dataloader.dataset.tensors\n    Xs = Xs.numpy()[:5]\n    ys = ys.numpy()[:5]\n    Xs = pd.DataFrame(Xs, columns=self.features)\n    dice_exp = self.exp.generate_counterfactuals(\n        Xs,\n        total_CFs=total_CFs,\n        desired_class=desired_class,\n        desired_range=None,\n        permitted_range=permitted_range,\n        features_to_vary=features_to_vary,\n        stopping_threshold=stopping_threshold,\n        posthoc_sparsity_param=posthoc_sparsity_param,\n        posthoc_sparsity_algorithm=posthoc_sparsity_algorithm,\n        verbose=verbose,\n        **kwargs,\n    )\n    coverage_mask = np.array([cf.final_cfs_df.shape[0] &gt; 0 for cf in dice_exp.cf_examples_list])\n    Xs_cfs = self.get_counterfactual(dice_exp)\n    Xs_cfs = np.array(Xs_cfs).squeeze()\n    Xs = np.array(Xs)[coverage_mask]\n    ys = np.array(ys)[coverage_mask]\n    ys_target = [desired_class] * len(Xs) if desired_class != \"opposite\" else np.abs(1 - ys)\n    return ExplanationResult(x_cfs=Xs_cfs, y_cf_targets=ys_target, x_origs=Xs, y_origs=ys)\n</code></pre>"},{"location":"methods/local/dicoflex/","title":"DiCoFlex","text":"<p>Diverse Counterfactuals with Flexible Constraints</p> <p>DiCoFlex generates diverse counterfactual explanations while respecting flexible feature constraints.</p>"},{"location":"methods/local/dicoflex/#overview","title":"Overview","text":"<p>DiCoFlex extends standard counterfactual generation by:</p> <ol> <li>Producing multiple diverse counterfactuals</li> <li>Supporting flexible actionability constraints</li> <li>Balancing diversity with validity</li> </ol>"},{"location":"methods/local/dicoflex/#usage","title":"Usage","text":"<pre><code>from counterfactuals.cf_methods.local_methods.DiCoFlex import DiCoFlex\n\nmethod = DiCoFlex(\n    gen_model=gen_model,\n    disc_model=classifier,\n    disc_model_criterion=criterion,\n    device=\"cuda\"\n)\n\nresult = method.explain(\n    X=instance,\n    y_origin=0,\n    y_target=1,\n    X_train=X_train,\n    y_train=y_train\n)\n</code></pre>"},{"location":"methods/local/dicoflex/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>gen_model</code> BaseGenerator required Trained generative model <code>disc_model</code> BaseClassifier required Trained classifier <code>n_counterfactuals</code> int 5 Number of CFs to generate"},{"location":"methods/local/dicoflex/#api-reference","title":"API Reference","text":""},{"location":"methods/local/dicoflex/#counterfactuals.cf_methods.local_methods.DiCoFlex.method.DiCoFlex","title":"DiCoFlex","text":"<pre><code>DiCoFlex(gen_model, disc_model, class_to_index, mask_vectors, params, device=None)\n</code></pre> <p>               Bases: <code>BaseCounterfactualMethod</code>, <code>LocalCounterfactualMixin</code></p> <p>Sample-based counterfactual generator backed by a conditional flow.</p> Source code in <code>counterfactuals/cf_methods/local_methods/DiCoFlex/method.py</code> <pre><code>def __init__(\n    self,\n    gen_model: GenerativePytorchMixin,\n    disc_model: PytorchBase,\n    class_to_index: Dict[int, int],\n    mask_vectors: list[np.ndarray],\n    params: DiCoFlexParams,\n    device: Optional[str] = None,\n) -&gt; None:\n    super().__init__(\n        gen_model=gen_model,\n        disc_model=disc_model,\n        device=device,\n    )\n    if not mask_vectors:\n        raise ValueError(\"At least one mask vector must be supplied for DiCoFlex.\")\n    if params.mask_index &gt;= len(mask_vectors):\n        raise ValueError(\"mask_index exceeds available mask vectors.\")\n    if params.target_class not in class_to_index:\n        raise ValueError(\n            f\"Target class {params.target_class} not observed in the training data.\"\n        )\n    self.class_to_index = class_to_index\n    self.mask_vectors = mask_vectors\n    self.params = params\n    self.device = device or \"cpu\"\n    self.gen_model.to(self.device)\n    self.disc_model.to(self.device)\n</code></pre>"},{"location":"methods/local/dicoflex/#counterfactuals.cf_methods.local_methods.DiCoFlex.method.DiCoFlex.explain","title":"explain","text":"<pre><code>explain(X, y_origin, y_target, X_train=None, y_train=None, **kwargs)\n</code></pre> <p>Generate counterfactuals for the provided samples.</p> Source code in <code>counterfactuals/cf_methods/local_methods/DiCoFlex/method.py</code> <pre><code>def explain(\n    self,\n    X: np.ndarray,\n    y_origin: np.ndarray,\n    y_target: np.ndarray,\n    X_train: Optional[np.ndarray] = None,\n    y_train: Optional[np.ndarray] = None,\n    **kwargs,\n) -&gt; ExplanationResult:\n    \"\"\"Generate counterfactuals for the provided samples.\"\"\"\n    x_np = np.asarray(X, dtype=np.float32)\n    y_origin_vec = np.asarray(y_origin).reshape(-1, 1)\n    y_target_vec = np.asarray(y_target).reshape(-1, 1)\n    (\n        cf_batch,\n        y_target_flat,\n        x_orig_flat,\n        y_origin_flat,\n        target_probs,\n        valid_mask,\n        log_probs,\n        group_ids,\n    ) = self._sample_counterfactuals(x_np, y_origin_vec, y_target_vec)\n    logs = {\n        \"sampling/mean_target_probability\": float(target_probs.mean()),\n        \"sampling/valid_ratio\": float(valid_mask.mean()),\n        \"sampling/log_prob_mean\": float(log_probs.mean()),\n        \"model_returned_mask\": valid_mask.tolist(),\n        \"cf_group_ids\": group_ids.tolist(),\n    }\n    return ExplanationResult(\n        x_cfs=cf_batch,\n        y_cf_targets=y_target_flat,\n        x_origs=x_orig_flat,\n        y_origs=y_origin_flat,\n        logs=logs,\n        cf_group_ids=group_ids,\n    )\n</code></pre>"},{"location":"methods/local/dicoflex/#counterfactuals.cf_methods.local_methods.DiCoFlex.method.DiCoFlex.explain_dataloader","title":"explain_dataloader","text":"<pre><code>explain_dataloader(dataloader, epochs, lr, patience_eps=1e-05, **kwargs)\n</code></pre> <p>Adapter around explain() for DataLoader inputs.</p> Source code in <code>counterfactuals/cf_methods/local_methods/DiCoFlex/method.py</code> <pre><code>def explain_dataloader(\n    self,\n    dataloader,\n    epochs: int,\n    lr: float,\n    patience_eps: float = 1e-5,\n    **kwargs,\n) -&gt; ExplanationResult:\n    \"\"\"Adapter around explain() for DataLoader inputs.\"\"\"\n    xs, ys = [], []\n    for batch_x, batch_y in dataloader:\n        xs.append(batch_x.numpy())\n        ys.append(batch_y.numpy())\n    X = np.vstack(xs)\n    y_origin = np.concatenate(ys)\n    y_target = np.full_like(y_origin, fill_value=self.params.target_class)\n    return self.explain(\n        X=X,\n        y_origin=y_origin,\n        y_target=y_target,\n    )\n</code></pre>"},{"location":"methods/local/ppcef/","title":"PPCEF","text":"<p>Plausible Probabilistic Counterfactual Explanations with Flows</p> <p>PPCEF is the flagship method of this library, generating counterfactuals that are both valid and plausible by leveraging normalizing flows.</p>"},{"location":"methods/local/ppcef/#overview","title":"Overview","text":"<p>PPCEF optimizes counterfactuals to lie in high-density regions of the data distribution, ensuring they represent realistic inputs rather than adversarial examples.</p> <p>Key Innovation</p> <p>Unlike proximity-only methods, PPCEF uses a generative model (normalizing flow) to assess and maximize the plausibility of generated counterfactuals.</p>"},{"location":"methods/local/ppcef/#algorithm","title":"Algorithm","text":"<p>The method minimizes a combined objective:</p> \\[ \\mathcal{L} = \\alpha \\cdot \\mathcal{L}_{\\text{validity}} + \\beta \\cdot \\mathcal{L}_{\\text{proximity}} + \\gamma \\cdot \\mathcal{L}_{\\text{plausibility}} \\] <p>Where: - \\(\\mathcal{L}_{\\text{validity}}\\): Cross-entropy loss for target class - \\(\\mathcal{L}_{\\text{proximity}}\\): Distance to original instance - \\(\\mathcal{L}_{\\text{plausibility}}\\): Negative log-likelihood under the flow</p>"},{"location":"methods/local/ppcef/#usage","title":"Usage","text":"<pre><code>from counterfactuals.cf_methods.local_methods import PPCEF\nfrom counterfactuals.models.generators import MaskedAutoregressiveFlow\nfrom counterfactuals.models.classifiers import MLPClassifier\n\n# Initialize models\ngen_model = MaskedAutoregressiveFlow(...)\nclassifier = MLPClassifier(...)\n\n# Create PPCEF instance\nmethod = PPCEF(\n    gen_model=gen_model,\n    disc_model=classifier,\n    disc_model_criterion=torch.nn.CrossEntropyLoss(),\n    device=\"cuda\"\n)\n\n# Generate counterfactual\nresult = method.explain(\n    X=instance,\n    y_origin=0,\n    y_target=1,\n    X_train=X_train,\n    y_train=y_train,\n    epochs=100,\n    lr=0.01,\n    alpha=1.0,\n    beta=0.5\n)\n</code></pre>"},{"location":"methods/local/ppcef/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>gen_model</code> BaseGenerator required Trained generative model (flow) <code>disc_model</code> BaseClassifier required Trained classifier <code>epochs</code> int 100 Optimization iterations <code>lr</code> float 0.01 Learning rate <code>alpha</code> float 1.0 Validity loss weight <code>beta</code> float 0.5 Proximity loss weight"},{"location":"methods/local/ppcef/#strengths","title":"Strengths","text":"<ul> <li>High plausibility of generated counterfactuals</li> <li>Works well with tabular data</li> <li>Supports actionability constraints</li> </ul>"},{"location":"methods/local/ppcef/#limitations","title":"Limitations","text":"<ul> <li>Requires training a generative model</li> <li>Slower than simple optimization methods</li> <li>Performance depends on flow quality</li> </ul>"},{"location":"methods/local/ppcef/#references","title":"References","text":"<ul> <li>[Paper citation placeholder]</li> </ul>"},{"location":"methods/local/ppcef/#api-reference","title":"API Reference","text":""},{"location":"methods/local/ppcef/#counterfactuals.cf_methods.local_methods.ppcef.ppcef.PPCEF","title":"PPCEF","text":"<pre><code>PPCEF(gen_model, disc_model, disc_model_criterion, device=None)\n</code></pre> <p>               Bases: <code>BaseCounterfactualMethod</code>, <code>LocalCounterfactualMixin</code></p> Source code in <code>counterfactuals/cf_methods/local_methods/ppcef/ppcef.py</code> <pre><code>def __init__(\n    self,\n    gen_model: GenerativePytorchMixin,\n    disc_model: PytorchBase,\n    disc_model_criterion,\n    device=None,\n):\n    self.disc_model_criterion = disc_model_criterion\n    self.gen_model = gen_model\n    self.disc_model = disc_model\n    self.device = device if device is not None else \"cpu\"\n    self.gen_model.to(self.device)\n    self.disc_model.to(self.device)\n    self.beta = 0\n</code></pre>"},{"location":"methods/local/ppcef/#counterfactuals.cf_methods.local_methods.ppcef.ppcef.PPCEF.explain","title":"explain","text":"<pre><code>explain(X, y_origin, y_target, X_train, y_train)\n</code></pre> <p>Explains the model's prediction for a given input.</p> Source code in <code>counterfactuals/cf_methods/local_methods/ppcef/ppcef.py</code> <pre><code>def explain(\n    self,\n    X: np.ndarray,\n    y_origin: np.ndarray,\n    y_target: np.ndarray,\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n):\n    \"\"\"\n    Explains the model's prediction for a given input.\n    \"\"\"\n    raise NotImplementedError(\"This method is not implemented for this class.\")\n</code></pre>"},{"location":"methods/local/ppcef/#counterfactuals.cf_methods.local_methods.ppcef.ppcef.PPCEF.explain_dataloader","title":"explain_dataloader","text":"<pre><code>explain_dataloader(dataloader, epochs=1000, lr=0.0005, patience_eps=1e-05, **search_step_kwargs)\n</code></pre> <p>Search counterfactual explanations for the given dataloader.</p> Source code in <code>counterfactuals/cf_methods/local_methods/ppcef/ppcef.py</code> <pre><code>def explain_dataloader(\n    self,\n    dataloader: DataLoader,\n    epochs: int = 1000,\n    lr: float = 0.0005,\n    patience_eps: int = 1e-5,\n    **search_step_kwargs,\n):\n    \"\"\"\n    Search counterfactual explanations for the given dataloader.\n    \"\"\"\n    self.epochs = epochs\n    self.gen_model.eval()\n    for param in self.gen_model.parameters():\n        param.requires_grad = False\n\n    if self.disc_model:\n        self.disc_model.eval()\n        for param in self.disc_model.parameters():\n            param.requires_grad = False\n\n    deltas = []\n    target_class = []\n    original = []\n    original_class = []\n    for xs_origin, contexts_origin in dataloader:\n        xs_origin = xs_origin.to(self.device)\n        contexts_origin = contexts_origin.to(self.device)\n\n        contexts_origin = contexts_origin.reshape(-1, 1)\n        contexts_target = torch.abs(1 - contexts_origin)\n\n        xs_origin = torch.as_tensor(xs_origin)\n        xs_origin.requires_grad = False\n        delta = torch.zeros_like(xs_origin, requires_grad=True)\n\n        optimizer = optim.Adam([delta], lr=lr)\n        loss_components_logging = {}\n\n        for epoch in (epoch_pbar := tqdm(range(epochs))):\n            search_step_kwargs[\"epoch\"] = epoch\n            optimizer.zero_grad()\n            loss_components = self._search_step(\n                delta,\n                xs_origin,\n                contexts_origin,\n                contexts_target,\n                **search_step_kwargs,\n            )\n            mean_loss = loss_components[\"loss\"].mean()\n            mean_loss.backward()\n            optimizer.step()\n\n            for loss_name, loss in loss_components.items():\n                loss_components_logging.setdefault(f\"cf_search/{loss_name}\", []).append(\n                    loss.mean().detach().cpu().item()\n                )\n\n            disc_loss = loss_components[\"loss_disc\"].detach().cpu().mean().item()\n            prob_loss = loss_components[\"max_inner\"].detach().cpu().mean().item()\n            epoch_pbar.set_description(\n                f\"Discriminator loss: {disc_loss:.4f}, Prob loss: {prob_loss:.4f}\"\n            )\n            # if disc_loss &lt; patience_eps and prob_loss &lt; patience_eps:\n            #     break\n\n        deltas.append(delta.detach().cpu().numpy())\n        original.append(xs_origin.detach().cpu().numpy())\n        original_class.append(contexts_origin.detach().cpu().numpy())\n        target_class.append(contexts_target.detach().cpu().numpy())\n\n    deltas = np.concatenate(deltas, axis=0)\n    originals = np.concatenate(original, axis=0)\n    original_classes = np.concatenate(original_class, axis=0)\n    target_classes = np.concatenate(target_class, axis=0)\n    x_cfs = originals + deltas\n\n    return ExplanationResult(\n        x_cfs=x_cfs,\n        y_cf_targets=target_classes,\n        x_origs=originals,\n        y_origs=original_classes,\n        logs=loss_components_logging,\n    )\n</code></pre>"},{"location":"methods/local/sace/","title":"SACE","text":"<p>SACE Counterfactual Methods</p> <p>SACE provides multiple variants for generating counterfactual explanations.</p>"},{"location":"methods/local/sace/#variants","title":"Variants","text":"<ul> <li>Standard SACE: Base implementation</li> <li>Case-based SACE: Uses case-based reasoning</li> <li>Feature SACE: Feature-focused approach</li> <li>Random SACE: Randomized search</li> <li>Neighbor SACE: Neighbor-based generation</li> <li>Tree SACE: Tree-structured search</li> <li>Distribution SACE: Distribution-aware generation</li> </ul>"},{"location":"methods/local/sace/#usage","title":"Usage","text":"<pre><code>from counterfactuals.cf_methods.local_methods.sace import SACE\n\nmethod = SACE(\n    gen_model=gen_model,\n    disc_model=classifier,\n    disc_model_criterion=criterion,\n    device=\"cuda\"\n)\n\nresult = method.explain(\n    X=instance,\n    y_origin=0,\n    y_target=1,\n    X_train=X_train,\n    y_train=y_train\n)\n</code></pre>"},{"location":"methods/local/sace/#api-reference","title":"API Reference","text":""},{"location":"methods/local/sace/#counterfactuals.cf_methods.local_methods.sace.sace.SACE","title":"SACE","text":"<pre><code>SACE(variable_features=None, weights=None, metric='euclidean', feature_names=None, continuous_features=None, categorical_features_index_lists=None, normalize=True, pooler=None, tol=0.01)\n</code></pre> <p>               Bases: <code>ABC</code></p> Source code in <code>counterfactuals/cf_methods/local_methods/sace/sace.py</code> <pre><code>def __init__(\n    self,\n    variable_features=None,\n    weights=None,\n    metric=\"euclidean\",\n    feature_names=None,\n    continuous_features=None,\n    categorical_features_index_lists=None,\n    normalize=True,\n    pooler=None,\n    tol=0.01,\n):\n    self.variable_features = variable_features\n    self.weights = weights\n    self.metric = metric\n    self.feature_names = feature_names\n    self.continuous_features = continuous_features\n    self.categorical_features_index_lists = categorical_features_index_lists\n    self.normalize = normalize\n    self.pooler = pooler\n    self.tol = tol\n\n    self.b = None\n    self.X = None\n    self.y = None\n    self.nbr_features = None\n    self.non_variable_features = None\n    self.nbr_variable_features = None\n    self.scaler = None\n    self.nX = None\n</code></pre>"},{"location":"methods/local/sace/#counterfactuals.cf_methods.local_methods.sace.sace.SACE.fit","title":"fit  <code>abstractmethod</code>","text":"<pre><code>fit(b, X)\n</code></pre> <p>:param b: black box predict function :param X: training set of b :param V: list of features to vary</p> Source code in <code>counterfactuals/cf_methods/local_methods/sace/sace.py</code> <pre><code>@abstractmethod\ndef fit(self, b, X):\n    \"\"\"\n\n    :param b: black box predict function\n    :param X: training set of b\n    :param V: list of features to vary\n    \"\"\"\n\n    self.b = b\n    self.y = self.b.predict(X)\n\n    if self.pooler:\n        X_p = self.pooler.transform(X)\n        self.X = X_p\n    else:\n        self.X = X\n\n    self.nbr_features = self.X.shape[1]\n    self.variable_features = (\n        self.variable_features\n        if self.variable_features is not None\n        else np.arange(self.nbr_features).tolist()\n    )\n    self.non_variable_features = [\n        i for i in range(self.nbr_features) if i not in self.variable_features\n    ]\n    self.nbr_variable_features = len(self.variable_features)\n\n    self.scaler = StandardScaler() if self.normalize else DummyScaler()\n    self.scaler.fit(self.X)\n    self.nX = self.scaler.transform(self.X)\n\n    self.__init_cont_cat_features(self.continuous_features)\n    self.__detect_ranges()\n</code></pre>"},{"location":"methods/local/wach/","title":"WACH","text":"<p>Weighted Actionable Counterfactual Explanations</p> <p>WACH focuses on generating actionable counterfactuals with weighted feature importance.</p>"},{"location":"methods/local/wach/#overview","title":"Overview","text":"<p>WACH emphasizes actionability by weighting features based on their modifiability.</p>"},{"location":"methods/local/wach/#usage","title":"Usage","text":"<pre><code>from counterfactuals.cf_methods.local_methods import WACH\n\nmethod = WACH(\n    gen_model=gen_model,\n    disc_model=classifier,\n    disc_model_criterion=criterion,\n    device=\"cuda\"\n)\n\nresult = method.explain(\n    X=instance,\n    y_origin=0,\n    y_target=1,\n    X_train=X_train,\n    y_train=y_train\n)\n</code></pre>"},{"location":"methods/local/wach/#api-reference","title":"API Reference","text":""},{"location":"methods/local/wach/#counterfactuals.cf_methods.local_methods.wach.wach.WACH","title":"WACH","text":"<pre><code>WACH(disc_model, target_class='other', **kwargs)\n</code></pre> <p>               Bases: <code>BaseCounterfactualMethod</code>, <code>LocalCounterfactualMixin</code></p> Source code in <code>counterfactuals/cf_methods/local_methods/wach/wach.py</code> <pre><code>def __init__(\n    self,\n    disc_model: PytorchBase,\n    target_class: int = \"other\",  # any class other than origin will do\n    **kwargs,  # ignore other arguments\n) -&gt; None:\n    tf.compat.v1.disable_eager_execution()\n    target_proba = 1.0\n    tol = 0.51  # want counterfactuals with p(class)&gt;0.99\n    self.target_class = target_class\n    max_iter = 1000\n    lam_init = 1e-1\n    max_lam_steps = 10\n    learning_rate_init = 0.1\n    predict_proba = lambda x: disc_model.predict_proba(x).numpy()  # noqa: E731\n    num_features = disc_model.input_size\n\n    # TODO: Change in future to allow for different feature ranges\n    feature_range = (0, 1)\n\n    self.cf = Counterfactual(\n        predict_proba,\n        shape=(1, num_features),\n        target_proba=target_proba,\n        tol=tol,\n        target_class=target_class,\n        max_iter=max_iter,\n        lam_init=lam_init,\n        max_lam_steps=max_lam_steps,\n        learning_rate_init=learning_rate_init,\n        feature_range=feature_range,\n    )\n</code></pre>"},{"location":"reference/","title":"API Reference","text":"<p>Complete API documentation for the Counterfactuals library.</p> <p>The API reference documentation is auto-generated from source code docstrings.</p>"},{"location":"reference/#core-modules","title":"Core Modules","text":""},{"location":"reference/#counterfactual-methods","title":"Counterfactual Methods","text":"<p>The main counterfactual explanation methods.</p>"},{"location":"reference/#counterfactuals.cf_methods.counterfactual_base","title":"counterfactual_base","text":""},{"location":"reference/#counterfactuals.cf_methods.counterfactual_base.ExplanationResult","title":"ExplanationResult  <code>dataclass</code>","text":"<pre><code>ExplanationResult(x_cfs, y_cf_targets, x_origs, y_origs, logs=None, cf_group_ids=None)\n</code></pre> <p>Data structure for storing the result of a counterfactual explanation.</p> <p>This dataclass encapsulates all the important outputs from a counterfactual explanation process, including the generated counterfactuals, their targets, the original instances, and any additional logging information.</p> <p>Attributes:</p> Name Type Description <code>x_cfs</code> <code>ndarray</code> <p>Generated counterfactual examples.</p> <code>y_cf_targets</code> <code>ndarray</code> <p>Target labels/values for the counterfactuals.</p> <code>x_origs</code> <code>ndarray</code> <p>Original input instances.</p> <code>y_origs</code> <code>ndarray</code> <p>Original labels/values for the input instances.</p> <code>logs</code> <code>Optional[Dict[str, Any]]</code> <p>Additional logging information such as loss curves, convergence metrics, or method-specific data.</p>"},{"location":"reference/#counterfactuals.cf_methods.counterfactual_base.BaseCounterfactualMethod","title":"BaseCounterfactualMethod","text":"<pre><code>BaseCounterfactualMethod(gen_model=None, disc_model=None, disc_model_criterion=None, device=None, **kwargs)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Abstract base class for all counterfactual explanation methods.</p> <p>This class defines the interface that all counterfactual methods must implement. It provides a consistent API for fitting, explaining, and generating counterfactuals across different methodological approaches.</p> <p>The class supports both individual explanations and batch processing through DataLoader objects, making it suitable for various use cases from single instance explanations to large-scale evaluations.</p> <p>Attributes:</p> Name Type Description <code>gen_model</code> <p>Generative model used for counterfactual generation (if applicable).</p> <code>disc_model</code> <code>PytorchBase</code> <p>Discriminative/classification model to be explained.</p> <code>disc_model_criterion</code> <p>Loss function for the discriminative model.</p> <code>device</code> <code>str</code> <p>Computing device ('cpu' or 'cuda') for PyTorch operations.</p> <p>Parameters:</p> Name Type Description Default <code>gen_model</code> <code>Optional[Any]</code> <p>Generative model for CF generation. Can be None for methods that don't use generative models.</p> <code>None</code> <code>disc_model</code> <code>Optional[PytorchBase]</code> <p>The model to be explained. Should be a PyTorch-based model wrapped in our PytorchBase interface.</p> <code>None</code> <code>disc_model_criterion</code> <code>Optional[Any]</code> <p>Loss function for the discriminative model. Required by optimization-based methods.</p> <code>None</code> <code>device</code> <code>Optional[str]</code> <p>Device for computation. Defaults to 'cpu'.</p> <code>None</code> <code>**kwargs</code> <p>Additional method-specific parameters.</p> <code>{}</code> Source code in <code>counterfactuals/cf_methods/counterfactual_base.py</code> <pre><code>def __init__(\n    self,\n    gen_model: Optional[Any] = None,\n    disc_model: Optional[PytorchBase] = None,\n    disc_model_criterion: Optional[Any] = None,\n    device: Optional[str] = None,\n    **kwargs,\n) -&gt; None:\n    \"\"\"\n    Initialize the counterfactual method.\n\n    Args:\n        gen_model (Optional[Any]): Generative model for CF generation. Can be None\n            for methods that don't use generative models.\n        disc_model (Optional[PytorchBase]): The model to be explained. Should be\n            a PyTorch-based model wrapped in our PytorchBase interface.\n        disc_model_criterion (Optional[Any]): Loss function for the discriminative\n            model. Required by optimization-based methods.\n        device (Optional[str]): Device for computation. Defaults to 'cpu'.\n        **kwargs: Additional method-specific parameters.\n    \"\"\"\n    self.gen_model = gen_model\n    self.disc_model = disc_model\n    self.disc_model_criterion = disc_model_criterion\n    self.device = device or \"cpu\"\n\n    # Move models to device if they exist and have a .to() method\n    if self.gen_model is not None and hasattr(self.gen_model, \"to\"):\n        self.gen_model.to(self.device)\n    if self.disc_model is not None and hasattr(self.disc_model, \"to\"):\n        self.disc_model.to(self.device)\n</code></pre>"},{"location":"reference/#counterfactuals.cf_methods.counterfactual_base.BaseCounterfactualMethod.fit","title":"fit","text":"<pre><code>fit(X_train, y_train, **kwargs)\n</code></pre> <p>Fit the counterfactual method on training data.</p> <p>This method allows the counterfactual explanation method to learn from training data. This might involve training auxiliary models, learning data distributions, or other preparatory steps.</p> <p>Parameters:</p> Name Type Description Default <code>X_train</code> <code>ndarray</code> <p>Training features with shape (n_samples, n_features).</p> required <code>y_train</code> <code>ndarray</code> <p>Training labels with shape (n_samples,).</p> required <code>**kwargs</code> <p>Additional method-specific parameters.</p> <code>{}</code> Source code in <code>counterfactuals/cf_methods/counterfactual_base.py</code> <pre><code>def fit(self, X_train: np.ndarray, y_train: np.ndarray, **kwargs) -&gt; None:\n    \"\"\"\n    Fit the counterfactual method on training data.\n\n    This method allows the counterfactual explanation method to learn\n    from training data. This might involve training auxiliary models,\n    learning data distributions, or other preparatory steps.\n\n    Args:\n        X_train (np.ndarray): Training features with shape (n_samples, n_features).\n        y_train (np.ndarray): Training labels with shape (n_samples,).\n        **kwargs: Additional method-specific parameters.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/#counterfactuals.cf_methods.counterfactual_base.BaseCounterfactualMethod.explain","title":"explain  <code>abstractmethod</code>","text":"<pre><code>explain(X, y_origin, y_target, X_train=None, y_train=None, **kwargs)\n</code></pre> <p>Generate counterfactual explanations for given instances.</p> <p>This is the core method that generates counterfactual explanations for input instances. It should return counterfactuals that, when passed through the model, produce the desired target outcomes.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Input instances to explain with shape (n_instances, n_features).</p> required <code>y_origin</code> <code>ndarray</code> <p>Original predictions/labels for X with shape (n_instances,).</p> required <code>y_target</code> <code>ndarray</code> <p>Desired target predictions/labels with shape (n_instances,).</p> required <code>X_train</code> <code>Optional[ndarray]</code> <p>Training data, if needed by the method.</p> <code>None</code> <code>y_train</code> <code>Optional[ndarray]</code> <p>Training labels, if needed by the method.</p> <code>None</code> <code>**kwargs</code> <p>Additional method-specific parameters.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ExplanationResult</code> <code>ExplanationResult</code> <p>Object containing counterfactuals, targets, originals, and any additional logging information.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the method is not implemented by the subclass.</p> Source code in <code>counterfactuals/cf_methods/counterfactual_base.py</code> <pre><code>@abstractmethod\ndef explain(\n    self,\n    X: np.ndarray,\n    y_origin: np.ndarray,\n    y_target: np.ndarray,\n    X_train: Optional[np.ndarray] = None,\n    y_train: Optional[np.ndarray] = None,\n    **kwargs,\n) -&gt; ExplanationResult:\n    \"\"\"\n    Generate counterfactual explanations for given instances.\n\n    This is the core method that generates counterfactual explanations\n    for input instances. It should return counterfactuals that, when\n    passed through the model, produce the desired target outcomes.\n\n    Args:\n        X (np.ndarray): Input instances to explain with shape (n_instances, n_features).\n        y_origin (np.ndarray): Original predictions/labels for X with shape (n_instances,).\n        y_target (np.ndarray): Desired target predictions/labels with shape (n_instances,).\n        X_train (Optional[np.ndarray]): Training data, if needed by the method.\n        y_train (Optional[np.ndarray]): Training labels, if needed by the method.\n        **kwargs: Additional method-specific parameters.\n\n    Returns:\n        ExplanationResult: Object containing counterfactuals, targets, originals,\n            and any additional logging information.\n\n    Raises:\n        NotImplementedError: If the method is not implemented by the subclass.\n    \"\"\"\n    raise NotImplementedError(\"Subclasses must implement the explain method\")\n</code></pre>"},{"location":"reference/#counterfactuals.cf_methods.counterfactual_base.BaseCounterfactualMethod.explain_dataloader","title":"explain_dataloader  <code>abstractmethod</code>","text":"<pre><code>explain_dataloader(dataloader, epochs, lr, patience_eps=1e-05, **search_step_kwargs)\n</code></pre> <p>Generate counterfactual explanations for data provided via DataLoader.</p> <p>This method is designed for batch processing of counterfactual generation, particularly useful for optimization-based methods that require iterative search procedures. It processes data in batches and typically involves gradient-based optimization.</p> <p>Parameters:</p> Name Type Description Default <code>dataloader</code> <code>DataLoader</code> <p>PyTorch DataLoader containing (X, y) pairs where X are instances to explain and y are their labels.</p> required <code>epochs</code> <code>int</code> <p>Maximum number of optimization epochs per instance.</p> required <code>lr</code> <code>float</code> <p>Learning rate for optimization procedures.</p> required <code>patience_eps</code> <code>Union[float, int]</code> <p>Convergence threshold. When loss drops below this value, optimization can terminate early.</p> <code>1e-05</code> <code>**search_step_kwargs</code> <p>Additional parameters passed to the search step function, such as regularization weights, constraints, etc.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ExplanationResult</code> <code>ExplanationResult</code> <p>Object containing all generated counterfactuals, their targets, original instances, and detailed logging information including loss curves and convergence metrics.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the method is not implemented by the subclass.</p> Source code in <code>counterfactuals/cf_methods/counterfactual_base.py</code> <pre><code>@abstractmethod\ndef explain_dataloader(\n    self,\n    dataloader: DataLoader,\n    epochs: int,\n    lr: float,\n    patience_eps: Union[float, int] = 1e-5,\n    **search_step_kwargs,\n) -&gt; ExplanationResult:\n    \"\"\"\n    Generate counterfactual explanations for data provided via DataLoader.\n\n    This method is designed for batch processing of counterfactual generation,\n    particularly useful for optimization-based methods that require iterative\n    search procedures. It processes data in batches and typically involves\n    gradient-based optimization.\n\n    Args:\n        dataloader (DataLoader): PyTorch DataLoader containing (X, y) pairs\n            where X are instances to explain and y are their labels.\n        epochs (int): Maximum number of optimization epochs per instance.\n        lr (float): Learning rate for optimization procedures.\n        patience_eps (Union[float, int]): Convergence threshold. When loss\n            drops below this value, optimization can terminate early.\n        **search_step_kwargs: Additional parameters passed to the search\n            step function, such as regularization weights, constraints, etc.\n\n    Returns:\n        ExplanationResult: Object containing all generated counterfactuals,\n            their targets, original instances, and detailed logging information\n            including loss curves and convergence metrics.\n\n    Raises:\n        NotImplementedError: If the method is not implemented by the subclass.\n    \"\"\"\n    raise NotImplementedError(\"Subclasses must implement the explain_dataloader method\")\n</code></pre>"},{"location":"reference/#datasets","title":"Datasets","text":"<p>Dataset loading and configuration utilities.</p>"},{"location":"reference/#counterfactuals.datasets.file_dataset","title":"file_dataset","text":""},{"location":"reference/#counterfactuals.datasets.file_dataset.FileDataset","title":"FileDataset","text":"<pre><code>FileDataset(config_path, samples_keep=None)\n</code></pre> <p>               Bases: <code>DatasetBase</code></p> <p>File dataset loader compatible with DatasetBase.</p> <pre><code>config_path: Path to the dataset configuration file.\ndataset_name: Optional name for the dataset (used for model paths).\n</code></pre> Source code in <code>counterfactuals/datasets/file_dataset.py</code> <pre><code>def __init__(\n    self,\n    config_path: Path,\n    samples_keep: Optional[int] = None,\n):\n    \"\"\"Initializes the File dataset with OmegaConf config.\n    Args:\n        config_path: Path to the dataset configuration file.\n        dataset_name: Optional name for the dataset (used for model paths).\n    \"\"\"\n    super().__init__(config_path=config_path)\n    self.samples_keep = samples_keep if samples_keep is not None else self.config.samples_keep\n    self.initial_transform_pipeline: Optional[InitialTransformPipeline] = (\n        build_initial_transform_pipeline(self.config.initial_transforms)\n    )\n    self.one_hot_feature_groups: dict[str, list[str]] = {}\n\n    raw_data = self._load_csv(self.config.raw_data_path)\n    context = self._apply_initial_transforms(raw_data)\n\n    if self.samples_keep &gt; 0 and len(context.data) &gt; self.samples_keep:\n        context.data = context.data.sample(self.samples_keep, random_state=42).reset_index(\n            drop=True\n        )\n\n    self.raw_data = context.data\n    self._update_metadata_from_context(context)\n    self.X, self.y = self.preprocess(self.raw_data)\n</code></pre>"},{"location":"reference/#counterfactuals.datasets.file_dataset.FileDataset.preprocess","title":"preprocess","text":"<pre><code>preprocess(raw_data)\n</code></pre> <p>Preprocesses raw data into feature and target arrays.</p> <p>Parameters:</p> Name Type Description Default <code>raw_data</code> <code>DataFrame</code> <p>Raw dataset as a pandas DataFrame.</p> required <p>Returns:</p> Type Description <code>tuple[ndarray, ndarray]</code> <p>Tuple (X, y) as numpy arrays.</p> Source code in <code>counterfactuals/datasets/file_dataset.py</code> <pre><code>def preprocess(self, raw_data: pd.DataFrame) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Preprocesses raw data into feature and target arrays.\n\n    Args:\n        raw_data: Raw dataset as a pandas DataFrame.\n\n    Returns:\n        Tuple (X, y) as numpy arrays.\n    \"\"\"\n    data = raw_data.copy()\n    if self.config.target_mapping:\n        data[self.config.target] = data[self.config.target].replace(self.config.target_mapping)\n\n    X = data[self.features].to_numpy()\n    y = data[self.config.target].to_numpy()\n    self.X, self.y = X, y\n    return X, y\n</code></pre>"},{"location":"reference/#models","title":"Models","text":""},{"location":"reference/#classifiers","title":"Classifiers","text":""},{"location":"reference/#counterfactuals.models.classifier.logistic_regression","title":"logistic_regression","text":""},{"location":"reference/#counterfactuals.models.classifier.multilayer_perceptron","title":"multilayer_perceptron","text":""},{"location":"reference/#generative-models","title":"Generative Models","text":""},{"location":"reference/#counterfactuals.models.generative.kde","title":"kde","text":"<p>Implementations of various mixture models.</p>"},{"location":"reference/#counterfactuals.models.generative.kde.GenerativeModel","title":"GenerativeModel","text":"<p>               Bases: <code>ABC</code>, <code>Module</code></p> <p>Base class inherited by all generative models in pytorch-generative.</p> Provides <ul> <li>An abstract <code>sample()</code> method which is implemented by subclasses that support   generating samples.</li> <li>Variables <code>self._c, self._h, self._w</code> which store the shape of the (first)   image Tensor the model was trained with. Note that <code>forward()</code> must have been   called at least once and the input must be an image for these variables to be   available.</li> <li>A <code>device</code> property which returns the device of the model's parameters.</li> </ul>"},{"location":"reference/#counterfactuals.models.generative.kde.GenerativeModel.__call__","title":"__call__","text":"<pre><code>__call__(x, *args, **kwargs)\n</code></pre> <p>Saves input tensor attributes so they can be accessed during sampling.</p> Source code in <code>counterfactuals/models/generative/kde.py</code> <pre><code>def __call__(self, x, *args, **kwargs):\n    \"\"\"Saves input tensor attributes so they can be accessed during sampling.\"\"\"\n    if getattr(self, \"_c\", None) is None and x.dim() == 4:\n        _, c, h, w = x.shape\n        self._create_shape_buffers(c, h, w)\n    return super().__call__(x, *args, **kwargs)\n</code></pre>"},{"location":"reference/#counterfactuals.models.generative.kde.GenerativeModel.load_state_dict","title":"load_state_dict","text":"<pre><code>load_state_dict(state_dict, strict=True)\n</code></pre> <p>Registers dynamic buffers before loading the model state.</p> Source code in <code>counterfactuals/models/generative/kde.py</code> <pre><code>def load_state_dict(self, state_dict, strict=True):\n    \"\"\"Registers dynamic buffers before loading the model state.\"\"\"\n    if \"_c\" in state_dict and not getattr(self, \"_c\", None):\n        c, h, w = state_dict[\"_c\"], state_dict[\"_h\"], state_dict[\"_w\"]\n        self._create_shape_buffers(c, h, w)\n    super().load_state_dict(state_dict, strict)\n</code></pre>"},{"location":"reference/#counterfactuals.models.generative.kde.Kernel","title":"Kernel","text":"<pre><code>Kernel(bandwidth=1.0)\n</code></pre> <p>               Bases: <code>ABC</code>, <code>Module</code></p> <p>Base class which defines the interface for all kernels.</p> <p>Parameters:</p> Name Type Description Default <code>bandwidth</code> <p>The kernel's (band)width.</p> <code>1.0</code> Source code in <code>counterfactuals/models/generative/kde.py</code> <pre><code>def __init__(self, bandwidth=1.0):\n    \"\"\"Initializes a new Kernel.\n\n    Args:\n        bandwidth: The kernel's (band)width.\n    \"\"\"\n    super().__init__()\n    self.bandwidth = bandwidth\n</code></pre>"},{"location":"reference/#counterfactuals.models.generative.kde.Kernel.forward","title":"forward  <code>abstractmethod</code>","text":"<pre><code>forward(test_Xs, train_Xs)\n</code></pre> <p>Computes log p(x) for each x in test_Xs given train_Xs.</p> Source code in <code>counterfactuals/models/generative/kde.py</code> <pre><code>@abc.abstractmethod\ndef forward(self, test_Xs, train_Xs):\n    \"\"\"Computes log p(x) for each x in test_Xs given train_Xs.\"\"\"\n</code></pre>"},{"location":"reference/#counterfactuals.models.generative.kde.Kernel.sample","title":"sample  <code>abstractmethod</code>","text":"<pre><code>sample(train_Xs)\n</code></pre> <p>Generates samples from the kernel distribution.</p> Source code in <code>counterfactuals/models/generative/kde.py</code> <pre><code>@abc.abstractmethod\ndef sample(self, train_Xs):\n    \"\"\"Generates samples from the kernel distribution.\"\"\"\n</code></pre>"},{"location":"reference/#counterfactuals.models.generative.kde.ParzenWindowKernel","title":"ParzenWindowKernel","text":"<pre><code>ParzenWindowKernel(bandwidth=1.0)\n</code></pre> <p>               Bases: <code>Kernel</code></p> <p>Implementation of the Parzen window kernel.</p> Source code in <code>counterfactuals/models/generative/kde.py</code> <pre><code>def __init__(self, bandwidth=1.0):\n    \"\"\"Initializes a new Kernel.\n\n    Args:\n        bandwidth: The kernel's (band)width.\n    \"\"\"\n    super().__init__()\n    self.bandwidth = bandwidth\n</code></pre>"},{"location":"reference/#counterfactuals.models.generative.kde.GaussianKernel","title":"GaussianKernel","text":"<pre><code>GaussianKernel(bandwidth=1.0)\n</code></pre> <p>               Bases: <code>Kernel</code></p> <p>Implementation of the Gaussian kernel.</p> Source code in <code>counterfactuals/models/generative/kde.py</code> <pre><code>def __init__(self, bandwidth=1.0):\n    \"\"\"Initializes a new Kernel.\n\n    Args:\n        bandwidth: The kernel's (band)width.\n    \"\"\"\n    super().__init__()\n    self.bandwidth = bandwidth\n</code></pre>"},{"location":"reference/#counterfactuals.models.generative.kde.KernelDensityEstimator","title":"KernelDensityEstimator","text":"<pre><code>KernelDensityEstimator(train_Xs, kernel=None)\n</code></pre> <p>               Bases: <code>GenerativeModel</code></p> <p>The KernelDensityEstimator model.</p> <p>Parameters:</p> Name Type Description Default <code>train_Xs</code> <p>The \"training\" data to use when estimating probabilities.</p> required <code>kernel</code> <p>The kernel to place on each of the train_Xs.</p> <code>None</code> Source code in <code>counterfactuals/models/generative/kde.py</code> <pre><code>def __init__(self, train_Xs, kernel=None):\n    \"\"\"Initializes a new KernelDensityEstimator.\n\n    Args:\n        train_Xs: The \"training\" data to use when estimating probabilities.\n        kernel: The kernel to place on each of the train_Xs.\n    \"\"\"\n    super().__init__()\n    self.kernel = kernel or GaussianKernel()\n    self.train_Xs = nn.Parameter(train_Xs, requires_grad=False)\n    assert len(self.train_Xs.shape) == 2, \"Input cannot have more than two axes.\"\n</code></pre>"},{"location":"reference/#counterfactuals.models.generative.kde.KDE","title":"KDE","text":"<pre><code>KDE(bandwidth=0.1, **kwargs)\n</code></pre> <p>               Bases: <code>PytorchBase</code>, <code>GenerativePytorchMixin</code></p> Source code in <code>counterfactuals/models/generative/kde.py</code> <pre><code>def __init__(self, bandwidth: float = 0.1, **kwargs):\n    super(KDE, self).__init__(None, None)\n    self.bandwidth = bandwidth\n    self.models = nn.ModuleDict()\n</code></pre>"},{"location":"reference/#counterfactuals.models.generative.kde.KDE.forward","title":"forward","text":"<pre><code>forward(x, context=None)\n</code></pre> <p>Forward pass with optional context for compatibility.</p> Source code in <code>counterfactuals/models/generative/kde.py</code> <pre><code>def forward(self, x: torch.Tensor, context: torch.Tensor = None):\n    \"\"\"Forward pass with optional context for compatibility.\"\"\"\n    if context is None:\n        # If no context provided, try to use all available models\n        # This is a fallback for PytorchBase compatibility\n        if len(self.models) == 1:\n            model = next(iter(self.models.values()))\n            return model(x).view(-1)\n        else:\n            raise ValueError(\"Context must be provided when multiple models exist\")\n\n    preds = torch.zeros_like(context, dtype=torch.float32)\n    for i in range(x.shape[0]):\n        model = self._get_model_for_context(context[i].item())\n        preds[i] = model(x[i].unsqueeze(0))\n    return preds.view(-1)\n</code></pre>"},{"location":"reference/#counterfactuals.models.generative.kde.KDE.predict_log_proba","title":"predict_log_proba","text":"<pre><code>predict_log_proba(X_test, context=None)\n</code></pre> <p>Predict log probabilities for input data.</p> Source code in <code>counterfactuals/models/generative/kde.py</code> <pre><code>def predict_log_proba(\n    self, X_test: np.ndarray, context: Optional[np.ndarray] = None\n) -&gt; np.ndarray:\n    \"\"\"Predict log probabilities for input data.\"\"\"\n    # Convert to torch tensor if needed\n    assert context is not None, \"Context must be provided for KDE\"\n    X_test = torch.from_numpy(X_test).float()\n    context = torch.from_numpy(context).float()\n    preds = torch.zeros_like(context, dtype=torch.float32)\n    for i in range(X_test.shape[0]):\n        model = self._get_model_for_context(context[i].item())\n        preds[i] = model(X_test[i].unsqueeze(0))\n    return preds.cpu().numpy()\n</code></pre>"},{"location":"reference/#counterfactuals.models.generative.kde.KDE.sample_and_log_proba","title":"sample_and_log_proba","text":"<pre><code>sample_and_log_proba(n_samples, context=None)\n</code></pre> <p>Sample from KDE and return log probabilities.</p> Source code in <code>counterfactuals/models/generative/kde.py</code> <pre><code>def sample_and_log_proba(self, n_samples: int, context: Optional[np.ndarray] = None) -&gt; tuple:\n    \"\"\"Sample from KDE and return log probabilities.\"\"\"\n    raise NotImplementedError(\"Sampling from KDE is not implemented\")\n</code></pre>"},{"location":"reference/#counterfactuals.models.generative.kde.auto_reshape","title":"auto_reshape","text":"<pre><code>auto_reshape(fn)\n</code></pre> <p>Decorator which flattens image inputs and reshapes them before returning.</p> <p>This is used to enable non-convolutional models to transparently work on images.</p> Source code in <code>counterfactuals/models/generative/kde.py</code> <pre><code>def auto_reshape(fn):\n    \"\"\"Decorator which flattens image inputs and reshapes them before returning.\n\n    This is used to enable non-convolutional models to transparently work on images.\n    \"\"\"\n\n    def wrapped_fn(self, x, *args, **kwargs):\n        original_shape = x.shape\n        x = x.view(original_shape[0], -1)\n        y = fn(self, x, *args, **kwargs)\n        return y.view(original_shape)\n\n    return wrapped_fn\n</code></pre>"},{"location":"reference/#quick-links","title":"Quick Links","text":"<p>For method-specific API documentation, see:</p> <ul> <li>PPCEF</li> <li>DICE</li> <li>GLOBE-CE</li> <li>ReViCE</li> </ul>"},{"location":"user-guide/","title":"User Guide","text":"<p>This guide covers the main workflows for using the Counterfactuals library.</p>"},{"location":"user-guide/#overview","title":"Overview","text":"<p>The library provides a complete pipeline for generating and evaluating counterfactual explanations:</p> <pre><code>flowchart LR\n    A[Load Dataset] --&gt; B[Train Models]\n    B --&gt; C[Generate CFs]\n    C --&gt; D[Evaluate]\n    D --&gt; E[Analyze Results]</code></pre>"},{"location":"user-guide/#sections","title":"Sections","text":"Section Description Working with Datasets Load and configure datasets for counterfactual generation Training Models Train discriminative and generative models Generating Counterfactuals Use various methods to generate explanations Evaluating Results Assess counterfactual quality with metrics Running Pipelines Execute end-to-end experiments with Hydra"},{"location":"user-guide/#quick-reference","title":"Quick Reference","text":""},{"location":"user-guide/#basic-workflow","title":"Basic Workflow","text":"<pre><code>from counterfactuals.datasets import FileDataset\nfrom counterfactuals.models.classifiers import MLPClassifier\nfrom counterfactuals.models.generators import MaskedAutoregressiveFlow\nfrom counterfactuals.cf_methods.local_methods import PPCEF\n\n# 1. Load dataset\ndataset = FileDataset(config_path=\"config/datasets/adult.yaml\")\n\n# 2. Train models\nclassifier = MLPClassifier(...)\ngen_model = MaskedAutoregressiveFlow(...)\n\n# 3. Generate counterfactuals\nmethod = PPCEF(gen_model, classifier, ...)\nresult = method.explain(X, y_origin, y_target, ...)\n\n# 4. Evaluate\nfrom counterfactuals.metrics import MetricsOrchestrator\nmetrics = MetricsOrchestrator(...)\nscores = metrics.compute(result)\n</code></pre>"},{"location":"user-guide/datasets/","title":"Working with Datasets","text":"<p>Learn how to load, configure, and use datasets for counterfactual generation.</p>"},{"location":"user-guide/datasets/#loading-pre-configured-datasets","title":"Loading Pre-configured Datasets","text":"<pre><code>from counterfactuals.datasets import FileDataset\n\n# Load a dataset from YAML config\ndataset = FileDataset(config_path=\"config/datasets/adult.yaml\")\n\n# Access splits\nX_train, X_test = dataset.X_train, dataset.X_test\ny_train, y_test = dataset.y_train, dataset.y_test\n</code></pre>"},{"location":"user-guide/datasets/#dataset-properties","title":"Dataset Properties","text":"<pre><code># Feature information\nprint(dataset.features)              # All feature names\nprint(dataset.numerical_features)    # Continuous features\nprint(dataset.categorical_features)  # Discrete features\nprint(dataset.actionable_features)   # Modifiable features\n</code></pre>"},{"location":"user-guide/datasets/#cross-validation","title":"Cross-Validation","text":"<pre><code># Get CV splits\ncv_splits = dataset.get_cv_splits(n_splits=5)\n\nfor fold, (train_idx, val_idx) in enumerate(cv_splits):\n    X_train_fold = dataset.X[train_idx]\n    X_val_fold = dataset.X[val_idx]\n</code></pre>"},{"location":"user-guide/datasets/#custom-traintest-splits","title":"Custom Train/Test Splits","text":"<pre><code># Custom split ratio\nX_train, X_test, y_train, y_test = dataset.split_data(\n    dataset.X,\n    dataset.y,\n    train_ratio=0.7,\n    stratify=True\n)\n</code></pre>"},{"location":"user-guide/datasets/#next-steps","title":"Next Steps","text":"<ul> <li>Training Models - Train classifiers and generative models</li> <li>Custom Datasets - Add your own datasets</li> </ul>"},{"location":"user-guide/evaluation/","title":"Evaluating Results","text":"<p>Assess the quality of generated counterfactuals using built-in metrics.</p>"},{"location":"user-guide/evaluation/#using-metricsorchestrator","title":"Using MetricsOrchestrator","text":"<pre><code>from counterfactuals.metrics import MetricsOrchestrator\n\n# Initialize with desired metrics\norchestrator = MetricsOrchestrator(\n    metrics=[\n        \"validity\",\n        \"proximity_l2\",\n        \"sparsity\",\n        \"plausibility\"\n    ],\n    gen_model=flow\n)\n\n# Compute metrics\nscores = orchestrator.compute(\n    x_cfs=result.x_cfs,\n    x_origs=result.x_origs,\n    y_targets=result.y_cf_targets,\n    classifier=classifier\n)\n\nfor metric, value in scores.items():\n    print(f\"{metric}: {value:.4f}\")\n</code></pre>"},{"location":"user-guide/evaluation/#available-metrics","title":"Available Metrics","text":""},{"location":"user-guide/evaluation/#validity-metrics","title":"Validity Metrics","text":"<ul> <li><code>validity</code> - Proportion achieving target class</li> <li><code>coverage</code> - Proportion of successful generations</li> </ul>"},{"location":"user-guide/evaluation/#distance-metrics","title":"Distance Metrics","text":"<ul> <li><code>proximity_l2</code> - Euclidean distance</li> <li><code>proximity_l1</code> - Manhattan distance</li> <li><code>proximity_mad</code> - Mean Absolute Deviation</li> </ul>"},{"location":"user-guide/evaluation/#sparsity-metrics","title":"Sparsity Metrics","text":"<ul> <li><code>sparsity</code> - Average features changed</li> </ul>"},{"location":"user-guide/evaluation/#plausibility-metrics","title":"Plausibility Metrics","text":"<ul> <li><code>plausibility</code> - Log-likelihood under flow</li> <li><code>lof_score</code> - Local Outlier Factor</li> <li><code>isolation_score</code> - Isolation Forest score</li> </ul>"},{"location":"user-guide/evaluation/#diversity-metrics","title":"Diversity Metrics","text":"<ul> <li><code>diversity</code> - Pairwise distance between CFs</li> </ul>"},{"location":"user-guide/evaluation/#custom-metrics","title":"Custom Metrics","text":"<pre><code>from counterfactuals.metrics import Metric, register_metric\n\n@register_metric\nclass MyMetric(Metric):\n    name = \"my_metric\"\n\n    def required_inputs(self):\n        return {\"x_cfs\", \"x_origs\"}\n\n    def __call__(self, x_cfs, x_origs, **kwargs):\n        # Your computation\n        return score\n</code></pre>"},{"location":"user-guide/evaluation/#next-steps","title":"Next Steps","text":"<ul> <li>Benchmark Results - Compare methods</li> </ul>"},{"location":"user-guide/generating-counterfactuals/","title":"Generating Counterfactuals","text":"<p>The core workflow for generating counterfactual explanations.</p> <p>Multiple Methods Available</p> <p>CEL provides 17+ counterfactual methods. The example below demonstrates PPCEF,  but the same <code>explain()</code> interface works for all local methods. See Local Methods  for a complete list and comparison.</p>"},{"location":"user-guide/generating-counterfactuals/#basic-usage","title":"Basic Usage","text":"<p>This example uses PPCEF (Probabilistically Plausible Counterfactual Explanations with Flows):</p> <pre><code>from counterfactuals.cf_methods.local_methods import PPCEF\n\n# Initialize method\nmethod = PPCEF(\n    gen_model=flow,\n    disc_model=classifier,\n    disc_model_criterion=torch.nn.CrossEntropyLoss(),\n    device=\"cuda\"\n)\n\n# Generate counterfactual\nresult = method.explain(\n    X=instance,           # Instance to explain\n    y_origin=0,           # Current prediction\n    y_target=1,           # Desired prediction\n    X_train=X_train,      # Training data (for some methods)\n    y_train=y_train\n)\n</code></pre>"},{"location":"user-guide/generating-counterfactuals/#using-different-methods","title":"Using Different Methods","text":"<p>All local methods share the same interface. Simply change the import:</p> <pre><code># Option 1: PPCEF (used in example above)\nfrom counterfactuals.cf_methods.local_methods import PPCEF\nmethod = PPCEF(gen_model=flow, disc_model=classifier, ...)\n\n# Option 2: DICE (diverse counterfactuals)\nfrom counterfactuals.cf_methods.local_methods import DICE\nmethod = DICE(model=classifier, ...)\n\n# Option 3: WACH (gradient-based)\nfrom counterfactuals.cf_methods.local_methods import WACH\nmethod = WACH(disc_model=classifier, ...)\n\n# Option 4: CEM (contrastive)\nfrom counterfactuals.cf_methods.local_methods import CEM\nmethod = CEM(model=classifier, ...)\n\n# All methods use the same explain() interface:\nresult = method.explain(X=instance, y_origin=0, y_target=1, ...)\n</code></pre>"},{"location":"user-guide/generating-counterfactuals/#understanding-explanationresult","title":"Understanding ExplanationResult","text":"<pre><code>from counterfactuals.cf_methods import ExplanationResult\n\n# Result structure\nresult.x_cfs         # Generated counterfactuals\nresult.y_cf_targets  # Target labels\nresult.x_origs       # Original instances\nresult.y_origs       # Original labels\nresult.logs          # Training logs (optional)\nresult.cf_group_ids  # Group assignments (for group methods)\n</code></pre>"},{"location":"user-guide/generating-counterfactuals/#batch-processing","title":"Batch Processing","text":"<pre><code># Create dataloader\nfrom counterfactuals.datasets import TorchDataLoader\n\nloader = TorchDataLoader(X_test, y_test, batch_size=32)\n\n# Generate for multiple instances\nresult = method.explain_dataloader(\n    dataloader=loader,\n    epochs=100,\n    lr=0.01\n)\n</code></pre>"},{"location":"user-guide/generating-counterfactuals/#common-parameters","title":"Common Parameters","text":"Parameter Description <code>epochs</code> Number of optimization iterations <code>lr</code> Learning rate <code>alpha</code> Validity loss weight <code>beta</code> Proximity loss weight <code>K</code> Number of counterfactuals per instance"},{"location":"user-guide/generating-counterfactuals/#next-steps","title":"Next Steps","text":"<ul> <li>Evaluating Results - Assess counterfactual quality</li> </ul>"},{"location":"user-guide/models/","title":"Training Models","text":"<p>Learn how to train discriminative and generative models for counterfactual generation.</p>"},{"location":"user-guide/models/#discriminative-models-classifiers","title":"Discriminative Models (Classifiers)","text":""},{"location":"user-guide/models/#mlp-classifier","title":"MLP Classifier","text":"<pre><code>from counterfactuals.models.classifiers import MLPClassifier\n\nclassifier = MLPClassifier(\n    input_dim=n_features,\n    hidden_dims=[128, 64],\n    output_dim=n_classes\n)\n\nclassifier.fit(\n    train_loader=train_loader,\n    test_loader=test_loader,\n    epochs=100,\n    lr=0.001\n)\n</code></pre>"},{"location":"user-guide/models/#logistic-regression","title":"Logistic Regression","text":"<pre><code>from counterfactuals.models.classifiers import LogisticRegression\n\nclassifier = LogisticRegression(input_dim=n_features, output_dim=n_classes)\nclassifier.fit(train_loader, test_loader, epochs=50)\n</code></pre>"},{"location":"user-guide/models/#generative-models-flows","title":"Generative Models (Flows)","text":""},{"location":"user-guide/models/#masked-autoregressive-flow-maf","title":"Masked Autoregressive Flow (MAF)","text":"<pre><code>from counterfactuals.models.generators import MaskedAutoregressiveFlow\n\nflow = MaskedAutoregressiveFlow(\n    input_dim=n_features,\n    hidden_dims=[128, 128],\n    n_layers=5\n)\n\nflow.fit(\n    train_loader=train_loader,\n    test_loader=test_loader,\n    epochs=200,\n    lr=0.0001\n)\n</code></pre>"},{"location":"user-guide/models/#other-flows","title":"Other Flows","text":"<ul> <li>RealNVP: Affine coupling layers</li> <li>NICE: Non-volume preserving</li> <li>CNF: Continuous normalizing flows (for regression)</li> </ul>"},{"location":"user-guide/models/#saving-and-loading-models","title":"Saving and Loading Models","text":"<pre><code># Save\nclassifier.save(\"models/classifier.pt\")\nflow.save(\"models/flow.pt\")\n\n# Load\nclassifier.load(\"models/classifier.pt\")\nflow.load(\"models/flow.pt\")\n</code></pre>"},{"location":"user-guide/models/#next-steps","title":"Next Steps","text":"<ul> <li>Generating Counterfactuals</li> </ul>"},{"location":"user-guide/pipelines/","title":"Running Pipelines","text":"<p>Execute end-to-end experiments using Hydra configuration system.</p>"},{"location":"user-guide/pipelines/#overview","title":"Overview","text":"<p>Pipelines automate the complete workflow: 1. Load dataset 2. Train/load models 3. Generate counterfactuals 4. Compute metrics 5. Log results</p>"},{"location":"user-guide/pipelines/#running-a-pipeline","title":"Running a Pipeline","text":"<pre><code># Run PPCEF pipeline\npython -m counterfactuals.pipelines.run_ppcef_pipeline\n\n# Override configuration\npython -m counterfactuals.pipelines.run_ppcef_pipeline \\\n    dataset.config_path=config/datasets/compas.yaml \\\n    counterfactuals_params.epochs=200\n</code></pre>"},{"location":"user-guide/pipelines/#configuration-structure","title":"Configuration Structure","text":"<pre><code># pipelines/conf/config.yaml\ndefaults:\n  - gen_model: large_maf\n  - disc_model: mlp\n  - metrics: default\n\ndataset:\n  _target_: counterfactuals.datasets.FileDataset\n  config_path: config/datasets/adult.yaml\n\ngen_model:\n  train_model: true\n  epochs: 200\n  lr: 0.0001\n\ndisc_model:\n  train_model: true\n  epochs: 100\n  lr: 0.001\n\ncounterfactuals_params:\n  epochs: 100\n  lr: 0.01\n  alpha: 1.0\n  beta: 0.5\n</code></pre>"},{"location":"user-guide/pipelines/#available-pipelines","title":"Available Pipelines","text":"Pipeline Method <code>run_ppcef_pipeline</code> PPCEF <code>run_dice_pipeline</code> DICE <code>run_globe_ce_pipeline</code> GLOBE-CE <code>run_rppcef_pipeline</code> ReViCE ... ..."},{"location":"user-guide/pipelines/#mlflow-logging","title":"MLflow Logging","text":"<p>Results are automatically logged to MLflow:</p> <pre><code>import mlflow\n\n# View logged runs\nmlflow.search_runs()\n</code></pre>"},{"location":"user-guide/pipelines/#creating-custom-pipelines","title":"Creating Custom Pipelines","text":"<p>See existing pipelines in <code>counterfactuals/pipelines/</code> for examples.</p>"}]}