{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from matplotlib import cm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "from counterfactuals.datasets import LawDataset, MoonsDataset\n",
    "from counterfactuals.generative_models import MaskedAutoregressiveFlowDistance as MaskedAutoregressiveFlow\n",
    "from counterfactuals.discriminative_models import (\n",
    "    LogisticRegression,\n",
    "    MultilayerPerceptron,\n",
    ")\n",
    "from counterfactuals.metrics import CFMetrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class PairDistanceDataset(Dataset):\n",
    "    def __init__(self, class_zero, class_one, length=None):\n",
    "        \"\"\"\n",
    "        Initialize with two arrays, one for each class.\n",
    "        \"\"\"\n",
    "        self.length = length\n",
    "        self.class_zero = torch.tensor(class_zero, dtype=torch.float32).to(device)  # ✅ Move to CUDA\n",
    "        self.class_one = torch.tensor(class_one, dtype=torch.float32).to(device)  # ✅ Move to CUDA\n",
    "\n",
    "        # Calculate pairwise distances between zero and one classes\n",
    "        self.zero_one_distance = torch.cdist(self.class_zero, self.class_one) ** 4\n",
    "\n",
    "        self.size_zero = class_zero.shape[0]\n",
    "        self.size_one = class_one.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        # The total combinations are len(class_zero) * len(class_one)\n",
    "        if self.length is not None:\n",
    "            return self.length\n",
    "        return self.size_zero * self.size_one\n",
    "\n",
    "    def get_specific_item(self, idx):\n",
    "        \"\"\"\n",
    "        Get the specific item based on the index.\n",
    "        Sample the second point based on the distance weight from another class.\n",
    "        \"\"\"\n",
    "        if idx < self.size_zero:\n",
    "            i = idx\n",
    "            x_orig = self.class_zero[i]\n",
    "\n",
    "            # Calculate weights for sampling y\n",
    "            zero_one_weight = 1 / self.zero_one_distance[i]\n",
    "            zero_one_weight /= zero_one_weight.sum()\n",
    "\n",
    "            j = torch.multinomial(zero_one_weight, num_samples=1).item()\n",
    "            x_cf = self.class_one[j]\n",
    "        else:\n",
    "            i = idx + self.size_zero\n",
    "            x_orig = self.class_one[i]\n",
    "\n",
    "            # Calculate weights for sampling y\n",
    "            zero_one_weight = 1 / self.zero_one_distance[:, i]\n",
    "            zero_one_weight /= zero_one_weight.sum()\n",
    "\n",
    "            j = torch.multinomial(zero_one_weight, num_samples=1).item()\n",
    "            x_cf = self.class_zero[j]\n",
    "        return torch.tensor(x_cf, dtype=torch.float32), torch.tensor(\n",
    "            x_orig, dtype=torch.float32\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Randomly select a point from one class.\n",
    "        Sample the second point based on the distance weight from another class.\n",
    "        \"\"\"\n",
    "        if torch.rand(1) > 0.5:\n",
    "            i = torch.randint(0, self.size_zero, (1,)).item()\n",
    "            x_orig = self.class_zero[i]\n",
    "\n",
    "            # Calculate weights for sampling y\n",
    "            zero_one_weight = 1 / self.zero_one_distance[i]\n",
    "            zero_one_weight /= zero_one_weight.sum()\n",
    "\n",
    "            j = torch.multinomial(zero_one_weight, num_samples=1).item()\n",
    "            x_cf = self.class_one[j]\n",
    "        else:\n",
    "            i = torch.randint(0, self.size_one, (1,)).item()\n",
    "            x_orig = self.class_one[i]\n",
    "\n",
    "            # Calculate weights for sampling y\n",
    "            zero_one_weight = 1 / self.zero_one_distance[:, i]\n",
    "            zero_one_weight /= zero_one_weight.sum()\n",
    "\n",
    "            j = torch.multinomial(zero_one_weight, num_samples=1).item()\n",
    "            x_cf = self.class_zero[j]\n",
    "        return torch.tensor(x_cf, dtype=torch.float32), torch.tensor(\n",
    "            x_orig, dtype=torch.float32\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from counterfactuals.datasets import GermanCreditDataset, AdultDataset\n",
    "\n",
    "# dataset = MoonsDataset(file_path=\"../../data/moons.csv\")\n",
    "# dataset = LawDataset(file_path=\"../../data/law.csv\")\n",
    "# dataset = GermanCreditDataset(file_path=\"data/german_credit.csv\")\n",
    "dataset = AdultDataset(file_path=\"data/adult.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from counterfactuals.generative_models import MaskedAutoregressiveFlow as baseMAF\n",
    "dataset.X_train = np.array(dataset.X_train, dtype=np.float32)\n",
    "dataset.y_train = np.array(dataset.y_train, dtype=np.float32)\n",
    "dataset.X_test = np.array(dataset.X_test, dtype=np.float32)\n",
    "dataset.y_test = np.array(dataset.y_test, dtype=np.float32)\n",
    "\n",
    "\n",
    "\n",
    "flow_train_dataloader = dataset.train_dataloader(\n",
    "    batch_size=128, shuffle=True, noise_lvl=0.03\n",
    ")\n",
    "flow_test_dataloader = dataset.test_dataloader(batch_size=128, shuffle=False)\n",
    "flow = baseMAF(\n",
    "    features=dataset.X_test.shape[1],\n",
    "    hidden_features=16,\n",
    "    num_blocks_per_layer=4,\n",
    "    num_layers=8,\n",
    "    context_features=1,\n",
    "    device=device\n",
    ").to(device)\n",
    "flow.fit(flow_train_dataloader, flow_test_dataloader, num_epochs=1000, patience=50)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "disc_model = MultilayerPerceptron(\n",
    "    input_size=29,  # Adjust based on the actual feature size of your dataset\n",
    "    hidden_layer_sizes=[256, 256],\n",
    "    target_size=1,\n",
    "    dropout=0.2,\n",
    "    device=device\n",
    ").to(device)\n",
    "\n",
    "train_dataloader = dataset.train_dataloader(batch_size=64, shuffle=True, noise_lvl=0.0)\n",
    "test_dataloader = dataset.test_dataloader(batch_size=64, shuffle=False)\n",
    "disc_model.fit(train_dataloader, test_dataloader, epochs=10000, patience=100, lr=1e-3)\n",
    "\n",
    "# validate\n",
    "y_pred = disc_model.predict(dataset.X_test).cpu().detach().numpy()  # ✅ Fix added\n",
    "\n",
    "y_true = dataset.y_test\n",
    "print(f\"Accuracy: {np.mean(y_pred == y_true)}\")\n",
    "# disc_model.load(\"../models/MoonsDataset/disc_model_MultilayerPerceptron.pt\")\n",
    "\n",
    "disc_model.eval()\n",
    "dataset.y_train = disc_model.predict(dataset.X_train).cpu().detach().numpy()\n",
    "dataset.y_test = disc_model.predict(dataset.X_test).cpu().detach().numpy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Plot training dataset\n",
    "axes[0].scatter(dataset.X_train[:, 0], dataset.X_train[:, 1], c=dataset.y_train, cmap='viridis', marker='o')\n",
    "axes[0].set_title('Training Dataset')\n",
    "axes[0].set_xlabel('Feature 1')\n",
    "axes[0].set_ylabel('Feature 2')\n",
    "\n",
    "# Plot test dataset\n",
    "axes[1].scatter(dataset.X_test[:, 0], dataset.X_test[:, 1], c=dataset.y_test, cmap='viridis', marker='o')\n",
    "axes[1].set_title('Test Dataset')\n",
    "axes[1].set_xlabel('Feature 1')\n",
    "axes[1].set_ylabel('Feature 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class_zero = dataset.X_train[dataset.y_train == 0]\n",
    "class_one = dataset.X_train[dataset.y_train == 1]\n",
    "\n",
    "pair_dataset_train = PairDistanceDataset(class_zero, class_one, length=5000)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    X, y = zip(*batch)\n",
    "    X = torch.stack(X).to(device)\n",
    "    y = torch.stack(y).to(device)\n",
    "    noise = torch.randn_like(X) * 0.03\n",
    "    noise = torch.randn_like(y) * 0.03\n",
    "    X = X + noise\n",
    "    y = y + noise\n",
    "    return X, y\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    pair_dataset_train, batch_size=256, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "train_dataloader = DataLoader(pair_dataset_train, batch_size=128, shuffle=True)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class_zero = dataset.X_test[dataset.y_test == 0]\n",
    "class_one = dataset.X_test[dataset.y_test == 1]\n",
    "\n",
    "pair_dataset_test = PairDistanceDataset(class_zero, class_one)\n",
    "\n",
    "test_dataloader = DataLoader(pair_dataset_test, batch_size=2048, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cf = MaskedAutoregressiveFlow(\n",
    "    features=dataset.X_test.shape[1],\n",
    "    hidden_features=16,\n",
    "    num_blocks_per_layer=2,\n",
    "    num_layers=2,\n",
    "    context_features=dataset.X_test.shape[1],\n",
    "    device=device\n",
    ").to(device)\n",
    "cf.fit(\n",
    "    train_dataloader, test_dataloader, num_epochs=1000, learning_rate=1e-3, patience=100, lambda_dist=0.2, checkpoint_path=\"best_cf_model_dist_0.2_adult_cuda.pt\"\n",
    ")\n",
    "cf.load(\"best_cf_model_dist_0.2_adult_cuda.pt\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "cfs = []\n",
    "with torch.no_grad():\n",
    "    for x in dataset.X_test:\n",
    "        points, log_prob = cf.sample_and_log_prob(\n",
    "            100, context=torch.from_numpy(np.array([x]))\n",
    "        )\n",
    "        cfs.append(points)\n",
    "cfs = torch.stack(cfs).squeeze().permute(1, 0, 2).cpu().numpy()\n",
    "\n",
    "all_metrics = []\n",
    "for i in tqdm(range(cfs.shape[0])):\n",
    "    metrics = CFMetrics(\n",
    "        X_cf=cfs[i],\n",
    "        y_target=np.abs(dataset.y_test - 1),\n",
    "        X_train=dataset.X_train,\n",
    "        y_train=dataset.y_train,\n",
    "        X_test=dataset.X_test,\n",
    "        y_test=dataset.y_test,\n",
    "        gen_model=flow,\n",
    "        disc_model=disc_model,\n",
    "        continuous_features=dataset.numerical_features,\n",
    "        categorical_features=dataset.categorical_features,\n",
    "        prob_plausibility_threshold=1.2,\n",
    "    )\n",
    "\n",
    "    all_metrics.append(metrics.calc_all_metrics())\n",
    "\n",
    "# Calculate mean and standard deviation for each metric\n",
    "mean_metrics = {key: np.mean([m[key] for m in all_metrics]) for key in all_metrics[0]}\n",
    "std_metrics = {key: np.std([m[key] for m in all_metrics]) for key in all_metrics[0]}\n",
    "\n",
    "# Print the results\n",
    "for key in mean_metrics:\n",
    "    print(f\"{key}: {mean_metrics[key]:.4f} ± {std_metrics[key]:.4f}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from metrics import distance, feasibility, constraint_violation, success_rate\n",
    "cfs = cfs.reshape(-1, cfs.shape[-1])  # Flatten first two dimensions\n",
    "\n",
    "distance_pd = pd.DataFrame(distance(cfs, dataset.y_test, dataset))\n",
    "\n",
    "feasibility_pd = pd.DataFrame(feasibility(cfs, dataset, dataset.df.columns), columns=['feasibility'])\n",
    "\n",
    "# const_pd = pd.DataFrame(constraint_violation(decoded_cfs, decoded_factuals, dataset), columns=['violation'])\n",
    "\n",
    "success_pd = pd.DataFrame(success_rate(cfs[dataset.df.columns], cf), columns=['success'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = pd.concat([distance_pd, feasibility_pd, success_pd], axis=1)\n",
    "print(results)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
