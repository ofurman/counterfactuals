{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diversity Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on \"**Achieving Diversity in Counterfactual Explanations: a Review and Discussion**\"\n",
    "\n",
    " by **Thibault Laugel and Adulam Jeyasothy et al.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Diversity Type          | Metric or Criterion               | Examples from Literature         |\n",
    "|--------------------------|------------------------------------|-----------------------------------|\n",
    "| **Criteria Diversity**   | Pareto spread, objective variance | Dandl et al. (2020), CARE (2022) |\n",
    "| **Feature Diversity**    | \\( l_0 \\), \\( l_2 \\), cosine distance | Mothilal et al. (2020), DiVE (2021) |\n",
    "| **Action Diversity**     | Jaccard distance, direction count | Russell (2019), Guidotti et al. (2019) |\n",
    "| **Optimization Diversity** | Randomization, clustering         | CERTIFAI (2020), GeCo (2021)     |\n",
    "| **Mixed Diversity (All Types)** | Custom definitions          | Tsirtsis et al. (2021), DECE (2020) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pareto Spread\n",
    "- **Definition**: Measures the spread of solutions along the Pareto front in multi-objective optimization.\n",
    "- **Formula**:\n",
    "  $$\n",
    "  \\text{Pareto Spread} = \\sqrt{\\sum_{i=1}^{k} \\| f(e_i) - \\bar{f} \\|_2^2}\n",
    "  $$\n",
    "  Where:\n",
    "  - $ f(e_i) $: Vector of objective values for the $i$-th counterfactual.\n",
    "  - $ \\bar{f} $: Centroid of the Pareto front (average of $ f(e_i) $).\n",
    "  - $ k $: Number of counterfactuals.\n",
    "\n",
    "This metric captures how far the solutions are distributed across the Pareto front.\n",
    "\n",
    "---\n",
    "\n",
    "### Objective Variance\n",
    "- **Definition**: Quantifies the variability of individual objectives across a set of counterfactuals.\n",
    "- **Formula**:\n",
    "  $$\n",
    "  \\text{Objective Variance} = \\frac{1}{k} \\sum_{j=1}^{m} \\text{Var}([f_j(e_1), f_j(e_2), \\dots, f_j(e_k)])\n",
    "  $$\n",
    "  Where:\n",
    "  - $ m $: Number of objectives.\n",
    "  - $ f_j(e_i) $: Value of the $j$-th objective for the $i$-th counterfactual.\n",
    "  - $ \\text{Var} $: Variance function.\n",
    "\n",
    "This metric evaluates how well the objectives differ across the generated counterfactuals.\n",
    "\n",
    "---\n",
    "\n",
    "Both metrics focus on diversity in the trade-offs between objectives, with Pareto Spread assessing distribution and Objective Variance quantifying variability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective Function\n",
    "\n",
    "Given:\n",
    "- $ \\mathbf{x}' \\sim p_{dist}(\\mathbf{x}', \\mathbf{x}) = \\frac{1}{\\lambda \\|\\mathbf{x} - \\mathbf{x}'\\|_2^2} $: Target variable (Counterfactual Explanation).\n",
    "- $ \\mathbf{x} \\in \\mathbb{R}^d $: Conditional variable (Data Point).\n",
    "- $ p_\\theta(\\mathbf{x}' | \\mathbf{x}) $: Conditional density modeled by the flow.\n",
    "\n",
    "The optimization objective:\n",
    "$$\n",
    "\\mathcal{L}(\\theta) = - \\log p_\\theta(f_\\theta(\\mathbf{x}, \\mathbf{x}')) - \\log \\left| \\det \\frac{\\partial f_\\theta(\\mathbf{x}, \\mathbf{x}')}{\\partial \\mathbf{x}} \\right|\n",
    "$$\n",
    "\n",
    "### Optimization Problem\n",
    "\n",
    "$$\n",
    "\\theta^* = \\arg\\min_\\theta \\mathcal{L}(\\theta).\n",
    "$$\n",
    "\n",
    "### Components\n",
    "1. **Conditional Log-Likelihood**:\n",
    "   $$\n",
    "   -\\log p_\\theta(\\mathbf{x} | \\mathbf{x}').\n",
    "   $$\n",
    "2. **Regularization**: \n",
    "   $$\n",
    "   \\lambda \\|\\mathbf{x} - \\mathbf{x}'\\|_2^2.\n",
    "   $$\n",
    "3. **Trade-off**: Controlled by $ \\lambda $.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method results\n",
    "| **Dataset** | **Method** | **Coverage ↑** | **Validity ↑** | **Prob. Plaus. ↑** | **LOF** | **IsoForest** | **Log Dens. ↑** | **L1 ↓** | **L2 ↓** | **Time ↓** |\n",
    "|-------------|------------|---------------|---------------|-------------------|---------|--------------|----------------|---------|---------|-----------|\n",
    "| **Moons**   | CBCE       | 1.00          | 1.00          | 0.10              | 1.06    | 0.03         | -5.81          | 0.62    | 0.48    | 0.07 s     |\n",
    "|             | CEGP       | 1.00          | 1.00          | 0.09              | 1.36    | 0.00         | -6.66          | 0.36    | 0.28    | 904.11 s   |\n",
    "|             | CEM        | 1.00          | 1.00          | 0.14              | 2.03    | -0.07        | -10.09         | 0.55    | 0.50    | 211.56 s   |\n",
    "|             | WACH       | 0.98          | 1.00          | 0.11              | 1.55    | -0.01        | -6.34          | 0.36    | 0.36    | 198.29 s   |\n",
    "|             | ARTELT     | 1.00          | 1.00          | 0.08              | 1.53    | -0.03        | -8.74          | 0.32    | 0.32    | 4.15 s     |\n",
    "|             | PPCEF      | 1.00          | 1.00          | 1.00              | 1.01    | 0.04         | 1.69           | 0.45    | 0.36    | 1.85 s     |\n",
    "|             | GenCE      | **1.0000 ± 0.0000** | **0.9239 ± 0.0173** | **0.4560 ± 0.0271** | **1.0991 ± 0.0089** | **0.0158 ± 0.0017** | **0.7627 ± 0.1422** | **0.4330 ± 0.0139** | **0.3403 ± 0.0103** | **-**        |\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    ".\n",
    "| **Dataset** | **Method** | **Coverage ↑** | **Validity ↑** | **Prob. Plaus. ↑** | **LOF** | **IsoForest** | **Log Dens. ↑** | **L1 ↓** | **L2 ↓** | **Time ↓** |\n",
    "|-------------|------------|---------------|---------------|-------------------|---------|--------------|----------------|---------|---------|-----------|\n",
    "| **Law**     | CBCE       | 1.00          | 1.00          | 0.49              | 1.05    | 0.04         | 1.28           | 0.61    | 0.40    | 0.23 s     |\n",
    "|             | CEGP       | 1.00          | 1.00          | 0.49              | 1.07    | 0.02         | 1.08           | 0.23    | 0.18    | 1973.76 s  |\n",
    "|             | CEM        | 1.00          | 1.00          | 0.26              | 1.26    | -0.02        | -0.56          | 0.33    | 0.31    | 368.10 s   |\n",
    "|             | WACH       | 1.00          | 1.00          | 0.39              | 1.30    | -0.01        | -0.13          | 0.45    | 0.35    | 359.00 s   |\n",
    "|             | ARTELT     | 1.00          | 1.00          | 0.40              | 1.12    | 0.02         | -0.54          | 0.20    | 0.20    | 0.42 s     |\n",
    "|             | PPCEF      | 1.00          | 1.00          | 1.00              | 1.03    | 0.07         | 2.05           | 0.37    | 0.23    | 2.42 s     |\n",
    "|             | GenCE      | **1.0000 ± 0.0000** | **0.7306 ± 0.0189** | **0.7499 ± 0.0207** | **1.0443 ± 0.0039** | **0.0503 ± 0.0017** | **1.5886 ± 0.0439** | **0.5388 ± 0.0096** | **0.3487 ± 0.0057** | **-**        |\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    ".\n",
    "| **Dataset** | **Method** | **Coverage ↑** | **Validity ↑** | **Prob. Plaus. ↑** | **LOF** | **IsoForest** | **Log Dens. ↑** | **L1 ↓** | **L2 ↓** | **Time ↓** |\n",
    "|-------------|------------|---------------|---------------|-------------------|---------|--------------|----------------|---------|---------|-----------|\n",
    "| **Audit**   | CBCE       | **1.00**      | **1.00**      | 0.79              | 11.70   | 0.14         | **54.97**      | 2.55    | 1.24    | **0.04 s** |\n",
    "|             | CEGP       | 0.97          | **1.00**      | 0.26              | 6.08·10⁷| 0.06         | 8.09           | 1.65    | 0.67    | 561.04 s   |\n",
    "|             | CEM        | 0.52          | **1.00**      | 0.01              | 8.28·10⁶| -0.04        | 10.24          | **0.37**| **0.37**| 105.92 s   |\n",
    "|             | WACH       | **0.99**      | **1.00**      | 0.02              | 1.42·10⁸| 0.06         | -40.34         | 1.78    | 0.80    | 101.27 s   |\n",
    "|             | ARTELT     | 0.60          | 0.97          | 0.00              | 4.09·10⁸| 0.10         | -3585.76       | **0.90**| 0.88    | 43.84 s    |\n",
    "|             | PPCEF      | **1.00**      | **0.99**      | **0.99**          | 4.25·10⁷| 0.08         | 51.64          | 2.04    | 0.79    | 7.01 s     |\n",
    "|             | GenCE      | **1.0000 ± 0.0000** | **0.8306 ± 0.0189** | | **4.48·10⁷** | **0.1443 ± 0.0039** | **41.588 ± 0.043** | **2.038 ± 0.001** | **0.858 ± 0.005** | **-**        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from matplotlib import cm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "# from counterfactuals.datasets import LawDataset, MoonsDataset\n",
    "# from counterfactuals.generative_models import MaskedAutoregressiveFlowDistance as MaskedAutoregressiveFlow\n",
    "from counterfactuals.generative_models import MaskedAutoregressiveFlow\n",
    "from counterfactuals.discriminative_models import (\n",
    "    LogisticRegression,\n",
    "    MultilayerPerceptron,\n",
    ")\n",
    "from counterfactuals.metrics import CFMetrics\n",
    "def l0_distance(X_cf, X_test):\n",
    "    \"\"\"Compute L0 distance (number of changed features).\"\"\"\n",
    "    return np.sum(X_cf != X_test, axis=1).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairDistanceDataset(Dataset):\n",
    "    def __init__(self, class_zero, class_one, length=None):\n",
    "        \"\"\"\n",
    "        Initialize with two arrays, one for each class.\n",
    "        \"\"\"\n",
    "        self.length = length\n",
    "        self.class_zero = torch.tensor(class_zero, dtype=torch.float32) # ✅ Move to CUDA\n",
    "        self.class_one = torch.tensor(class_one, dtype=torch.float32)\n",
    "\n",
    "        # Calculate pairwise distances between zero and one classes\n",
    "        self.zero_one_distance = torch.cdist(self.class_zero, self.class_one) ** 4\n",
    "\n",
    "        self.size_zero = class_zero.shape[0]\n",
    "        self.size_one = class_one.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        # The total combinations are len(class_zero) * len(class_one)\n",
    "        if self.length is not None:\n",
    "            return self.length\n",
    "        return self.size_zero * self.size_one\n",
    "\n",
    "    def get_specific_item(self, idx):\n",
    "        \"\"\"\n",
    "        Get the specific item based on the index.\n",
    "        Sample the second point based on the distance weight from another class.\n",
    "        \"\"\"\n",
    "        if idx < self.size_zero:\n",
    "            i = idx\n",
    "            x_orig = self.class_zero[i]\n",
    "\n",
    "            # Calculate weights for sampling y\n",
    "            zero_one_weight = 1 / self.zero_one_distance[i]\n",
    "            zero_one_weight /= zero_one_weight.sum()\n",
    "\n",
    "            j = torch.multinomial(zero_one_weight, num_samples=1).item()\n",
    "            x_cf = self.class_one[j]\n",
    "        else:\n",
    "            i = idx + self.size_zero\n",
    "            x_orig = self.class_one[i]\n",
    "\n",
    "            # Calculate weights for sampling y\n",
    "            zero_one_weight = 1 / self.zero_one_distance[:, i]\n",
    "            zero_one_weight /= zero_one_weight.sum()\n",
    "\n",
    "            j = torch.multinomial(zero_one_weight, num_samples=1).item()\n",
    "            x_cf = self.class_zero[j]\n",
    "        return torch.tensor(x_cf, dtype=torch.float32), torch.tensor(\n",
    "            x_orig, dtype=torch.float32\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Randomly select a point from one class.\n",
    "        Sample the second point based on the distance weight from another class.\n",
    "        \"\"\"\n",
    "        if torch.rand(1) > 0.5:\n",
    "            i = torch.randint(0, self.size_zero, (1,)).item()\n",
    "            x_orig = self.class_zero[i]\n",
    "\n",
    "            # Calculate weights for sampling y\n",
    "            zero_one_weight = 1 / self.zero_one_distance[i]\n",
    "            zero_one_weight /= zero_one_weight.sum()\n",
    "\n",
    "            j = torch.multinomial(zero_one_weight, num_samples=1).item()\n",
    "            x_cf = self.class_one[j]\n",
    "        else:\n",
    "            i = torch.randint(0, self.size_one, (1,)).item()\n",
    "            x_orig = self.class_one[i]\n",
    "\n",
    "            # Calculate weights for sampling y\n",
    "            zero_one_weight = 1 / self.zero_one_distance[:, i]\n",
    "            zero_one_weight /= zero_one_weight.sum()\n",
    "\n",
    "            j = torch.multinomial(zero_one_weight, num_samples=1).item()\n",
    "            x_cf = self.class_zero[j]\n",
    "        return torch.tensor(x_cf, dtype=torch.float32), torch.tensor(\n",
    "            x_orig, dtype=torch.float32\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PairDistanceDensityDataset(Dataset):\n",
    "#     def __init__(self, class_zero, class_one, gen_model: BaseGenModel, length=None):\n",
    "#         \"\"\"\n",
    "#         Initialize with two arrays, one for each class.\n",
    "#         \"\"\"\n",
    "#         self.length = length\n",
    "#         self.gen_model = gen_model\n",
    "#         self.class_zero = torch.tensor(class_zero, dtype=torch.float32)\n",
    "#         self.class_one = torch.tensor(class_one, dtype=torch.float32)\n",
    "\n",
    "#         # Calculate pairwise distances between zero and one classes\n",
    "#         self.zero_one_distance = torch.cdist(self.class_zero, self.class_one) ** 4\n",
    "#         self.zero_log_likelihood = self.gen_model(\n",
    "#             class_zero, torch.zeros(class_zero.shape[0])\n",
    "#         ).exp()\n",
    "#         self.one_log_likelihood = self.gen_model(\n",
    "#             class_one, torch.ones(class_one.shape[0])\n",
    "#         ).exp()\n",
    "\n",
    "#         self.size_zero = class_zero.shape[0]\n",
    "#         self.size_one = class_one.shape[0]\n",
    "\n",
    "#     def __len__(self):\n",
    "#         # The total combinations are len(class_zero) * len(class_one)\n",
    "#         if self.length is not None:\n",
    "#             return self.length\n",
    "#         return self.size_zero * self.size_one\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         \"\"\"\n",
    "#         Randomly select a point from one class.\n",
    "#         Sample the second point based on the distance weight from another class.\n",
    "#         \"\"\"\n",
    "#         if torch.rand(1) > 0.5:\n",
    "#             i = torch.randint(0, self.size_zero, (1,)).item()\n",
    "#             x_orig = self.class_zero[i]\n",
    "\n",
    "#             # Calculate weights for sampling y\n",
    "#             zero_one_weight = (1 / self.zero_one_distance[i]) + self.one_log_likelihood\n",
    "#             zero_one_weight /= zero_one_weight.sum()\n",
    "\n",
    "#             j = torch.multinomial(zero_one_weight, num_samples=1).item()\n",
    "#             x_cf = self.class_one[j]\n",
    "#         else:\n",
    "#             i = torch.randint(0, self.size_one, (1,)).item()\n",
    "#             x_orig = self.class_one[i]\n",
    "\n",
    "#             # Calculate weights for sampling y\n",
    "#             zero_one_weight = (\n",
    "#                 1 / self.zero_one_distance[:, i] + self.zero_log_likelihood\n",
    "#             )\n",
    "#             zero_one_weight /= zero_one_weight.sum()\n",
    "\n",
    "#             j = torch.multinomial(zero_one_weight, num_samples=1).item()\n",
    "#             x_cf = self.class_zero[j]\n",
    "#         return torch.tensor(x_cf, dtype=torch.float32), torch.tensor(\n",
    "#             x_orig, dtype=torch.float32\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/Projects/counterfactuals/counterfactuals/datasets/givemecredit.py:71: RuntimeWarning: invalid value encountered in cast\n",
      "  y_test = y_test.astype(np.int64)\n"
     ]
    }
   ],
   "source": [
    "from counterfactuals.datasets import GiveMeSomeCreditDataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = GiveMeSomeCreditDataset(\n",
    "    train_file=\"data/GiveMeSomeCredit-training.csv\",\n",
    "    test_file=\"data/GiveMeSomeCredit-testing.csv\"\n",
    ")\n",
    "\n",
    "# dataset = MoonsDataset(file_path=\"../../data/moons.csv\")\n",
    "# dataset = LawDataset(file_path=\"../../data/law.csv\")\n",
    "# dataset = AdultDataset(file_path=\"data/adult.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, Train: -17.3366, test: nan, patience: 7:   1%|          | 8/1000 [04:43<9:45:45, 35.43s/it]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from counterfactuals.generative_models import MaskedAutoregressiveFlow as baseMAF\n",
    "\n",
    "flow_train_dataloader = dataset.train_dataloader(\n",
    "    batch_size=128, shuffle=True, noise_lvl=0.03\n",
    ")\n",
    "flow_test_dataloader = dataset.test_dataloader(batch_size=128, shuffle=False)\n",
    "flow = baseMAF(\n",
    "    features=len(dataset.feature_columns),\n",
    "    hidden_features=16,\n",
    "    num_blocks_per_layer=4,\n",
    "    num_layers=8,\n",
    "    context_features=1,\n",
    ")\n",
    "\n",
    "\n",
    "flow.fit(flow_train_dataloader, flow_test_dataloader, num_epochs=1000, patience=50)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# disc_model = LogisticRegression(input_size=2, target_size=1)\n",
    "# train_dataloader = dataset.train_dataloader(batch_size=256, shuffle=True, noise_lvl=0.0)\n",
    "# test_dataloader = dataset.test_dataloader(batch_size=256, shuffle=False)\n",
    "# disc_model.fit(train_dataloader, test_dataloader, epochs=10000, patience=300, lr=1e-3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "disc_model = MultilayerPerceptron(\n",
    "    input_size=len(dataset.feature_columns),\n",
    "    hidden_layer_sizes=[256, 256],\n",
    "    target_size=1,\n",
    "    dropout=0.2,\n",
    ")\n",
    "\n",
    "train_dataloader = dataset.train_dataloader(batch_size=64, shuffle=True, noise_lvl=0.0)\n",
    "test_dataloader = dataset.test_dataloader(batch_size=64, shuffle=False)\n",
    "disc_model.fit(train_dataloader, test_dataloader, epochs=10000, patience=100, lr=1e-3)\n",
    "# validate\n",
    "# y_pred = disc_model.predict(dataset.X_test).detach().numpy()\n",
    "# dataset.X_test = torch.tensor(dataset.X_test)  # Ensure data is on the same device\n",
    "y_pred = disc_model.predict(dataset.X_test).cpu().detach().numpy()\n",
    "y_true = dataset.y_test\n",
    "print(f\"Accuracy: {np.mean(y_pred == y_true)}\")\n",
    "\n",
    "# disc_model.load(\"../models/MoonsDataset/disc_model_MultilayerPerceptron.pt\")\n",
    "\n",
    "disc_model.eval()\n",
    "dataset.y_train = disc_model.predict(dataset.X_train).cpu().detach().numpy()\n",
    "dataset.y_test = disc_model.predict(dataset.X_test).cpu().detach().numpy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Plot training dataset\n",
    "axes[0].scatter(dataset.X_train[:, 0], dataset.X_train[:, 1], c=dataset.y_train, cmap='viridis', marker='o')\n",
    "axes[0].set_title('Training Dataset')\n",
    "axes[0].set_xlabel('Feature 1')\n",
    "axes[0].set_ylabel('Feature 2')\n",
    "\n",
    "# Plot test dataset\n",
    "axes[1].scatter(dataset.X_test[:, 0], dataset.X_test[:, 1], c=dataset.y_test, cmap='viridis', marker='o')\n",
    "axes[1].set_title('Test Dataset')\n",
    "axes[1].set_xlabel('Feature 1')\n",
    "axes[1].set_ylabel('Feature 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PairDistanceDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_zero = dataset.X_train[dataset.y_train == 0]\n",
    "class_one = dataset.X_train[dataset.y_train == 1]\n",
    "\n",
    "pair_dataset_train = PairDistanceDataset(class_zero, class_one, length=5000)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    X, y = zip(*batch)\n",
    "    X = torch.stack(X)\n",
    "    y = torch.stack(y)\n",
    "    noise = torch.randn_like(X) * 0.03\n",
    "    noise = torch.randn_like(y) * 0.03\n",
    "    X = X + noise\n",
    "    y = y + noise\n",
    "    return X, y\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    pair_dataset_train, batch_size=256, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "train_dataloader = DataLoader(pair_dataset_train, batch_size=128, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_zero = dataset.X_test[dataset.y_test == 0]\n",
    "class_one = dataset.X_test[dataset.y_test == 1]\n",
    "\n",
    "pair_dataset_test = PairDistanceDataset(class_zero, class_one)\n",
    "\n",
    "test_dataloader = DataLoader(pair_dataset_test, batch_size=2048, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = MaskedAutoregressiveFlow(\n",
    "    features=dataset.X_test.shape[1],\n",
    "    hidden_features=16,\n",
    "    num_blocks_per_layer=2,\n",
    "    num_layers=2,\n",
    "    context_features=dataset.X_test.shape[1],\n",
    ")\n",
    "cf.fit(\n",
    "    train_dataloader, test_dataloader, num_epochs=1000, learning_rate=1e-3, patience=100, lambda_dist=0.2, checkpoint_path=\"best_cf_model_dist_0.2_give_credit.pt\"\n",
    ")\n",
    "cf.load(\"best_cf_model_dist_0.2_give_credit.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from counterfactuals.plot_utils import plot_generative_model_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(6, 3, figsize=(20, 30))\n",
    "#\n",
    "# for t_i, t in enumerate([0.1, 0.3, 0.5, 1.0, 1.5, 2.0]):\n",
    "#     for i_i, i in enumerate(range(54, 57)):\n",
    "#         with torch.no_grad():\n",
    "#             points, log_prob = cf.sample_and_log_prob(\n",
    "#                 1000, context=torch.from_numpy(np.array([class_one[i]])), temp=t\n",
    "#             )\n",
    "#\n",
    "#         points = points.squeeze().numpy()\n",
    "#         plot_generative_model_distribution(ax[t_i, i_i], flow, 1.2, torch.Tensor([0]))\n",
    "#         plot_generative_model_distribution(ax[t_i, i_i], flow, 1.2, torch.Tensor([1]))\n",
    "#         ax[t_i, i_i].scatter(\n",
    "#             class_one[i, 0],\n",
    "#             class_one[i, 1],\n",
    "#             c=\"blue\",\n",
    "#             s=500,\n",
    "#             marker=\"x\",\n",
    "#             label=\"Original\",\n",
    "#         )\n",
    "#         ax[t_i, i_i].scatter(\n",
    "#             points[:, 0],\n",
    "#             points[:, 1],\n",
    "#             c=log_prob,\n",
    "#             cmap=cm.coolwarm,\n",
    "#             label=\"Counterfactuals\",\n",
    "#         )\n",
    "#         ax[t_i, i_i].set_title(f\"Temperature: {t}\")\n",
    "#\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(2, 1, figsize=(10, 15))\n",
    "#\n",
    "# i = 52\n",
    "# with torch.no_grad():\n",
    "#     points, log_prob = cf.sample_and_log_prob(\n",
    "#         1000, context=torch.from_numpy(np.array([class_zero[i]])), temp=1.0\n",
    "#     )\n",
    "# points = points.squeeze().numpy()\n",
    "# plot_generative_model_distribution(\n",
    "#     ax[0], cf, None, torch.Tensor([class_zero[i]]), contourf=True\n",
    "# )\n",
    "# ax[0].scatter(class_zero[i, 0], class_zero[i, 1], c=\"blue\", marker=\"x\", s=500)\n",
    "# ax[0].scatter(\n",
    "#     pair_dataset_train.class_one[:, 0],\n",
    "#     pair_dataset_train.class_one[:, 1],\n",
    "#     c=\"red\",\n",
    "#     s=10,\n",
    "#     alpha=0.5,\n",
    "# )\n",
    "# ax[0].scatter(\n",
    "#     pair_dataset_train.class_zero[:, 0],\n",
    "#     pair_dataset_train.class_zero[:, 1],\n",
    "#     c=\"red\",\n",
    "#     s=10,\n",
    "#     alpha=0.5,\n",
    "# )\n",
    "#\n",
    "# i = 55\n",
    "# with torch.no_grad():\n",
    "#     points, log_prob = cf.sample_and_log_prob(\n",
    "#         1000, context=torch.from_numpy(np.array([class_one[i]])), temp=1.0\n",
    "#     )\n",
    "# points = points.squeeze().numpy()\n",
    "# plot_generative_model_distribution(\n",
    "#     ax[1], cf, None, torch.Tensor([class_one[i]]), contourf=True\n",
    "# )\n",
    "# ax[1].scatter(class_one[i, 0], class_one[i, 1], c=\"blue\", marker=\"x\", s=500)\n",
    "# ax[1].scatter(\n",
    "#     pair_dataset_train.class_one[:, 0],\n",
    "#     pair_dataset_train.class_one[:, 1],\n",
    "#     c=\"red\",\n",
    "#     s=10,\n",
    "#     alpha=0.5,\n",
    "# )\n",
    "# ax[1].scatter(\n",
    "#     pair_dataset_train.class_zero[:, 0],\n",
    "#     pair_dataset_train.class_zero[:, 1],\n",
    "#     c=\"red\",\n",
    "#     s=10,\n",
    "#     alpha=0.5,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Poprawić wizualizacje.\n",
    "1. Zwiększenie temperatury flow'a\n",
    "2. Generacja z odfiltrowywaniem podobnych (+-)\n",
    "3. Metryki z literatury + referencje innych metod\n",
    "4. Metody referencyjne\n",
    "5. Policzyć metryki które mamy dla par z losowaniem 1 punktu\n",
    "6. Przetestować uczenie flow'a na kategorycznych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 1\n",
    "# with torch.no_grad():\n",
    "#     points, log_prob = cf.sample_and_log_prob(\n",
    "#         1000, context=torch.from_numpy(np.array([class_zero[i]]))\n",
    "#     )\n",
    "#\n",
    "# points = points.squeeze().numpy()\n",
    "# # plot points with colorized log prob\n",
    "# fig, ax = plt.subplots(figsize=(10, 10))\n",
    "# # Convert context to a PyTorch tensor\n",
    "# context_tensor = torch.tensor([1.0], dtype=torch.float32)  # Replace 1.2 with desired context value\n",
    "# plot_generative_model_distribution(ax, flow, 1.2, context_tensor)  # Pass tensor as context\n",
    "# ax.scatter(class_zero[i, 0], class_zero[i, 1], c=\"blue\", s=500)\n",
    "# ax.scatter(points[:, 0], points[:, 1], c=log_prob, cmap=cm.coolwarm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "cfs = []\n",
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    for x in dataset.X_test:\n",
    "        x_tensor = x.unsqueeze(0) if x.ndimension() == 1 else x  # Ensure correct tensor shape\n",
    "        points, log_prob = flow.sample_and_log_prob(100, context=x_tensor)\n",
    "        cfs.append(points.cpu())\n",
    "generation_time = time.time() - start_time  # Calculate time taken\n",
    "print(f\"Time to generate all counterfactuals: {generation_time:.4f} seconds\")\n",
    "# replace dims to 1,0,2\n",
    "cfs = torch.stack(cfs).squeeze().permute(1, 0, 2).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in cfs:\n",
    "    print(i.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "cfs = []\n",
    "with torch.no_grad():\n",
    "    for x in dataset.X_test:\n",
    "        points, log_prob = cf.sample_and_log_prob(\n",
    "            100, context=torch.from_numpy(np.array([x]))\n",
    "        )\n",
    "        cfs.append(points)\n",
    "cfs = torch.stack(cfs).squeeze().permute(1, 0, 2).numpy()\n",
    "\n",
    "all_metrics = []\n",
    "for i in tqdm(range(cfs.shape[0])):\n",
    "    metrics = CFMetrics(\n",
    "        X_cf=cfs[i],\n",
    "        y_target=np.abs(dataset.y_test - 1),\n",
    "        X_train=dataset.X_train,\n",
    "        y_train=dataset.y_train,\n",
    "        X_test=dataset.X_test.cpu().numpy(),\n",
    "        y_test=dataset.y_test,\n",
    "        gen_model=flow,\n",
    "        disc_model=disc_model,\n",
    "        continuous_features=dataset.numerical_features,\n",
    "        categorical_features=dataset.categorical_features,\n",
    "        prob_plausibility_threshold=1.2,\n",
    "    )\n",
    "\n",
    "    metric_results = metrics.calc_all_metrics()\n",
    "    metric_results[\"L0 Distance\"] = l0_distance(cfs[i], dataset.X_test)\n",
    "    all_metrics.append(metric_results)\n",
    "\n",
    "# Calculate mean and standard deviation for each metric\n",
    "mean_metrics = {key: np.mean([m[key] for m in all_metrics]) for key in all_metrics[0]}\n",
    "std_metrics = {key: np.std([m[key] for m in all_metrics]) for key in all_metrics[0]}\n",
    "\n",
    "# Print the results\n",
    "for key in mean_metrics:\n",
    "    print(f\"{key}: {mean_metrics[key]:.4f} ± {std_metrics[key]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from metrics import distance, feasibility, constraint_violation, success_rate\n",
    "\n",
    "distance_pd = pd.DataFrame(distance(cfs, dataset.y_test, dataset))\n",
    "\n",
    "feasibility_pd = pd.DataFrame(feasibility(cfs, dataset, dataset.df.columns), columns=['feasibility'])\n",
    "\n",
    "# const_pd = pd.DataFrame(constraint_violation(decoded_cfs, decoded_factuals, dataset), columns=['violation'])\n",
    "\n",
    "success_pd = pd.DataFrame(success_rate(cfs[dataset.df.columns], cf), columns=['success'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = pd.concat([distance_pd, feasibility_pd, success_pd], axis=1)\n",
    "results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
