{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lukasz/miniconda3/envs/globe-ce/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from counterfactuals.datasets import LawDataset, AdultDataset, GermanCreditDataset\n",
    "from counterfactuals.cf_methods.dice import DiceExplainerWrapper\n",
    "from counterfactuals.generative_models import MaskedAutoregressiveFlow\n",
    "from counterfactuals.discriminative_models import MultilayerPerceptron\n",
    "from counterfactuals.losses import MulticlassDiscLoss\n",
    "from counterfactuals.metrics import evaluate_cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    \"adult\": (\n",
    "        AdultDataset(\"../data/adult.csv\"),\n",
    "        \"adult_disc_model.pt\",\n",
    "        \"adult_flow.pth\",\n",
    "    ),\n",
    "    \"law\": (LawDataset(\"../data/law.csv\"), \"law_disc_model.pt\", \"law_flow.pth\"),\n",
    "    \"german\": (\n",
    "        GermanCreditDataset(\"../data/german_credit.csv\"),\n",
    "        \"german_disc_model.pt\",\n",
    "        \"german_flow.pth\",\n",
    "    ),\n",
    "}\n",
    "\n",
    "dataset, disc_model_path, gen_model_path = datasets[\"law\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5000 [00:00<?, ?it/s]/Users/lukasz/Genwro/counterfactuals/counterfactuals/discriminative_models/multilayer_perceptron.py:109: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path))\n",
      "Epoch 138, Train: 0.4974, test: 0.4869, patience: 100:   3%|▎         | 139/5000 [00:02<01:38, 49.20it/s]\n"
     ]
    }
   ],
   "source": [
    "# disc_model = MultilayerPerceptron(dataset.X_test.shape[1], [512, 512], 2)\n",
    "disc_model = MultilayerPerceptron(dataset.X_test.shape[1], [256, 256], 2)\n",
    "disc_model.fit(\n",
    "    dataset.train_dataloader(batch_size=128, shuffle=True),\n",
    "    dataset.test_dataloader(batch_size=128, shuffle=False),\n",
    "    epochs=5000,\n",
    "    patience=100,\n",
    "    lr=1e-3,\n",
    "    checkpoint_path=disc_model_path,\n",
    ")\n",
    "disc_model.load(disc_model_path)\n",
    "# disc_model.load(\"german_disc_model_onehot.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "y_pred = disc_model.predict(dataset.X_test).detach().numpy().flatten()\n",
    "print(\"Test accuracy:\", (y_pred == dataset.y_test).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.y_train = disc_model.predict(dataset.X_train).detach().numpy()\n",
    "dataset.y_test = disc_model.predict(dataset.X_test).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lukasz/Genwro/counterfactuals/counterfactuals/generative_models/maf/maf.py:154: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path))\n"
     ]
    }
   ],
   "source": [
    "gen_model = MaskedAutoregressiveFlow(\n",
    "    features=dataset.X_train.shape[1],\n",
    "    hidden_features=16,\n",
    "    num_blocks_per_layer=4,\n",
    "    num_layers=8,\n",
    "    context_features=1,\n",
    "    batch_norm_within_layers=True,\n",
    "    batch_norm_between_layers=True,\n",
    "    use_random_permutations=True,\n",
    ")\n",
    "train_dataloader = dataset.train_dataloader(\n",
    "    batch_size=256, shuffle=True, noise_lvl=0.03\n",
    ")\n",
    "test_dataloader = dataset.test_dataloader(batch_size=256, shuffle=False)\n",
    "\n",
    "# gen_model.fit(\n",
    "#     train_dataloader,\n",
    "#     train_dataloader,\n",
    "#     learning_rate=1e-3,\n",
    "#     patience=100,\n",
    "#     num_epochs=500,\n",
    "#     checkpoint_path=gen_model_path,\n",
    "# )\n",
    "gen_model.load(gen_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = torch.nn.functional.gumbel_softmax(torch.rand(10, 4), dim=1)\n",
    "# t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 4], [5, 6, 7, 8, 9, 10, 11, 12]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.categorical_features_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4, 5, 6, 7, 8, 9, 10, 11, 12]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dice_ml\n",
    "\n",
    "X_train, y_train = dataset.X_train, dataset.y_train\n",
    "\n",
    "features = dataset.numerical_features + dataset.categorical_features + [\"label\"]\n",
    "features = list(map(str, features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', 'label']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dataframe = pd.DataFrame(\n",
    "    np.concatenate((X_train, y_train.reshape(-1, 1)), axis=1),\n",
    "    columns=features,\n",
    ")\n",
    "\n",
    "dice = dice_ml.Data(\n",
    "    dataframe=input_dataframe,\n",
    "    continuous_features=list(map(str, dataset.numerical_features)),\n",
    "    outcome_name=features[-1],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ohe = list(map(str, dataset.numerical_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_id = str(dataset.numerical_columns[-1] + 1)\n",
    "max_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in dataset.categorical_features_lists:\n",
    "    for i, oh_num in enumerate(feature):\n",
    "        new_ohe.append(f\"{max_id}_{i}.0\")\n",
    "    max_id = str(int(max_id) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0',\n",
       " '1',\n",
       " '2',\n",
       " '3_0.0',\n",
       " '3_1.0',\n",
       " '4_0.0',\n",
       " '4_1.0',\n",
       " '4_2.0',\n",
       " '4_3.0',\n",
       " '4_4.0',\n",
       " '4_5.0',\n",
       " '4_6.0',\n",
       " '4_7.0']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dice.ohe_encoded_feature_names = new_ohe\n",
    "dice.ohe_encoded_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = dice_ml.Model(disc_model, backend=\"PYT\", func=\"ohe-min-max\")\n",
    "\n",
    "exp = dice_ml.Dice(dice, model, method=\"gradient\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.encoded_categorical_feature_indexes = dataset.categorical_features_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTransformer:\n",
    "    def transform(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.transformer = CustomTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_instance = pd.DataFrame(dataset.X_test, columns=features[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(444, 13)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_instance.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/444 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_counterfactuals\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_instance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_CFs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesired_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mopposite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposthoc_sparsity_param\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Genwro/DiCE/dice_ml/explainer_interfaces/explainer_base.py:186\u001b[0m, in \u001b[0;36mExplainerBase.generate_counterfactuals\u001b[0;34m(self, query_instances, total_CFs, desired_class, desired_range, permitted_range, features_to_vary, stopping_threshold, posthoc_sparsity_param, proximity_weight, sparsity_weight, diversity_weight, categorical_penalty, posthoc_sparsity_algorithm, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m query_instance \u001b[38;5;129;01min\u001b[39;00m tqdm(query_instances_list):\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_interface\u001b[38;5;241m.\u001b[39mset_continuous_feature_indexes(query_instance)\n\u001b[0;32m--> 186\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_counterfactuals\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_instance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_CFs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdesired_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesired_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdesired_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesired_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpermitted_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpermitted_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeatures_to_vary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures_to_vary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposthoc_sparsity_param\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposthoc_sparsity_param\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposthoc_sparsity_algorithm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposthoc_sparsity_algorithm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m     res\u001b[38;5;241m.\u001b[39mtest_instance_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_interface\u001b[38;5;241m.\u001b[39mensure_consistent_type(\n\u001b[1;32m    198\u001b[0m             res\u001b[38;5;241m.\u001b[39mtest_instance_df, query_instance)\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res\u001b[38;5;241m.\u001b[39mfinal_cfs_df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(res\u001b[38;5;241m.\u001b[39mfinal_cfs_df) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Genwro/DiCE/dice_ml/explainer_interfaces/dice_pytorch.py:123\u001b[0m, in \u001b[0;36mDicePyTorch._generate_counterfactuals\u001b[0;34m(self, query_instance, total_CFs, desired_class, desired_range, proximity_weight, diversity_weight, categorical_penalty, algorithm, features_to_vary, permitted_range, yloss_type, diversity_loss_type, feature_weights, optimizer, learning_rate, min_iter, max_iter, project_iter, loss_diff_thres, loss_converge_maxiter, verbose, init_near_query_instance, tie_random, stopping_threshold, posthoc_sparsity_param, posthoc_sparsity_algorithm, limit_steps_ls)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m [proximity_weight, diversity_weight, categorical_penalty] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhyperparameters:\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_hyperparameters(proximity_weight, diversity_weight, categorical_penalty)\n\u001b[1;32m    122\u001b[0m final_cfs_df, test_instance_df, final_cfs_df_sparse \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_counterfactuals\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_instance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesired_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproject_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_diff_thres\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_converge_maxiter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_near_query_instance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtie_random\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstopping_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposthoc_sparsity_param\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposthoc_sparsity_algorithm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit_steps_ls\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m exp\u001b[38;5;241m.\u001b[39mCounterfactualExamples(\n\u001b[1;32m    129\u001b[0m     data_interface\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_interface,\n\u001b[1;32m    130\u001b[0m     final_cfs_df\u001b[38;5;241m=\u001b[39mfinal_cfs_df,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    133\u001b[0m     posthoc_sparsity_param\u001b[38;5;241m=\u001b[39mposthoc_sparsity_param,\n\u001b[1;32m    134\u001b[0m     desired_class\u001b[38;5;241m=\u001b[39mdesired_class)\n",
      "File \u001b[0;32m~/Genwro/DiCE/dice_ml/explainer_interfaces/dice_pytorch.py:509\u001b[0m, in \u001b[0;36mDicePyTorch.find_counterfactuals\u001b[0;34m(self, query_instance, desired_class, optimizer, learning_rate, min_iter, max_iter, project_iter, loss_diff_thres, loss_converge_maxiter, verbose, init_near_query_instance, tie_random, stopping_threshold, posthoc_sparsity_param, posthoc_sparsity_algorithm, limit_steps_ls)\u001b[0m\n\u001b[1;32m    506\u001b[0m iterations \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;66;03m# backing up CFs if they are valid\u001b[39;00m\n\u001b[0;32m--> 509\u001b[0m temp_cfs_stored \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mround_off_cfs\u001b[49m\u001b[43m(\u001b[49m\u001b[43massign\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    510\u001b[0m test_preds_stored \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_fn(cf) \u001b[38;5;28;01mfor\u001b[39;00m cf \u001b[38;5;129;01min\u001b[39;00m temp_cfs_stored]\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_cf_class \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(i \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstopping_threshold \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m test_preds_stored)) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m    513\u001b[0m    (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_cf_class \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstopping_threshold \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m test_preds_stored))):\n",
      "File \u001b[0;32m~/Genwro/DiCE/dice_ml/explainer_interfaces/dice_pytorch.py:363\u001b[0m, in \u001b[0;36mDicePyTorch.round_off_cfs\u001b[0;34m(self, assign)\u001b[0m\n\u001b[1;32m    361\u001b[0m         ix \u001b[38;5;241m=\u001b[39m maxs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 363\u001b[0m     ix \u001b[38;5;241m=\u001b[39m \u001b[43mmaxs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m vi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(v)):\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m vi \u001b[38;5;241m==\u001b[39m ix:\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "exp.generate_counterfactuals(\n",
    "    query_instance, total_CFs=1, desired_class=\"opposite\", posthoc_sparsity_param=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Discriminator loss: 0.3668, Prob loss: 0.0000: 100%|██████████| 4000/4000 [01:10<00:00, 56.87it/s]  \n"
     ]
    }
   ],
   "source": [
    "cf = DiceExplainerWrapper(\n",
    "    gen_model=gen_model,\n",
    "    disc_model=disc_model,\n",
    "    disc_model_criterion=MulticlassDiscLoss(),\n",
    "    neptune_run=None,\n",
    ")\n",
    "\n",
    "target_class = 0\n",
    "X_test_origin = dataset.X_test[dataset.y_test != target_class]\n",
    "y_test_origin = dataset.y_test[dataset.y_test != target_class]\n",
    "\n",
    "cf_dataloader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(\n",
    "        torch.tensor(X_test_origin).float(),\n",
    "        torch.tensor(y_test_origin).float(),\n",
    "    ),\n",
    "    batch_size=1024,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "\n",
    "log_prob_threshold = torch.quantile(gen_model.predict_log_prob(cf_dataloader), 0.25)\n",
    "deltas, X_orig, y_orig, y_target, logs = cf.explain_dataloader(\n",
    "    cf_dataloader,\n",
    "    alpha=100,\n",
    "    log_prob_threshold=log_prob_threshold,\n",
    "    epochs=4000,\n",
    "    lr=0.001,\n",
    "    categorical_intervals=dataset.categorical_features_lists,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_cf = X_orig + deltas\n",
    "\n",
    "# evaluate_cf(\n",
    "#     disc_model=disc_model,\n",
    "#     gen_model=gen_model,\n",
    "#     X_cf=X_cf,\n",
    "#     model_returned=np.ones(X_cf.shape[0]),\n",
    "#     continuous_features=dataset.numerical_features,\n",
    "#     categorical_features=dataset.categorical_features,\n",
    "#     X_train=dataset.X_train,\n",
    "#     y_train=dataset.y_train,\n",
    "#     X_test=X_orig,\n",
    "#     y_test=y_orig,\n",
    "#     median_log_prob=log_prob_threshold,\n",
    "#     y_target=y_target,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn.functional.gumbel_softmax(torch.rand(4, 3), tau=0.1, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disc_model = MultilayerPerceptron(dataset.X_test.shape[1], [512, 512], 2)\n",
    "# disc_model.load(\"german_disc_model_onehot.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cf = X_orig + deltas\n",
    "X_cf_cat = X_cf.copy()\n",
    "\n",
    "# X_cf_cat = torch.tensor(X_cf_cat)\n",
    "# for interval in dataset.intervals:\n",
    "#     begin, end = interval\n",
    "#     X_cf_cat[:, begin:end] = torch.nn.functional.gumbel_softmax(\n",
    "#         X_cf_cat[:, begin:end], tau=0.1, dim=1\n",
    "#     )\n",
    "# X_cf_cat[:, begin:end] = torch.nn.functional.softmax(X_cf_cat[:, begin:end], dim=1)\n",
    "# X_cf_cat = X_cf_cat.numpy()\n",
    "\n",
    "for interval in dataset.categorical_features_lists:\n",
    "    max_indices = np.argmax(X_cf_cat[:, interval], axis=1)\n",
    "    X_cf_cat[:, interval] = np.eye(X_cf_cat[:, interval].shape[1])[max_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'coverage': 1.0,\n",
       " 'validity': 0.7323943661971831,\n",
       " 'actionability': 0.0,\n",
       " 'sparsity': 0.8461538461538461,\n",
       " 'proximity_categorical_hamming': 0.7041734617815552,\n",
       " 'proximity_categorical_jaccard': 0.7041734617815552,\n",
       " 'proximity_continuous_manhattan': 0.758775208960976,\n",
       " 'proximity_continuous_euclidean': 0.7041734617815552,\n",
       " 'proximity_continuous_mad': 1.8347812809887722,\n",
       " 'proximity_l2_jaccard': 0.7041734617815552,\n",
       " 'proximity_mad_hamming': 1.8347812809887722,\n",
       " 'prob_plausibility': 1.0,\n",
       " 'log_density_cf': -183.50435,\n",
       " 'log_density_test': -14291.4,\n",
       " 'lof_scores_cf': 1.7357957,\n",
       " 'lof_scores_test': 2.9949172,\n",
       " 'isolation_forest_scores_cf': -0.013422678107123704,\n",
       " 'isolation_forest_scores_test': -0.05890787676718063}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_cf(\n",
    "    disc_model=disc_model,\n",
    "    gen_model=gen_model,\n",
    "    X_cf=X_cf_cat,\n",
    "    model_returned=np.ones(X_cf_cat.shape[0]),\n",
    "    continuous_features=dataset.numerical_features,\n",
    "    categorical_features=dataset.categorical_features,\n",
    "    X_train=dataset.X_train,\n",
    "    y_train=dataset.y_train,\n",
    "    X_test=X_orig,\n",
    "    y_test=y_orig,\n",
    "    median_log_prob=log_prob_threshold,\n",
    "    y_target=y_target,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "# import bisect\n",
    "\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import KFold\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# SEED = 42\n",
    "\n",
    "# class TargetEncoderNormalizingDataCatalog():\n",
    "#     def __init__(self, data):\n",
    "#         self.data_frame = data.raw\n",
    "#         self.continous = data.continous\n",
    "#         self.categoricals = data.categoricals\n",
    "#         self.feature_names = self.categoricals + self.continous\n",
    "#         self.scaler = StandardScaler()\n",
    "#         self.target = data.target\n",
    "#         self.data_catalog = data\n",
    "#         self.convert_to_target_encoding_form()\n",
    "#         self.normalize_feature()\n",
    "#         self.encoded_feature_name = \"\"\n",
    "#         self.immutables = data.immutables\n",
    "\n",
    "#     def convert_to_target_encoding_form(self):\n",
    "#         self.cat_dict = {}\n",
    "#         self.target_encoded_dict = {}\n",
    "#         for feature in self.categoricals:\n",
    "#             tmp_dict = defaultdict(lambda: 0)\n",
    "#             data_tmp = pd.DataFrame({feature: self.data_frame[feature], self.target: self.data_frame[self.target]})\n",
    "#             target_mean = data_tmp.groupby(feature)[self.target].mean()\n",
    "#             self.target_encoded_dict[feature] = target_mean\n",
    "#             for cat in target_mean.index.tolist():\n",
    "#                 tmp_dict[cat] = target_mean[cat]\n",
    "#             self.cat_dict[feature] = dict(tmp_dict)\n",
    "\n",
    "#             tmp = np.repeat(np.nan, self.data_frame.shape[0])\n",
    "#             kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
    "#             for idx_1, idx_2 in kf.split(self.data_frame):\n",
    "#                 target_mean = data_tmp.iloc[idx_1].groupby(feature)[self.target].mean()\n",
    "#                 tmp[idx_2] = self.data_frame[feature].iloc[idx_2].map(target_mean)\n",
    "#             self.data_frame[feature] = tmp\n",
    "\n",
    "#         self.data_frame[self.categoricals] = self.data_frame[self.categoricals].astype('float')\n",
    "\n",
    "#     def normalize_feature(self):\n",
    "#         self.data_frame[self.feature_names] = self.scaler.fit_transform(self.data_frame[self.feature_names])\n",
    "\n",
    "#     def denormalize_continuous_feature(self, df):\n",
    "#         df[self.feature_names] = self.scaler.inverse_transform(df[self.feature_names])\n",
    "#         return df\n",
    "\n",
    "#     def convert_from_targetenc_to_original_forms(self, df):\n",
    "#         for cat in self.categoricals:\n",
    "#             d = self.cat_dict[cat]\n",
    "#             # ソート済みのキーと値のリストを作成\n",
    "#             sorted_keys = sorted(d.keys(), key=lambda k: d[k])\n",
    "#             sorted_values = [d[k] for k in sorted_keys]\n",
    "\n",
    "#             values = df[cat].values\n",
    "#             replace_values = []\n",
    "#             for val in values:\n",
    "#                 # 二分探索でbに最も近い値のインデックスを見つける\n",
    "#                 index = bisect.bisect_left(sorted_values, val)\n",
    "\n",
    "#                 # 最も近い値のインデックスを範囲内に収める\n",
    "#                 if index == len(sorted_values):\n",
    "#                     index -= 1\n",
    "#                 elif index > 0 and abs(sorted_values[index] - val) > abs(sorted_values[index - 1] - val):\n",
    "#                     index -= 1\n",
    "\n",
    "#                 # 最も絶対値の差が小さいキーを見つける\n",
    "#                 closest_key = sorted_keys[index]\n",
    "#                 replace_values.append(closest_key)\n",
    "#             df[cat] = replace_values\n",
    "#         return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns = {\n",
    "#     \"compas\": [\"Sex\", \"Age_Cat\", \"Race\", \"C_Charge_Degree\",\n",
    "#                 \"Priors_Count\", \"Time_Served\", \"Status\"],\n",
    "#     \"german_credit\": [\"Existing-Account-Status\", \"Month-Duration\",\n",
    "#                         \"Credit-History\", \"Purpose\", \"Credit-Amount\",\n",
    "#                         \"Savings-Account\", \"Present-Employment\", \"Instalment-Rate\",\n",
    "#                         \"Sex\", \"Guarantors\", \"Residence\",\"Property\", \"Age\",\n",
    "#                         \"Installment\", \"Housing\", \"Existing-Credits\", \"Job\",\n",
    "#                         \"Num-People\", \"Telephone\", \"Foreign-Worker\", \"Status\"],\n",
    "#     \"adult_income\": [\"Age\", \"Workclass\", \"Fnlwgt\", \"Education\", \"Marital-Status\",\n",
    "#                         \"Occupation\", \"Relationship\", \"Race\", \"Sex\", \"Capital-Gain\",\n",
    "#                         \"Capital-Loss\", \"Hours-Per-Week\", \"Native-Country\", \"Status\"],\n",
    "#     \"default_credit\": ['Limit_Bal', 'Sex', 'Education', 'Marriage', 'Age', 'Pay_0',\n",
    "#                         'Pay_2', 'Pay_3', 'Pay_4', 'Pay_5', 'Pay_6', 'Bill_Amt1',\n",
    "#                         'Bill_Amt2', 'Bill_Amt3', 'Bill_Amt4', 'Bill_Amt5',\n",
    "#                         'Bill_Amt6', 'Pay_Amt1', 'Pay_Amt2', 'Pay_Amt3', 'Pay_Amt4',\n",
    "#                         'Pay_Amt5', 'Pay_Amt6', 'Status'],\n",
    "#     \"heloc\": ['ExternalRiskEstimate', 'MSinceOldestTradeOpen',\n",
    "#                 'MSinceMostRecentTradeOpen', 'AverageMInFile',\n",
    "#                 'NumSatisfactoryTrades', 'NumTrades60Ever2DerogPubRec',\n",
    "#                 'NumTrades90Ever2DerogPubRec', 'PercentTradesNeverDelq',\n",
    "#                 'MSinceMostRecentDelq', 'MaxDelq2PublicRecLast12M', 'MaxDelqEver',\n",
    "#                 'NumTotalTrades', 'NumTradesOpeninLast12M', 'PercentInstallTrades',\n",
    "#                 'MSinceMostRecentInqexcl7days', 'NumInqLast6M',\n",
    "#                 'NumInqLast6Mexcl7days', 'NetFractionRevolvingBurden',\n",
    "#                 'NetFractionInstallBurden', 'NumRevolvingTradesWBalance',\n",
    "#                 'NumInstallTradesWBalance', 'NumBank2NatlTradesWHighUtilization',\n",
    "#                 'PercentTradesWBalance', 'Status']\n",
    "# }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "globe-ce",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
