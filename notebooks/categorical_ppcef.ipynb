{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from counterfactuals.datasets import LawDataset, AdultDataset, GermanCreditDataset\n",
    "from counterfactuals.cf_methods.ppcef import PPCEF\n",
    "from counterfactuals.generative_models import MaskedAutoregressiveFlow\n",
    "from counterfactuals.discriminative_models import MultilayerPerceptron\n",
    "from counterfactuals.losses import MulticlassDiscLoss\n",
    "from counterfactuals.metrics import evaluate_cf\n",
    "from counterfactuals.datasets.utils import (\n",
    "    dequantize,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    \"adult\": (\n",
    "        AdultDataset(\"../data/adult.csv\"),\n",
    "        \"adult_disc_model.pt\",\n",
    "        \"adult_flow.pth\",\n",
    "    ),\n",
    "    \"law\": (\n",
    "        LawDataset(\"../data/law.csv\"),\n",
    "        \"law_disc_model.pt\",\n",
    "        \"law_flow.pth\",\n",
    "    ),\n",
    "    \"german\": (\n",
    "        GermanCreditDataset(\"../data/german_credit.csv\"),\n",
    "        \"german_disc_model.pt\",\n",
    "        \"german_flow.pth\",\n",
    "    ),\n",
    "}\n",
    "\n",
    "dataset, disc_model_path, gen_model_path = datasets[\"adult\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lukasz/genwro/counterfactuals/counterfactuals/discriminative_models/multilayer_perceptron.py:110: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path))\n"
     ]
    }
   ],
   "source": [
    "# disc_model = MultilayerPerceptron(dataset.X_test.shape[1], [512, 512], 2)\n",
    "disc_model = MultilayerPerceptron(dataset.X_test.shape[1], [256, 256], 2)\n",
    "# disc_model.fit(\n",
    "#     dataset.train_dataloader(batch_size=128, shuffle=True),\n",
    "#     dataset.test_dataloader(batch_size=128, shuffle=False),\n",
    "#     epochs=5000,\n",
    "#     patience=100,\n",
    "#     lr=1e-3,\n",
    "#     checkpoint_path=disc_model_path,\n",
    "# )\n",
    "disc_model.load(disc_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.84765625\n"
     ]
    }
   ],
   "source": [
    "y_pred = disc_model.predict(dataset.X_test).detach().numpy().flatten()\n",
    "print(\"Test accuracy:\", (y_pred == dataset.y_test).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.y_train = disc_model.predict(dataset.X_train).detach().numpy()\n",
    "dataset.y_test = disc_model.predict(dataset.X_test).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dequantizer, _ = dequantize(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lukasz/genwro/counterfactuals/counterfactuals/generative_models/maf/maf.py:154: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path))\n"
     ]
    }
   ],
   "source": [
    "gen_model = MaskedAutoregressiveFlow(\n",
    "    features=dataset.X_train.shape[1],\n",
    "    hidden_features=16,\n",
    "    num_blocks_per_layer=4,\n",
    "    num_layers=8,\n",
    "    context_features=1,\n",
    "    batch_norm_within_layers=True,\n",
    "    batch_norm_between_layers=True,\n",
    "    use_random_permutations=True,\n",
    ")\n",
    "train_dataloader = dataset.train_dataloader(\n",
    "    batch_size=256, shuffle=True, noise_lvl=0.03\n",
    ")\n",
    "test_dataloader = dataset.test_dataloader(batch_size=256, shuffle=False)\n",
    "\n",
    "# gen_model.fit(\n",
    "#     train_dataloader,\n",
    "#     train_dataloader,\n",
    "#     learning_rate=1e-3,\n",
    "#     patience=100,\n",
    "#     num_epochs=500,\n",
    "#     checkpoint_path=gen_model_path,\n",
    "# )\n",
    "gen_model.load(gen_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, _, _ = datasets[\"adult\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DequantizerWrapper:\n",
    "    def __init__(self, dequantizer):\n",
    "        self.dequantizer = dequantizer\n",
    "\n",
    "    def __call__(self, x):\n",
    "        data_copy = x.copy()\n",
    "\n",
    "        for i, group in enumerate(dataset.categorical_features_lists):\n",
    "            transformer_name = f\"cat_group_{i}\"\n",
    "\n",
    "            group_data = data_copy[:, group]\n",
    "\n",
    "            transformed_data = self.dequantizer.named_transformers_[\n",
    "                transformer_name\n",
    "            ].transform(group_data)\n",
    "\n",
    "            for j, feature_idx in enumerate(group):\n",
    "                data_copy[:, feature_idx] = transformed_data[:, j]\n",
    "\n",
    "        return data_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AdultDataset(\"../data/adult.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from counterfactuals.datasets.torch_utils import TorchCategoricalTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dividers = [1 for _ in range(len(dataset.numerical_features))] + [\n",
    "    2 for _ in range(len(dataset.categorical_features))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "dequantizer_torch = TorchCategoricalTransformer(dividers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dequantizer_wrapper = DequantizerWrapper(dequantizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.273973</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.410959</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.465753</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.328767</td>\n",
       "      <td>0.346939</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.232877</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>0.397260</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>0.424658</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>0.191781</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>0.547945</td>\n",
       "      <td>0.295918</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>0.301370</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>256 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1    2    3    4    5   ...   23   24   25   26   27   28\n",
       "0    0.273973  1.000000  0.0  0.0  1.0  0.0  ...  0.0  0.0  0.0  1.0  0.0  1.0\n",
       "1    0.410959  0.500000  0.0  0.0  1.0  0.0  ...  0.0  0.0  0.0  1.0  0.0  1.0\n",
       "2    0.465753  0.397959  0.0  0.0  1.0  0.0  ...  0.0  0.0  1.0  0.0  0.0  1.0\n",
       "3    0.328767  0.346939  0.0  0.0  1.0  0.0  ...  0.0  0.0  0.0  1.0  0.0  1.0\n",
       "4    0.232877  0.397959  0.0  0.0  1.0  0.0  ...  0.0  0.0  0.0  1.0  1.0  0.0\n",
       "..        ...       ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...\n",
       "251  0.397260  0.397959  0.0  0.0  1.0  0.0  ...  0.0  0.0  0.0  1.0  1.0  0.0\n",
       "252  0.424658  0.397959  1.0  0.0  0.0  0.0  ...  0.0  1.0  0.0  1.0  0.0  1.0\n",
       "253  0.191781  0.397959  0.0  0.0  1.0  0.0  ...  0.0  1.0  0.0  1.0  0.0  1.0\n",
       "254  0.547945  0.295918  0.0  0.0  1.0  0.0  ...  1.0  0.0  1.0  0.0  1.0  0.0\n",
       "255  0.301370  0.397959  0.0  0.0  1.0  0.0  ...  0.0  0.0  0.0  1.0  0.0  1.0\n",
       "\n",
       "[256 rows x 29 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(dataset.X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.273973</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.170297</td>\n",
       "      <td>-0.934777</td>\n",
       "      <td>2.933995</td>\n",
       "      <td>-1.650979</td>\n",
       "      <td>-1.178828</td>\n",
       "      <td>-0.997925</td>\n",
       "      <td>-1.261674</td>\n",
       "      <td>1.993535</td>\n",
       "      <td>-0.235556</td>\n",
       "      <td>-1.585705</td>\n",
       "      <td>-1.628724</td>\n",
       "      <td>-1.306919</td>\n",
       "      <td>-1.329526</td>\n",
       "      <td>0.724896</td>\n",
       "      <td>-0.757682</td>\n",
       "      <td>-0.926177</td>\n",
       "      <td>-0.583801</td>\n",
       "      <td>1.171040</td>\n",
       "      <td>-1.387497</td>\n",
       "      <td>-0.786720</td>\n",
       "      <td>-0.538330</td>\n",
       "      <td>-0.737916</td>\n",
       "      <td>-0.633819</td>\n",
       "      <td>-1.420151</td>\n",
       "      <td>0.978214</td>\n",
       "      <td>-1.325772</td>\n",
       "      <td>1.222409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.410959</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.377397</td>\n",
       "      <td>-0.467473</td>\n",
       "      <td>0.746473</td>\n",
       "      <td>-0.236984</td>\n",
       "      <td>-2.189113</td>\n",
       "      <td>-0.884691</td>\n",
       "      <td>-0.636885</td>\n",
       "      <td>-0.797540</td>\n",
       "      <td>-1.235138</td>\n",
       "      <td>-1.827757</td>\n",
       "      <td>-1.209382</td>\n",
       "      <td>0.567342</td>\n",
       "      <td>-0.525872</td>\n",
       "      <td>1.941185</td>\n",
       "      <td>-1.616271</td>\n",
       "      <td>-1.423881</td>\n",
       "      <td>-0.351181</td>\n",
       "      <td>-2.321211</td>\n",
       "      <td>-1.461822</td>\n",
       "      <td>2.235583</td>\n",
       "      <td>-1.843709</td>\n",
       "      <td>-1.147979</td>\n",
       "      <td>-1.838447</td>\n",
       "      <td>-0.965943</td>\n",
       "      <td>0.214690</td>\n",
       "      <td>-1.151894</td>\n",
       "      <td>1.699486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.465753</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>-0.603044</td>\n",
       "      <td>-0.976545</td>\n",
       "      <td>1.336800</td>\n",
       "      <td>-0.679214</td>\n",
       "      <td>-0.574513</td>\n",
       "      <td>-1.911274</td>\n",
       "      <td>-0.833291</td>\n",
       "      <td>2.319466</td>\n",
       "      <td>-1.701604</td>\n",
       "      <td>-0.939376</td>\n",
       "      <td>-0.637641</td>\n",
       "      <td>-1.224242</td>\n",
       "      <td>-0.530315</td>\n",
       "      <td>1.156975</td>\n",
       "      <td>-0.796727</td>\n",
       "      <td>-2.093104</td>\n",
       "      <td>-0.721547</td>\n",
       "      <td>1.479964</td>\n",
       "      <td>-1.263314</td>\n",
       "      <td>-0.545035</td>\n",
       "      <td>-0.570605</td>\n",
       "      <td>-2.470812</td>\n",
       "      <td>-1.421225</td>\n",
       "      <td>2.302171</td>\n",
       "      <td>-0.317172</td>\n",
       "      <td>-0.278790</td>\n",
       "      <td>0.955953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.328767</td>\n",
       "      <td>0.346939</td>\n",
       "      <td>-0.998668</td>\n",
       "      <td>-1.247520</td>\n",
       "      <td>0.250421</td>\n",
       "      <td>-1.270245</td>\n",
       "      <td>-1.082763</td>\n",
       "      <td>-2.700841</td>\n",
       "      <td>-0.839767</td>\n",
       "      <td>-0.801358</td>\n",
       "      <td>-0.365925</td>\n",
       "      <td>-1.636058</td>\n",
       "      <td>1.087114</td>\n",
       "      <td>-2.504772</td>\n",
       "      <td>-0.564647</td>\n",
       "      <td>1.490057</td>\n",
       "      <td>-0.132079</td>\n",
       "      <td>-0.510187</td>\n",
       "      <td>-1.022088</td>\n",
       "      <td>2.006005</td>\n",
       "      <td>-1.560177</td>\n",
       "      <td>-1.263640</td>\n",
       "      <td>-1.248062</td>\n",
       "      <td>-1.794741</td>\n",
       "      <td>-1.012490</td>\n",
       "      <td>-0.381033</td>\n",
       "      <td>2.396269</td>\n",
       "      <td>-0.587415</td>\n",
       "      <td>0.563770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.232877</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>-1.287165</td>\n",
       "      <td>-2.231602</td>\n",
       "      <td>1.477300</td>\n",
       "      <td>-1.394033</td>\n",
       "      <td>-1.265493</td>\n",
       "      <td>0.805548</td>\n",
       "      <td>-0.807351</td>\n",
       "      <td>-1.035606</td>\n",
       "      <td>-1.060286</td>\n",
       "      <td>-1.544641</td>\n",
       "      <td>-1.030638</td>\n",
       "      <td>-0.932146</td>\n",
       "      <td>-2.007001</td>\n",
       "      <td>-1.980569</td>\n",
       "      <td>-0.459191</td>\n",
       "      <td>1.466244</td>\n",
       "      <td>-0.752555</td>\n",
       "      <td>-0.984483</td>\n",
       "      <td>-1.491629</td>\n",
       "      <td>2.228874</td>\n",
       "      <td>-0.567328</td>\n",
       "      <td>-1.670483</td>\n",
       "      <td>-1.199868</td>\n",
       "      <td>-1.240672</td>\n",
       "      <td>0.976455</td>\n",
       "      <td>0.334471</td>\n",
       "      <td>-0.916839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>0.397260</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>-0.756036</td>\n",
       "      <td>-0.620906</td>\n",
       "      <td>2.224864</td>\n",
       "      <td>-1.007287</td>\n",
       "      <td>-2.346214</td>\n",
       "      <td>-1.485558</td>\n",
       "      <td>-0.257130</td>\n",
       "      <td>-2.582365</td>\n",
       "      <td>-1.208548</td>\n",
       "      <td>-1.525567</td>\n",
       "      <td>-1.414763</td>\n",
       "      <td>1.286874</td>\n",
       "      <td>0.737389</td>\n",
       "      <td>-0.623332</td>\n",
       "      <td>-1.563213</td>\n",
       "      <td>-1.078997</td>\n",
       "      <td>-0.815224</td>\n",
       "      <td>-0.788494</td>\n",
       "      <td>-1.694611</td>\n",
       "      <td>-0.815186</td>\n",
       "      <td>1.422865</td>\n",
       "      <td>-0.455965</td>\n",
       "      <td>-0.266515</td>\n",
       "      <td>-0.385061</td>\n",
       "      <td>1.704159</td>\n",
       "      <td>1.833749</td>\n",
       "      <td>-3.746720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>0.424658</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>1.349905</td>\n",
       "      <td>-1.141819</td>\n",
       "      <td>-0.658627</td>\n",
       "      <td>-0.675131</td>\n",
       "      <td>-1.672927</td>\n",
       "      <td>-1.097559</td>\n",
       "      <td>-1.309924</td>\n",
       "      <td>-1.674365</td>\n",
       "      <td>-2.471585</td>\n",
       "      <td>-1.089029</td>\n",
       "      <td>-1.788346</td>\n",
       "      <td>1.547317</td>\n",
       "      <td>0.732031</td>\n",
       "      <td>-2.279099</td>\n",
       "      <td>-0.463367</td>\n",
       "      <td>-0.699261</td>\n",
       "      <td>-0.580376</td>\n",
       "      <td>-1.012158</td>\n",
       "      <td>-0.948822</td>\n",
       "      <td>-2.855185</td>\n",
       "      <td>-2.677552</td>\n",
       "      <td>-1.016929</td>\n",
       "      <td>2.083496</td>\n",
       "      <td>-1.026978</td>\n",
       "      <td>0.264562</td>\n",
       "      <td>-1.064726</td>\n",
       "      <td>2.402987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>0.191781</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>-1.195338</td>\n",
       "      <td>-0.136758</td>\n",
       "      <td>0.431342</td>\n",
       "      <td>-0.623203</td>\n",
       "      <td>1.891596</td>\n",
       "      <td>-1.117687</td>\n",
       "      <td>-0.613693</td>\n",
       "      <td>-1.798489</td>\n",
       "      <td>-2.156335</td>\n",
       "      <td>-1.430445</td>\n",
       "      <td>-1.012069</td>\n",
       "      <td>-0.885025</td>\n",
       "      <td>-2.026349</td>\n",
       "      <td>0.856738</td>\n",
       "      <td>-2.385995</td>\n",
       "      <td>-2.587605</td>\n",
       "      <td>-1.620425</td>\n",
       "      <td>-1.002967</td>\n",
       "      <td>-0.632535</td>\n",
       "      <td>-2.075387</td>\n",
       "      <td>-2.376551</td>\n",
       "      <td>-1.349528</td>\n",
       "      <td>0.627372</td>\n",
       "      <td>-0.848665</td>\n",
       "      <td>0.747060</td>\n",
       "      <td>-1.486922</td>\n",
       "      <td>2.259174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>0.547945</td>\n",
       "      <td>0.295918</td>\n",
       "      <td>-2.307264</td>\n",
       "      <td>-1.696024</td>\n",
       "      <td>1.017839</td>\n",
       "      <td>-0.895737</td>\n",
       "      <td>-1.050861</td>\n",
       "      <td>-2.239344</td>\n",
       "      <td>-1.319688</td>\n",
       "      <td>-0.342777</td>\n",
       "      <td>-2.027091</td>\n",
       "      <td>-1.738401</td>\n",
       "      <td>1.400112</td>\n",
       "      <td>-0.681375</td>\n",
       "      <td>-1.154985</td>\n",
       "      <td>-0.675662</td>\n",
       "      <td>-2.626636</td>\n",
       "      <td>-1.658227</td>\n",
       "      <td>1.273172</td>\n",
       "      <td>-0.481925</td>\n",
       "      <td>-1.179945</td>\n",
       "      <td>-1.781787</td>\n",
       "      <td>-0.449309</td>\n",
       "      <td>1.614726</td>\n",
       "      <td>-1.808484</td>\n",
       "      <td>1.279507</td>\n",
       "      <td>-2.168596</td>\n",
       "      <td>1.262677</td>\n",
       "      <td>-0.769372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>0.301370</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>-1.226750</td>\n",
       "      <td>-0.522623</td>\n",
       "      <td>0.525694</td>\n",
       "      <td>-1.420017</td>\n",
       "      <td>-0.570497</td>\n",
       "      <td>-0.385785</td>\n",
       "      <td>-0.304177</td>\n",
       "      <td>1.772246</td>\n",
       "      <td>-1.048234</td>\n",
       "      <td>-1.213291</td>\n",
       "      <td>-3.024829</td>\n",
       "      <td>-1.083404</td>\n",
       "      <td>-0.964246</td>\n",
       "      <td>1.664152</td>\n",
       "      <td>-0.692411</td>\n",
       "      <td>-1.440131</td>\n",
       "      <td>-3.772251</td>\n",
       "      <td>2.820607</td>\n",
       "      <td>-1.731505</td>\n",
       "      <td>-1.153456</td>\n",
       "      <td>-1.358992</td>\n",
       "      <td>-1.578122</td>\n",
       "      <td>-2.025153</td>\n",
       "      <td>-0.505491</td>\n",
       "      <td>1.427898</td>\n",
       "      <td>-0.281420</td>\n",
       "      <td>1.659876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>256 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2   ...        26        27        28\n",
       "0    0.273973  1.000000 -0.170297  ...  0.978214 -1.325772  1.222409\n",
       "1    0.410959  0.500000 -0.377397  ...  0.214690 -1.151894  1.699486\n",
       "2    0.465753  0.397959 -0.603044  ... -0.317172 -0.278790  0.955953\n",
       "3    0.328767  0.346939 -0.998668  ...  2.396269 -0.587415  0.563770\n",
       "4    0.232877  0.397959 -1.287165  ...  0.976455  0.334471 -0.916839\n",
       "..        ...       ...       ...  ...       ...       ...       ...\n",
       "251  0.397260  0.397959 -0.756036  ...  1.704159  1.833749 -3.746720\n",
       "252  0.424658  0.397959  1.349905  ...  0.264562 -1.064726  2.402987\n",
       "253  0.191781  0.397959 -1.195338  ...  0.747060 -1.486922  2.259174\n",
       "254  0.547945  0.295918 -2.307264  ... -2.168596  1.262677 -0.769372\n",
       "255  0.301370  0.397959 -1.226750  ...  1.427898 -0.281420  1.659876\n",
       "\n",
       "[256 rows x 29 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(dequantizer_torch(torch.tensor(dataset.X_test)).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [62, 27]], which is output 0 of AsStridedBackward0, is at version 29; expected version 28 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 23\u001b[0m\n\u001b[1;32m     12\u001b[0m cf_dataloader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[1;32m     13\u001b[0m     torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mTensorDataset(\n\u001b[1;32m     14\u001b[0m         torch\u001b[38;5;241m.\u001b[39mtensor(X_test_origin)\u001b[38;5;241m.\u001b[39mfloat(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     22\u001b[0m log_prob_threshold \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mquantile(gen_model\u001b[38;5;241m.\u001b[39mpredict_log_prob(cf_dataloader), \u001b[38;5;241m0.25\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m deltas, X_orig, y_orig, y_target, logs \u001b[38;5;241m=\u001b[39m \u001b[43mcf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplain_dataloader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcf_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_prob_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_prob_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcategorical_intervals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcategorical_features_lists\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdequantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdequantizer_torch\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m log_prob_threshold\n",
      "File \u001b[0;32m~/genwro/counterfactuals/counterfactuals/cf_methods/ppcef/ppcef.py:195\u001b[0m, in \u001b[0;36mPPCEF.explain_dataloader\u001b[0;34m(self, dataloader, epochs, lr, patience_eps, **search_step_kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m loss_components \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_search_step(\n\u001b[1;32m    188\u001b[0m     delta,\n\u001b[1;32m    189\u001b[0m     xs_origin,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msearch_step_kwargs,\n\u001b[1;32m    193\u001b[0m )\n\u001b[1;32m    194\u001b[0m mean_loss \u001b[38;5;241m=\u001b[39m loss_components[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m--> 195\u001b[0m \u001b[43mmean_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m loss_name, loss \u001b[38;5;129;01min\u001b[39;00m loss_components\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/miniconda3/envs/cf/lib/python3.11/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cf/lib/python3.11/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cf/lib/python3.11/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [62, 27]], which is output 0 of AsStridedBackward0, is at version 29; expected version 28 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "cf = PPCEF(\n",
    "    gen_model=gen_model,\n",
    "    disc_model=disc_model,\n",
    "    disc_model_criterion=MulticlassDiscLoss(),\n",
    "    neptune_run=None,\n",
    ")\n",
    "\n",
    "target_class = 0\n",
    "X_test_origin = dataset.X_test[dataset.y_test != target_class]\n",
    "y_test_origin = dataset.y_test[dataset.y_test != target_class]\n",
    "\n",
    "cf_dataloader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(\n",
    "        torch.tensor(X_test_origin).float(),\n",
    "        torch.tensor(y_test_origin).float(),\n",
    "    ),\n",
    "    batch_size=1024,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "\n",
    "log_prob_threshold = torch.quantile(gen_model.predict_log_prob(cf_dataloader), 0.25)\n",
    "deltas, X_orig, y_orig, y_target, logs = cf.explain_dataloader(\n",
    "    cf_dataloader,\n",
    "    alpha=100,\n",
    "    log_prob_threshold=log_prob_threshold,\n",
    "    epochs=10000,\n",
    "    lr=0.001,\n",
    "    categorical_intervals=dataset.categorical_features_lists,\n",
    "    dequantizer=dequantizer_torch,\n",
    ")\n",
    "log_prob_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cf = X_orig + deltas\n",
    "X_cf_cat = X_cf.copy()\n",
    "\n",
    "for interval in dataset.categorical_features_lists:\n",
    "    max_indices = np.argmax(X_cf_cat[:, interval], axis=1)\n",
    "    X_cf_cat[:, interval] = np.eye(X_cf_cat[:, interval].shape[1])[max_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for categorical_features, transform in zip(\n",
    "#         dataset.categorical_features_lists, dequantizer.named_transformers_\n",
    "#     ):\n",
    "\n",
    "#     X_cf[:, categorical_features] = dequantizer.named_transformers_[\n",
    "#         transform\n",
    "#     ].inverse_transform(X_cf[:, list(range(len(categorical_features)))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_cf_deq = inverse_dequantize(dataset, dequantizer, X_cf_cat)\n",
    "_, X_cf_q = dequantize(dataset, X_cf_cat, dequantizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 14:10:16,301 - counterfactuals.metrics.distances - INFO - Calculating combined distance\n",
      "2025-04-19 14:10:16,301 - counterfactuals.metrics.distances - INFO - Calculating continuous distance\n",
      "2025-04-19 14:10:16,302 - counterfactuals.metrics.distances - INFO - Calculating categorical distance\n",
      "2025-04-19 14:10:16,304 - counterfactuals.metrics.distances - INFO - Calculating combined distance\n",
      "2025-04-19 14:10:16,305 - counterfactuals.metrics.distances - INFO - Calculating continuous distance\n",
      "2025-04-19 14:10:16,306 - counterfactuals.metrics.distances - INFO - Calculating categorical distance\n",
      "2025-04-19 14:10:16,307 - counterfactuals.metrics.distances - INFO - Calculating combined distance\n",
      "2025-04-19 14:10:16,307 - counterfactuals.metrics.distances - INFO - Calculating continuous distance\n",
      "2025-04-19 14:10:16,308 - counterfactuals.metrics.distances - INFO - Calculating categorical distance\n",
      "2025-04-19 14:10:16,308 - counterfactuals.metrics.distances - INFO - Calculating combined distance\n",
      "2025-04-19 14:10:16,308 - counterfactuals.metrics.distances - INFO - Calculating continuous distance\n",
      "2025-04-19 14:10:16,309 - counterfactuals.metrics.distances - INFO - Calculating categorical distance\n",
      "2025-04-19 14:10:16,309 - counterfactuals.metrics.distances - INFO - Calculating combined distance\n",
      "2025-04-19 14:10:16,309 - counterfactuals.metrics.distances - INFO - Calculating continuous distance\n",
      "2025-04-19 14:10:16,310 - counterfactuals.metrics.distances - INFO - Calculating categorical distance\n",
      "2025-04-19 14:10:16,311 - counterfactuals.metrics.distances - INFO - Calculating combined distance\n",
      "2025-04-19 14:10:16,311 - counterfactuals.metrics.distances - INFO - Calculating continuous distance\n",
      "2025-04-19 14:10:16,311 - counterfactuals.metrics.distances - INFO - Calculating categorical distance\n",
      "2025-04-19 14:10:16,311 - counterfactuals.metrics.distances - INFO - Calculating combined distance\n",
      "2025-04-19 14:10:16,312 - counterfactuals.metrics.distances - INFO - Calculating continuous distance\n",
      "2025-04-19 14:10:16,313 - counterfactuals.metrics.distances - INFO - Calculating categorical distance\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'coverage': 1.0,\n",
       " 'validity': 0.3090909090909091,\n",
       " 'actionability': 0.0,\n",
       " 'sparsity': 1.0,\n",
       " 'proximity_categorical_hamming': 0.9749532019011216,\n",
       " 'proximity_categorical_jaccard': 0.9749532019011216,\n",
       " 'proximity_continuous_manhattan': 0.9884099700085773,\n",
       " 'proximity_continuous_euclidean': 0.9749532019011216,\n",
       " 'proximity_continuous_mad': 1.861275817027682,\n",
       " 'proximity_l2_jaccard': 0.9749532019011216,\n",
       " 'proximity_mad_hamming': 1.861275817027682,\n",
       " 'prob_plausibility': 0.18181818181818182,\n",
       " 'log_density_cf': -45.080257,\n",
       " 'log_density_test': -82.857475,\n",
       " 'lof_scores_cf': 1.0623248,\n",
       " 'lof_scores_test': 1.0580301,\n",
       " 'isolation_forest_scores_cf': 0.06439388425658807,\n",
       " 'isolation_forest_scores_test': 0.07902376467016735}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_cf(\n",
    "    disc_model=disc_model,\n",
    "    gen_model=gen_model,\n",
    "    X_cf=X_cf_q,\n",
    "    model_returned=np.ones(X_cf_cat.shape[0]),\n",
    "    continuous_features=dataset.numerical_features,\n",
    "    categorical_features=dataset.categorical_features,\n",
    "    X_train=dataset.X_train,\n",
    "    y_train=dataset.y_train,\n",
    "    X_test=X_orig,\n",
    "    y_test=y_orig,\n",
    "    median_log_prob=log_prob_threshold,\n",
    "    y_target=y_target,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
