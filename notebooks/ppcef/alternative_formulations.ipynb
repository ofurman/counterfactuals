{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets as datasets\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import Adam\n",
    "\n",
    "from nflows.flows import MaskedAutoregressiveFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from counterfactuals.utils import plot_distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = datasets.make_moons(512, noise=0.1)\n",
    "plt.scatter(x[:, 0], x[:, 1], c=y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow = MaskedAutoregressiveFlow(features=2, hidden_features=4, context_features=1)\n",
    "optimizer = optim.Adam(flow.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iter = 10000\n",
    "for i in range(num_iter):\n",
    "    x, y = datasets.make_moons(128, noise=0.1)\n",
    "    x = torch.tensor(x, dtype=torch.float32)\n",
    "    y = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)\n",
    "    optimizer.zero_grad()\n",
    "    loss = -flow.log_prob(inputs=x, context=y).mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (i + 1) % 1000 == 0:\n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 1\n",
    "\n",
    "$$ argmin \\quad d(x, x’) - log p(x’|y’)$$\n",
    "$$ log p(x’|y’) = log p(y’|x’)*p(x') $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_origin = torch.tensor([[1.0, 0.0]], requires_grad=False)\n",
    "x = torch.tensor([[1.0, 0.0]], requires_grad=True)\n",
    "\n",
    "\n",
    "def optim_function(x, x_origin, model, alpha=1.0):\n",
    "    model.eval()\n",
    "    # y_orig = torch.zeros(x.shape[0]).reshape(-1, 1)\n",
    "    y_hat = torch.ones(x.shape[0]).reshape(-1, 1)\n",
    "    p_hat = model.log_prob(x, context=y_hat)  # .exp()\n",
    "    dist = torch.linalg.norm(x_origin - x, axis=1)\n",
    "    return dist - p_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam([x], lr=0.01)\n",
    "\n",
    "num_iterations = 50\n",
    "for i in range(num_iterations):\n",
    "    flow.train()\n",
    "    y = optim_function(x, x_origin, flow, alpha=100)\n",
    "    optimizer.zero_grad()\n",
    "    y.backward()\n",
    "    optimizer.step()\n",
    "    if (i + 1) % 10 == 0:\n",
    "        plot_distributions(x, x_origin, flow, optim_function, alpha=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 2\n",
    "\n",
    "$$agmin\\quad d(x, x’) - \\lambda (log p(x’|y’) - log(p(x’|y) + p(x’|y')))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_origin = torch.tensor([[1.0, 0.0]], requires_grad=False)\n",
    "x = torch.tensor([[1.0, 0.0]], requires_grad=True)\n",
    "\n",
    "\n",
    "def optim_function(x, x_origin, model, alpha=1.0):\n",
    "    model.eval()\n",
    "\n",
    "    y_orig = torch.zeros(x.shape[0]).reshape(-1, 1)\n",
    "    y_hat = torch.ones(x.shape[0]).reshape(-1, 1)\n",
    "\n",
    "    dist = torch.linalg.norm(x_origin - x, axis=1)\n",
    "\n",
    "    p_orig = model.log_prob(x, context=y_orig)\n",
    "    p_hat = model.log_prob(x, context=y_hat)\n",
    "\n",
    "    return dist - alpha * (\n",
    "        p_hat - torch.logsumexp(torch.concat([p_orig, p_hat]), dim=0)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam([x], lr=0.01)\n",
    "alpha = 1\n",
    "\n",
    "num_iterations = 50\n",
    "for i in range(num_iterations):\n",
    "    flow.train()\n",
    "    y = optim_function(x, x_origin, flow, alpha=alpha)\n",
    "    optimizer.zero_grad()\n",
    "    y.backward()\n",
    "    optimizer.step()\n",
    "    if (i + 1) % 10 == 0:\n",
    "        # plot_x_point(x, x_origin, flow)\n",
    "        plot_distributions(x, x_origin, flow, optim_function, alpha=alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 3\n",
    "\n",
    "$$ \\beta = 0.01 $$\n",
    "$$\\theta = \\beta + p(x'|y) - p(x'|y') $$\n",
    "$$agmin\\quad d(x, x’) + \\alpha*max(\\theta, 0)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_origin = torch.tensor([[1.0, 0.0]], requires_grad=False)\n",
    "x = torch.tensor([[1.0, 0.0]], requires_grad=True)\n",
    "\n",
    "\n",
    "def optim_function(x, x_origin, model, alpha=20.0, beta=0.01):\n",
    "    model.eval()\n",
    "    dist = torch.linalg.norm(x_origin - x, axis=1)\n",
    "\n",
    "    y_orig = torch.zeros(x.shape[0]).reshape(-1, 1)\n",
    "    y_hat = torch.ones(x.shape[0]).reshape(-1, 1)\n",
    "\n",
    "    p_orig = model.log_prob(x, context=y_orig).exp()\n",
    "    p_hat = model.log_prob(x, context=y_hat).exp()\n",
    "    theta = p_orig - p_hat + beta\n",
    "    max_theta = torch.clamp(theta, min=0)\n",
    "    loss = alpha * max_theta + dist\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam([x], lr=0.01)\n",
    "alpha = 10\n",
    "\n",
    "num_iterations = 100\n",
    "for i in range(num_iterations):\n",
    "    y = optim_function(x, x_origin, flow, alpha=alpha)\n",
    "    optimizer.zero_grad()\n",
    "    y.backward()\n",
    "    optimizer.step()\n",
    "    if (i + 1) % 10 == 0:\n",
    "        plot_distributions(x, x_origin, flow, optim_function, alpha=alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 4\n",
    "$$ \\alpha = 0.001 $$\n",
    "$$agmin\\quad \\alpha * d(x, x’) + |p(x'|y') - p(x|y)|$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_origin = torch.tensor([[1.0, 0.0]], requires_grad=False)\n",
    "x = torch.tensor([[1.0, 0.0]], requires_grad=True)\n",
    "\n",
    "\n",
    "def optim_function(x, x_origin, model, alpha=1.0):\n",
    "    model.eval()\n",
    "    y_orig = torch.zeros(x_origin.shape[0]).reshape(-1, 1)\n",
    "    y_hat = torch.ones(x.shape[0]).reshape(-1, 1)\n",
    "    p_orig = model.log_prob(x_origin, context=y_orig).exp()\n",
    "    p_hat = model.log_prob(x, context=y_hat).exp()\n",
    "    theta = torch.abs(p_orig - p_hat)\n",
    "    dist = torch.linalg.norm(x_origin - x, axis=1)\n",
    "    return alpha * dist + theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam([x], lr=0.1)\n",
    "alpha = 0.001\n",
    "\n",
    "num_iterations = 100\n",
    "for i in range(num_iterations):\n",
    "    y = optim_function(x, x_origin, flow, alpha=alpha)\n",
    "    optimizer.zero_grad()\n",
    "    y.backward()\n",
    "    optimizer.step()\n",
    "    if (i + 1) % 10 == 0:\n",
    "        plot_distributions(x, x_origin, flow, optim_function, alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
