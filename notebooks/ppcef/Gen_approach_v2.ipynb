{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nflows.flows import MaskedAutoregressiveFlow\n",
    "\n",
    "from counterfactuals.datasets import (\n",
    "    AdultDataset,\n",
    "    CompasDataset,\n",
    ")\n",
    "from counterfactuals.discriminative_models import LogisticRegression\n",
    "from counterfactuals.optimizers.base import BaseCounterfactualModel\n",
    "\n",
    "from counterfactuals.metrics.metrics import (\n",
    "    evaluate_cf,\n",
    ")\n",
    "\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CompasDataset(file_path=\"../data/compas_two_years.csv\")\n",
    "dataset = AdultDataset(file_path=\"../data/adult.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disc_model = MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=100)\n",
    "\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# disc_model = LogisticRegression()\n",
    "# disc_model.fit(dataset.X_train, dataset.y_train.reshape(-1))\n",
    "# print(classification_report(dataset.y_test, disc_model.predict(dataset.X_test)))\n",
    "# print(disc_model.predict(dataset.X_test).shape)\n",
    "\n",
    "# build custom module for logistic regression\n",
    "# class LogisticRegression(torch.nn.Module):\n",
    "#     # build the constructor\n",
    "#     def __init__(self, n_inputs, n_outputs):\n",
    "#         super(LogisticRegression, self).__init__()\n",
    "#         self.linear = torch.nn.Linear(n_inputs, n_outputs)\n",
    "#     # make predictions\n",
    "#     def forward(self, x):\n",
    "#         y_pred = torch.sigmoid(self.linear(x))\n",
    "#         return y_pred\n",
    "#     def fit(self, train_loader):\n",
    "#         # defining the optimizer\n",
    "#         optimizer = torch.optim.Adam(self.linear.parameters(), lr=0.01)\n",
    "#         # defining Cross-Entropy loss\n",
    "#         criterion = torch.nn.BCELoss()\n",
    "\n",
    "#         epochs = 200\n",
    "\n",
    "#         for epoch in range(epochs):\n",
    "#             for i, (examples, labels) in enumerate(train_loader):\n",
    "#                 optimizer.zero_grad()\n",
    "#                 outputs = self.forward(examples)\n",
    "#                 labels = labels.reshape(-1, 1)\n",
    "#                 loss = criterion(outputs, labels)\n",
    "#                 # Loss.append(loss.item())\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "\n",
    "#     def predict(self, X_test):\n",
    "#         probs = self.forward(torch.from_numpy(X_test))\n",
    "#         probs = probs > 0.5\n",
    "#         return np.squeeze(probs.numpy().astype(np.float32))\n",
    "\n",
    "\n",
    "disc_model = LogisticRegression(dataset.X_train.shape[1], 1)\n",
    "train_dataloader = dataset.train_dataloader(\n",
    "    batch_size=128, shuffle=True, noise_lvl=1e-5\n",
    ")\n",
    "disc_model.fit(train_dataloader)\n",
    "print(classification_report(dataset.y_test, disc_model.predict(dataset.X_test)))\n",
    "disc_model.predict(dataset.X_test).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relabeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = disc_model.predict(dataset.X_train)\n",
    "y_pred_test = disc_model.predict(dataset.X_test)\n",
    "dataset.y_train = y_pred_train\n",
    "dataset.y_test = y_pred_test\n",
    "\n",
    "# noise_lvl - zaszumianie numerycznych cech treningowego datasetu\n",
    "train_dataloader = dataset.train_dataloader(\n",
    "    batch_size=128, shuffle=True, noise_lvl=1e-5\n",
    ")\n",
    "test_dataloader = dataset.test_dataloader(batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create flow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nflows.flows import SimpleRealNVP\n",
    "\n",
    "# flow = SimpleRealNVP(use_volume_preserving=True, features=dataset.X_train.shape[1], hidden_features=4, context_features=1, num_layers=5)\n",
    "\n",
    "flow = MaskedAutoregressiveFlow(\n",
    "    features=dataset.X_train.shape[1],\n",
    "    hidden_features=4,\n",
    "    num_blocks_per_layer=2,\n",
    "    num_layers=1,\n",
    "    context_features=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define custom search step within class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomApproach(BaseCounterfactualModel):\n",
    "    def search_step(\n",
    "        self, x_param, x_origin, context_origin, context_target, **search_step_kwargs\n",
    "    ):\n",
    "        alpha = search_step_kwargs.get(\"alpha\", None)\n",
    "        beta = search_step_kwargs.get(\"beta\", None)\n",
    "        if alpha is None:\n",
    "            raise ValueError(\"Parameter 'alpha' should be in kwargs\")\n",
    "        if beta is None:\n",
    "            raise ValueError(\"Parameter 'beta' should be in kwargs\")\n",
    "\n",
    "        dist = torch.linalg.norm(x_origin - x_param, axis=1)\n",
    "        self.lr_model.eval()\n",
    "        outputs = self.lr_model.forward(x_param)\n",
    "        loss_d = self.criterion(outputs, context_target)\n",
    "\n",
    "        p_x_param_c_orig = self.gen_model.log_prob(x_param, context=context_origin)\n",
    "        p_x_param_c_target = self.gen_model.log_prob(x_param, context=context_target)\n",
    "        p_x_orig_c_orig = self.gen_model.log_prob(\n",
    "            x_origin, context=context_origin.flatten()[0].repeat((x_origin.shape[0], 1))\n",
    "        )\n",
    "\n",
    "        p_x_param_c_orig_with_beta = p_x_param_c_orig + beta\n",
    "        max_inner = torch.nn.functional.relu(p_x_orig_c_orig - p_x_param_c_target)\n",
    "        max_outer = torch.nn.functional.relu(\n",
    "            p_x_param_c_orig_with_beta - p_x_param_c_target\n",
    "        )\n",
    "        loss = dist + alpha * (max_outer + max_inner + loss_d)\n",
    "        return loss, dist, max_inner, max_outer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create cf class, train and test flow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = CustomApproach(\n",
    "    gen_model=flow, disc_model=disc_model, checkpoint_path=\"model.pt\", neptune_run=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf.lr_model = disc_model\n",
    "cf.criterion = torch.nn.BCELoss()\n",
    "cf.train_model(\n",
    "    train_loader=train_dataloader,\n",
    "    test_loader=test_dataloader,\n",
    "    epochs=100,\n",
    "    patience=20,\n",
    "    eps=1e-3,  # eps for patience\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf.test_model(test_loader=test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search counterfactuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_step_kwargs = {\n",
    "    \"alpha\": 20,\n",
    "    \"beta\": 0.1,\n",
    "}\n",
    "test_dataloader = dataset.test_dataloader(batch_size=16, shuffle=False)\n",
    "Xs_cf, Xs_orig, ys_orig = cf.search_batch(\n",
    "    dataloader=test_dataloader, epochs=1000, lr=0.005, **search_step_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_cf(\n",
    "    cf_class=None,\n",
    "    disc_model=disc_model,\n",
    "    X=Xs_orig,\n",
    "    X_cf=Xs_cf,\n",
    "    model_returned=np.ones(Xs_cf.shape[0]).astype(bool),\n",
    "    continuous_features=dataset.numerical_features,\n",
    "    categorical_features=dataset.categorical_features,\n",
    "    X_train=dataset.X_train,\n",
    "    y_train=dataset.y_train,\n",
    "    X_test=dataset.X_test,\n",
    "    y_test=dataset.y_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
