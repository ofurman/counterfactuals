%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,sigconf')
%%
%% IMPORTANT NOTICE:
%%
%% For the copyright see the source file.
%%
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%%
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%%
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[sigconf]{acmart}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{XXXXXXX.XXXXXXX}
%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Conference acronym 'XX]{Make sure to enter the correct
  conference title from your rights confirmation email}{June 03--05,
  2018}{Woodstock, NY}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
\acmISBN{978-1-4503-XXXX-X/2018/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

\usepackage{xcolor}
\usepackage{listings}
\usepackage{multirow}
\usepackage{realboxes} % Required to safely colorbox verbatim text

\definecolor{lightblue}{RGB}{230, 240, 255}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{lightblue},
    commentstyle=\color{codegreen},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{red},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=4pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2,
    frame=ltb,
    framerule=0pt,
    keywordstyle={\bfseries \color{blue}}
}
\lstset{style=mystyle}
\newcommand{\code}[1]{\Colorbox{lightblue}{\lstinline{#1}}}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{CEL: Comprehensive Counterfactual Explanations\\Library and Benchmark}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.

\author{Oleksii Furman}
\authornote{Both authors contributed equally to this research.}
\email{oleksii.furman@pwr.edu.pl}
\orcid{0009-0001-2184-3096}
\affiliation{
  \institution{Wrocław University of Science and Technology}
  \city{Wrocław}
  \country{Poland}
}


\author{Łukasz Lenkiewicz}
\authornotemark[1]
\email{lukasz.lenkiewicz@pwr.edu.pl}
\orcid{0009-0006-6282-8649}
\affiliation{
  \institution{Wrocław University of Science and Technology}
  \city{Wrocław}
  \country{Poland}
}

\author{Marcel Musiałek}
\email{279704@student.pwr.edu.pl}
\orcid{0009-0009-4964-0547}
\affiliation{
  \institution{Wrocław University of Science and Technology}
  \city{Wrocław}
  \country{Poland}
}

\author{Maciej Zięba}
\email{maciej.zieba@pwr.edu.pl}
\orcid{0000-0003-4217-7712}
\affiliation{
  \institution{Wrocław University of Science and Technology}
  \city{Wrocław}
  \country{Poland}
}


%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Furman et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
Counterfactual explanations are a prominent approach in explainable artificial intelligence (xAI), providing actionable guidance on what input changes would alter a model’s prediction to a desired outcome. While early methods primarily focused on minimal feature changes, recent work incorporates additional properties such as sparsity, actionability and plausibility. Despite this progress, fair and systematic evaluation remains challenging. Existing studies often rely on different data splits, predictive models, and evaluation metrics, which limits objective comparison across methods. To fill this gap, we introduce CEL (\textbf{C}ounterfactual \textbf{E}xplanations \textbf{L}ibrary), a unified library and benchmark for counterfactual explanations designed to support consistent implementation and evaluation. CEL includes 18 datasets of varying size and complexity and provides implementations or reimplementations of 13 widely used counterfactual methods. Using this standardized setup, we conduct a comprehensive quantitative comparison across a variety of methods on datasets that differ in size, number, and types of attributes. The evaluation protocol incorporates multiple complementary metrics capturing validity, coverage, sparsity, proximity, and distributional plausibility, including density- and outlier-based measures to assess the realism of generated counterfactuals. To the best of our knowledge, this is the first comprehensive benchmark that systematically evaluates recent counterfactual explanation methods within a unified and reproducible framework. While prior libraries and benchmarking efforts exist in the literature, many are outdated, limited in scope, or lack consistent evaluation protocols. The proposed benchmark aims to improve reproducibility, enable fair comparison, and establish a workbench for the development of future counterfactual explanation methods.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
% \begin{CCSXML}
% <ccs2012>
%  <concept>
%   <concept_id>00000000.0000000.0000000</concept_id>
%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%   <concept_significance>500</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>00000000.00000000.00000000</concept_id>
%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%   <concept_significance>300</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>00000000.00000000.00000000</concept_id>
%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%   <concept_significance>100</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>00000000.00000000.00000000</concept_id>
%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%   <concept_significance>100</concept_significance>
%  </concept>
% </ccs2012>
% \end{CCSXML}

% \ccsdesc[500]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
% \ccsdesc[300]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
% \ccsdesc{Do Not Use This Code~Generate the Correct Terms for Your Paper}
% \ccsdesc[100]{Do Not Use This Code~Generate the Correct Terms for Your Paper}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Counterfactual Explanations, Explainable Artificial Intelligence, Benchmarking, Machine Learning}
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.
% \begin{teaserfigure}
%   \includegraphics[width=\textwidth]{figures/teaser.pdf}
%   \caption{CEL architecture that demostrates modules implemented in CEL.}
%   \Description{Teaser}
%   \label{fig:teaser}
% \end{teaserfigure}


\received{20 February 2007}
\received[revised]{12 March 2009}
\received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figures/teaser.pdf}
    \caption{Overview of the CEL architecture. It consists of four interconnected modules: (i) a Data Module handling dataset loading, preprocessing, and constraint specification; (ii) a Model Module providing predictive and probabilistic backbones; (iii) an Explanation Engine supporting local, global, and group-wise counterfactual methods; and (iv) a Metrics Orchestrator computing validity, proximity, sparsity, and plausibility measures. Arrows indicate data flow between components.}
    \label{fig:teaser}
\end{figure*}

\section{Introduction}
Machine learning models are increasingly deployed in high-stakes real-world applications. Recent advances have enabled their use in complex domains such as medical image analysis \cite{ronneberger2015u}, recommender systems \cite{pitoura2021fairness}, finance \cite{xie2023pixiu}, and autonomous systems \cite{sancaktar2022curious}. Despite their strong predictive performance, many of these systems rely on highly expressive black-box architectures whose internal decision processes are difficult to interpret. This lack of transparency poses challenges at multiple stages of the model lifecycle: it complicates debugging and validation during development, and it raises regulatory and ethical concerns in deployment. In particular, legal frameworks such as the European Union’s General Data Protection Regulation (GDPR) emphasize the need for meaningful explanations of automated decisions \cite{wachter2017counterfactual}. These challenges have led to the rapid growth of Explainable Artificial Intelligence (XAI), a field devoted to developing methods that provide human-understandable insights into model behavior and decision-making processes.


Counterfactual explanations provide actionable insights by identifying how input features should change to alter a model’s prediction. Early formulations, most notably by Wachter et al.~\cite{wachter2017counterfactual}, focused primarily on two criteria: validity (the counterfactual must achieve the desired prediction) and proximity (the modification from the original instance should be minimal). While intuitive, these constraints alone often lead to unrealistic or impractical explanations.

Subsequent work has expanded the set of desirable properties for counterfactual explanations. In addition to validity and proximity, researchers have emphasized plausibility, requiring counterfactuals to lie in high-density regions of the data distribution; sparsity, encouraging modifications to as few features as possible; and actionability, ensuring that only features that can be feasibly changed in practice are modified \cite{guidotti2024counterfactual}. As a result, modern counterfactual generation methods typically optimize for multiple, and sometimes competing, criteria. This growing set of constraints has significantly increased the complexity of both method design and evaluation.

Counterfactual explanation methods can be broadly categorized into three paradigms: local, global, and group-wise approaches. Local methods generate instance-specific perturbations tailored to individual data points \cite{karimi2020model, wachter2017counterfactual, mothilal2020explaining, pawelczyk2020learning}. While highly actionable at the individual level, they do not provide aggregated insights into overall model behavior. In contrast, global methods aim to identify a single transformation or shift that alters predictions for an entire class \cite{rawal2020beyond, ley2023globe}. These approaches offer a more holistic perspective but may sacrifice instance-level precision. Group-wise methods occupy an intermediate position by discovering multiple perturbations that apply to subgroups within a class \cite{bewley2024counterfactual, kavouras2024glance, furman2024unifying}. This formulation enables structured, segment-level understanding of model behavior while retaining more flexibility than fully global approaches.

Since the introduction of counterfactual explanation methods, their evaluation has remained fundamentally under-specified. For a given instance, multiple valid counterfactuals may exist, each satisfying different subsets of desirable properties \cite{mothilal2020explaining}. Consequently, modern methods optimize for trade-offs among criteria such as validity, proximity, plausibility, sparsity, robustness, and actionability. Evaluating these methods therefore requires a multi-dimensional assessment framework rather than a single scalar metric \cite{guidotti2024counterfactual}.

In practice, however, existing studies use different evaluation setups: metrics are defined differently, subsets of properties are emphasized inconsistently, and experimental settings vary across works \cite{de2021framework, pineau2021improving}. As a result, reported performance comparisons are often not directly comparable.

Beyond differences in metric definitions, studies also vary in their experimental protocol. Works employ different data splits, preprocessing pipelines, feature encodings, constraint definitions and the predictive models being explained. Because counterfactual explanations are inherently model-dependent, variations in backbone architecture can substantially affect both feasibility and quality. Consequently, observed performance differences may reflect changes in experimental configuration rather than methodological advances.

This lack of protocol standardization makes fair comparison difficult and can lead to unstable method rankings across studies. As a result, it is often unclear whether reported improvements reflect genuine methodological advances or simply differences in experimental setup. These limitations hinder reliable measurement of progress in counterfactual explanation research.

To address the fragmentation in counterfactual evaluation, we introduce CEL, a unified benchmark and reproducible evaluation framework for counterfactual explanations. Rather than serving solely as an implementation library, CEL establishes a controlled experimental protocol that standardizes datasets, predictive backbones, preprocessing pipelines, feature constraints, and evaluation metrics. This design enables systematic and fair comparison across different counterfactual generation paradigms. Figure~\ref{fig:teaser} provides a high-level overview of the CEL architecture, illustrating how its modular design connects data management, predictive and generative modeling, explanation generation, and standardized evaluation.

CEL includes 18 curated datasets spanning diverse domains and 13 widely used counterfactual generation methods implemented or reimplemented within a common interface. All methods are evaluated under harmonized training splits, shared predictive models, and a consistent metric taxonomy. In contrast to prior frameworks such as CARLA \cite{pawelczyk2021carla} and other evaluation toolkits \cite{de2021framework}, our benchmark emphasizes protocol control, methodological reproducibility, and comparability across local, global, and group-wise approaches.

\smallskip
\noindent
Our main contributions are:

\begin{itemize}
\item \textbf{A controlled evaluation protocol} for counterfactual explanations that standardizes datasets, preprocessing, predictive backbones, constraint handling, and metric definitions, enabling fair and reproducible comparison.
\item \textbf{A benchmark including} 18 datasets and 13 counterfactual generation methods implemented within a unified framework, supporting evaluation across local, global, and group-wise paradigms.
\item \textbf{An open-source programming library} designed to facilitate future method integration, transparent reporting, and community-driven benchmarking.
\end{itemize}


\section{Related Works}

Research on counterfactual explanations has led to several libraries and benchmarking efforts designed to support implementation and evaluation. However, despite these contributions, a fully controlled and protocol-standardized benchmark for counterfactual explanations is still lacking.

\paragraph{Counterfactual explanation benchmarks.}
CARLA \cite{pawelczyk2021carla} is one of the earliest and most widely adopted benchmarking libraries for counterfactual explanations and algorithmic recourse. It provides implementations of multiple counterfactual generation methods, integrated evaluation measures, and several datasets within a unified Python framework. CARLA significantly improved accessibility and comparability across methods. However, it primarily focuses on providing a unified implementation framework, and does not aim to enforce a fully fixed experimental protocol across predictive backbones, preprocessing choices, and constraint configurations.

More recently, RobustX \cite{jiang2025robustx} introduced a framework dedicated to studying the robustness of counterfactual explanations under model and data perturbations. It provides standardized tools for generating and evaluating counterfactual methods with respect to this property. While highly valuable for analyzing explanation stability, its scope is focused on this specific aspect and does not aim to offer a comprehensive benchmark covering multiple counterfactual paradigms under a unified experimental protocol.

In contrast to prior libraries, CEL emphasizes protocol-level control across datasets, predictive backbones, preprocessing steps, constraint handling, and metric definitions, enabling systematic and fair comparison across local, global, and group-wise counterfactual paradigms.

\paragraph{General XAI benchmarking frameworks.}
Beyond counterfactual-specific libraries, several broader explainable AI toolkits have been proposed. OpenXAI \cite{agarwal2022openxai} provides a comprehensive framework for benchmarking post-hoc feature attribution methods, including implementations of multiple reliability metrics and synthetic data generators. Its primary focus is on local feature attribution methods rather than counterfactual explanations. Similarly, Quantus \cite{hedstrom2023quantus} offers a large collection of evaluation metrics for explanation methods, particularly targeting faithfulness and robustness of attribution-based explanations. However, it does not provide a benchmarking protocol for counterfactual generation methods.

OmniXAI \cite{yang2022omnixai} provides a production-oriented, unified interface supporting a wide range of explainability techniques across modalities, including feature attribution and counterfactual explanations. While OmniXAI facilitates practical integration and visualization of explanation methods, it does not enforce a standardized benchmarking protocol designed for systematic research comparison.

\section{CEL: Counterfactual Explanations Library}

To address fragmentation in counterfactual research, we introduce CEL, a framework designed to support systematic implementation, evaluation, and benchmarking of counterfactual explanation methods. CEL is built with modular components that separate models, datasets, counterfactual methods, and evaluation metrics. This structure allows flexible composition of components and simplifies extension with new methods, datasets, and metrics, while typed interfaces help ensure consistent data handling.


\subsection{Architecture and Design Principles}

The core architecture of CEL is built upon strict separation of concerns, ensuring that data preprocessing, model training, explanation generation, and evaluation operate as decoupled yet interoperable components. The library is implemented in Python, leveraging modern language features such as strict type hinting and dataclasses to ensure code reliability and clear interfaces.

CEL employs a \textbf{configuration-driven} experimental workflow to manage complex experimental setups. This allows researchers to define experiments, including model hyperparameters, dataset constraints, and metric selection, via hierarchical configuration files rather than hard-coded scripts.

\subsection{Unified Method Interfaces}

A key challenge in benchmarking is the heterogeneity of counterfactual algorithms. CEL unifies these under a polymorphic class hierarchy rooted in a \code{BaseCounterfactualMethod}. This abstract base class defines a standard interface for the explanation generation process, handling device placement and batch processing automatically. All methods return a standardized \code{ExplanationResult} dataclass, which encapsulates the generated counterfactuals, target labels, and method-specific metadata, allowing the evaluation pipeline to process results uniformly.

To accommodate diverse explanation strategies, CEL employs a \textbf{Mixin design pattern}:
\begin{itemize}
    \item \textbf{\code{LocalCounterfactualMixin}}: For methods that optimize perturbations for individual instances (e.g., Wachter, DiCE, PPCEF).
    \item \textbf{\code{GlobalCounterfactualMixin}}: For methods that learn global translation rules or directions applicable to the entire dataset (e.g., AReS, GLOBE-CE).
    \item \textbf{\code{GroupCounterfactualMixin}}: For approaches that generate explanations for specific subgroups or clusters (e.g., GLANCE, TCREx).
\end{itemize}

\subsection{Data Management and Preprocessing}

Valid counterfactual evaluation requires rigorous data handling to ensuring that generated instances remain within valid feature domains. CEL implements a robust \code{DatasetBase} architecture that manages feature metadata, including actionability constraints, mutability, and feature bounds.

The library features a flexible \code{PreprocessingPipeline}, allowing users to chain transformations such as scalers (Standard, MinMax, Robust) and encoders (OneHot, Ordinal). Crucially, the pipeline supports precise inverse transformations, ensuring that counterfactuals generated in processed space can be accurately mapped back to the original input space for evaluation.

For methods requiring continuous density estimation (e.g., normalizing flows), CEL includes a specialized \code{Dequantization} module. This module converts discrete categorical features into continuous values using variational or noise-based dequantizers, bridging the gap between discrete tabular data and continuous generative models.

\subsection{Predictive and Generative Backbones}

CEL supports a wide range of model architectures via a unified \code{PytorchBase} interface, categorized into task-specific mixins.

\textbf{Discriminative Models.} To ensure broad compatibility, the library implements \code{ClassifierPytorchMixin} and \code{RegressionPytorchMixin}. Implemented architectures range from standard Multi-Layer Perceptrons (MLP) and Logistic Regression to advanced Neural Oblivious Decision Ensembles (NODE), allowing researchers to test counterfactual generation against models of varying complexity and interpretability and easily extend their code with new implementations.

\textbf{Generative Models.} Some of modern counterfactual methods and metrics rely on density estimation to ensure the plausibility of generated explanations. CEL integrates a \code{GenerativePytorchMixin} that standardizes likelihood estimation and sampling interfaces. The library provides implementations of state-of-the-art Normalizing Flows, including Masked Autoregressive Flow (MAF) \cite{papamakarios2017masked}, RealNVP \cite{dinh2016density}, and NICE \cite{DinhKB14}. Additionally, it supports Kernel Density Estimation (KDE) and Gaussian Mixture Models (GMM) for baseline comparisons. These models allow users to estimate the likelihood of generated counterfactuals, a key component in assessing plausibility.

\subsection{Evaluation Orchestration}

CEL introduces a \code{MetricsOrchestrator} which allows to use existing metrics or implement custom metrics. This component leverages a registry-based system to dynamically instantiate and compute metrics defined in the experiment configuration. The orchestrator handles input validation and compute a comprehensive suite of metrics covering validity, proximity, sparsity, and density-based plausibility scores. Metrics are detailed in Appendix \ref{app:metrics}.

\vspace{1em}

CEL modular workflow is demonstrated in Listing \ref{lst:code_snippet}, which illustrates how the Dataset, Model, Method, and MetricsOrchestrator components interact. As shown, the framework abstracts the complexity of training and optimization, allowing users to generate and evaluate counterfactuals with minimal boilerplate code.

\begin{lstlisting}[float=tp, language=Python, backgroundcolor=\color{lightblue}, caption={Code snippet showing the modular workflow of CEL.}, label={lst:code_snippet}]
from cel.datasets import FileDataset
from cel.models.classifiers import MLPClassifier
from cel.cf_methods.local_methods import PPCEF
from cel.metrics import MetricsOrchestrator

# 1. Configuration-driven Data Loading
dataset = FileDataset(config_path="dataset.yaml")

# 2. Model Training (Unified PytorchBase Interface)
classifier = MLPClassifier(...)
classifier.fit(dataset.train_dataloader())

# 3. Counterfactual Generation (LocalMixin)
method = PPCEF(classifier, gen_model=flow_model, ...)
result = method.explain_dataloader(dataset.test_dataloader())

# 4. Standardized Evaluation
metrics = MetricsOrchestrator(conf_path="default.yaml")
scores = metrics.compute(result)
\end{lstlisting}


\section{Benchmark}

A good counterfactual benchmark must be built on a foundation of standardization, comprehensiveness, and reproducibility to enable fair and meaningful comparisons across methods \cite{pawelczyk2021carla, de2021framework}. A key characteristic is a \textbf{standardized evaluation protocol}, which includes a fixed suite of diverse datasets, pre-defined predictive models, and consistent preprocessing pipelines. This eliminates experimental variation as a confounding factor in performance, a common issue in prior studies \cite{pineau2021improving}.

Furthermore, a good benchmark should be comprehensive in its coverage. This means including a wide array of counterfactual methods spanning local, group-wise, and global paradigms. The evaluation metrics should also be multi-faceted, capturing not just a single notion of quality but a range of desirable properties such as validity, proximity, sparsity and plausibility \cite{guidotti2024counterfactual}.

Finally, \textbf{reproducibility and extensibility} are crucial for long-term value. The benchmark should be open-source, allowing researchers to easily replicate the results and integrate their own methods, datasets, and metrics. This fosters a collaborative environment for the community to build on and drive progress in the field of counterfactual explanations. CEL is designed with these principles at its core.

\subsection{Datasets}
CEL includes a diverse collection of 18 pre-configured datasets, covering both classification (13 datasets) and regression (5 datasets) tasks. The datasets span various domains including finance and social sciences, ensuring that methods are tested across different data distributions and complexities. Table \ref{tab:datasets_complete} describes datasets and their characteristics.

\begin{table}[!ht]
\centering
\footnotesize
\setlength{\tabcolsep}{2pt}
\caption{Datasets included in CEL with feature types, distribution and references.}
\label{tab:datasets_complete}
\begin{tabular}{l l r r r r r}
\toprule
\textbf{Dataset} & \textbf{Task} & \textbf{$N$} & \textbf{Cat} & \textbf{Num} & \textbf{$C$} & \textbf{Label Distribution} \\
\midrule
Adult Census \cite{adult} & Clf. & 32,561 & 8 & 4 & 2 & 0: 75.9\%, 1: 24.1\% \\
Audit \cite{audit} & Clf. & 775 & 0 & 23 & 2 & 0: 60.6\%, 1: 39.4\% \\
Bank Marketing \cite{moro2014bank} & Clf. & 40,004 & 9 & 7 & 2 & 0: 88.3\%, 1: 11.7\% \\
Blobs & Clf. & 1,500 & 0 & 2 & 3 & 0, 1, 2: 33.3\% each \\
Credit Default \cite{credit_default_data} & Clf. & 30,000 & 9 & 14 & 2 & 0: 77.9\%, 1: 22.1\% \\
Digits \cite{digits} & Clf. & 1,797 & 0 & 64 & 10 & Balanced ($\sim$10\% each) \\
German Credit \cite{german_credit_data} & Clf. & 1,000 & 11 & 7 & 2 & 0: 70.0\%, 1: 30.0\% \\
GMC \cite{pawelczyk2020learning} & Clf. & 16,714 & 3 & 7 & 2 & 0: 50.0\%, 1: 50.0\% \\
HELOC \cite{heloc} & Clf. & 10,459 & 0 & 23 & 2 & 0: 52.2\%, 1: 47.8\% \\
Law \cite{law} & Clf. & 2,216 & 2 & 3 & 2 & 0: 50.0\%, 1: 50.0\% \\
Lending Club \cite{jagtiani2019roles} & Clf. & 93,888 & 4 & 8 & 2 & 0: 29.6\%, 1: 70.4\% \\
Moons & Clf. & 1,024 & 0 & 2 & 2 & 0: 50.0\%, 1: 50.0\% \\
Wine \cite{wine} & Clf. & 178 & 0 & 13 & 3 & 0: 33.1, 1: 39.9, 2: 27\% \\
\midrule
Synthetic & Regr. & 1,000 & 0 & 2 & Cont. & [0, 1] \\
Concrete \cite{concrete_compressive_strength_data} & Regr. & 1,030 & 0 & 8 & Cont. & [0, 1]\\
Diabetes \cite{diabetes_data} & Regr. & 442 & 0 & 10 & Cont. & [0, 1] \\
Yacht \cite{yacht_hydrodynamics_data} & Regr. & 308 & 0 & 6 & Cont. & [0, 1] \\
SCM20D \cite{scm20d_data} & Regr. & 8,966 & 0 & 61 & Cont. (16) & $[0, 1]^{16}$ \\
\bottomrule
\end{tabular}
\end{table}

% The benchmark handles data preprocessing, and splitting automatically. It supports distinct feature types (numerical, categorical) and allows users to define actionability constraints (immutable features) and feature bounds, which are critical for generating realistic counterfactuals.

Consistent preprocessing is applied across all datasets. Continuous features are scaled to the $[0, 1]$ range using Min-Max normalization. Categorical features are handled based on the method's requirements For discriminative models and CF methods, categorical features are maintained in their original encoded format. to facilitate training of density estimator we apply variational dequantization to discrete features, mapping them into a continuous latent space  while maintaining the discrete structure during inverse transformations.

\subsection{Models}
CEL integrates distinct types of predictive and generative models implemented in PyTorch, which are essential for generating and evaluating counterfactual explanations. To ensure rigorous evaluation, CEL implements several predictors that we used in our benchmark. \textbf{Classification Models:} The library includes implementations of Multi-Layer Perceptrons (MLP), Logistic Regression. \textbf{Regression Models:} For continuous target variables, CEL provides Linear Regression and MLP Regressor models. \textbf{Density Estimator:} To support plausibility evaluation and methods that rely on density estimation we utilise Masked Autoregressive Flow, accordingly to \citet{wielopolski2024probabilistically}.


\subsection{Methods}
The library implements 14 counterfactual explanation methods, categorized into local, global, and group-wise approaches. All methods adhere to a common interface, facilitating direct comparison.

\begin{table}[h!]
\centering
\caption{Counterfactual explanation methods included in the CEL benchmark.}
\label{tab:methods}
\begin{tabular}{l l l}
\toprule
\textbf{Category} & \textbf{Method} & \textbf{Citation} \\
\midrule
\multirow{9}{*}{Local} & Wachter & \cite{wachter2017counterfactual} \\
& Artelt & \cite{artelt2020convex} \\
& DiCE & \cite{mothilal2020explaining} \\
& CCHVAE & \cite{karimi2020model} \\
& PPCEF & \cite{wielopolski2024probabilistically} \\
& CEM & \cite{DhurandharCLTTS18} \\
& CEGP & \cite{LooverenK21} \\
& CADEX & \cite{moore2019explaining} \\
& SACE & \cite{keane2020good} \\
& CEARM & \cite{spooner2021counterfactual} \\
\midrule
\multirow{2}{*}{Global} & AReS & \cite{rawal2020beyond} \\
& GLOBE-CE & \cite{ley2023globe} \\
\midrule
\multirow{2}{*}{Group-wise} & GLANCE & \cite{kavouras2024glance} \\
& T-CREx & \cite{bewley2024tcrex} \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Local Methods}
Local methods generate explanations for individual instances. CEL includes classic optimization-based approaches such as Wachter et al. \cite{wachter2017counterfactual} and DiCE \cite{mothilal2020explaining}, which solve optimization problems to generate counterfactuals. The library also supports several methods that focus on plausibility via density-based constraints and generative models, including the method of \citet{artelt2020convex} (Artelt); CCHVAE \cite{karimi2020model}, which leverages variational autoencoders for manifold-constrained generation; PPCEF \cite{wielopolski2024probabilistically}, a flow-based approach with an explicit probabilistic formulation; and CEGP \cite{LooverenK21}, which guides perturbations toward prototype-based counterfactuals. CEM \cite{DhurandharCLTTS18} uses autoencoders to verify the plausibility of perturbed instances. The case-based method SACE \cite{keane2020good} and CADEX \cite{moore2019explaining} (constrained adversarial examples) are also included. Additionally, CEL implements CEARM \cite{spooner2021counterfactual}, a Bayesian optimization-based method designed for regression models with a globally convergent search algorithm.

\subsubsection{Global Methods}
Global methods provide dataset-level insights. CEL implements AReS \cite{rawal2020beyond}, which generates global recourse rules, and GLOBE-CE \cite{ley2023globe}, which learns global translation directions. Additionally, we include GLANCE \cite{kavouras2024glance} in our comparison by configuring it with a single group, effectively adapting this group-wise method for a global evaluation. These methods are essential for understanding high-level model behavior and identifying systemic biases.

\subsubsection{Group-wise Methods}
Bridging the gap between local and global, group-wise methods generate explanations for subgroups of data. CEL includes GLANCE \cite{kavouras2024glance} and TCREX \cite{bewley2024tcrex}, which allow users to inspect counterfactuals for specific clusters or demographic groups.

\subsection{Metrics}
\label{sec:metrics}
To ensure a holistic evaluation, CEL provides a comprehensive suite of metrics covering key counterfactual properties: Validity, Proximity, Sparsity, Plausibility. To ensure rigor evaluation we selected such metrics to evaluate each property:
\begin{itemize}
    \item \textbf{Validity:} Measures the success rate of generated counterfactuals in achieving the target prediction. For continuous targets, validity is measured as the mean absolute error (MAE) between the predicted value of the counterfactual and the target value.
    \item \textbf{Proximity:} Quantifies the distance between the original instance and the counterfactual using \textit{Euclidean} distance for continuous features. For mixed data we apply combined \textit{Euclidean-Hamming} distance, weighted proportionally by number of continuous and discrete features.
    \item \textbf{Sparsity:} Evaluates the fraction of modified features, encouraging simpler explanations.
    \item \textbf{Plausibility:} Assesses how well counterfactuals fit the data distribution using Log-Likelihood from density estimation model. We use MAF \cite{papamakarios2017masked} for this benchmark, due to its superiority verified by \citet{wielopolski2024probabilistically}.
\end{itemize}
This evaluation ensures that methods are not just optimized for a single criterion (e.g., validity) but produce high-quality, actionable, and realistic explanations. We include detailed explanation of all metrics in Appendix \ref{app:metrics}.

\subsection{Experimental Setup}
The goal of our experimental evaluation is to systematically compare counterfactual explanation methods across local, global, and group-wise paradigms under controlled and identical conditions, thereby isolating methodological differences from confounding experimental factors. We evaluate all methods across 18 datasets of varying size, dimensionality, and feature composition, using two predictive backbones per task type, and assess performance along multiple complementary characteristics including validity, proximity, sparsity, and plausibility. This broad scope is designed to reveal not only absolute method performance but also the trade-offs and failure modes that emerge across diverse data characteristics and evaluation criteria.

To ensure a fair and reproducible comparison across all counterfactual explanation methods, we adopt a standardized experimental protocol. We evaluate all methods using a 5-fold cross-validation strategy. For each fold, we train both the discriminative (predictive) model on the training split. To ensure that conditional density is estimated with respect to the appropriate class distribution, we train a generative density estimator on data labeled according to the predictions of the trained discriminative model. Specifically, we model class-conditional densities using normalizing flows, and in particular masked autoregressive flows (MAF) \cite{papamakarios2017masked}. This approach allows flexible estimation of complex data distributions while preserving tractable likelihood computation.

For each dataset, we define a target class and generate counterfactual explanations for test instances whose prediction differs from this target. For multiclass datasets, we restrict the evaluation to a binary setting by selecting two classes and generating counterfactuals only between this class pair. This ensures that counterfactual generation and density estimation are aligned with a well-defined decision boundary and allows consistent comparison across datasets with different numbers of classes.

For regression tasks, we set the problem to increasing the predicted value. The desired target value is defined as the original prediction increased by 20\% of the overall range of the target variable. We measure validity as the Mean Absolute Error (MAE) between the counterfactual's prediction and this target value.

\subsection{Results}

In this section, we present representative results from our benchmark. The complete evaluation across all datasets and models is provided in Appendix~\ref{app:full_results}. Our benchmark reports multiple metrics capturing different properties of counterfactual explanations, including validity, proximity, and plausibility. This multi-dimensional evaluation enables analysis of trade-offs between competing objectives.

\subsubsection{Local Methods}

We evaluate local counterfactual methods on datasets with mixed feature types (categorical and numerical) as well as purely numerical datasets. Some methods, such as Wachter, are restricted to numerical features, which limits their applicability in mixed settings.

Tables~\ref{tab:results_law_local} and~\ref{tab:results_wine_local} report example results for the Law and Wine datasets, respectively. Across both datasets, most methods achieve near-perfect validity, indicating that generating counterfactuals that flip the prediction is generally feasible in these settings. However, substantial differences emerge in proximity and plausibility metrics.

On the Law dataset (Table~\ref{tab:results_law_local}), PPCEF achieves the lowest perturbation distance while maintaining high plausibility, illustrating that combining gradient-based optimization with density-aware objectives can balance proximity and realism. In contrast, some methods produce valid counterfactuals with higher distances or lower density values, highlighting trade-offs between minimal change and plausibility.

On the Wine dataset (Table~\ref{tab:results_wine_local}), variability across methods is more pronounced. While validity remains saturated, proximity and log-density scores differ considerably. PPCEF achieves the highest density values, indicating counterfactuals located in higher-density regions of the data distribution. At the same time, methods such as CADEX and CEGP achieve lower perturbation distances, suggesting stronger proximity but not necessarily improved plausibility.

These results illustrate that single-metric evaluation can obscure important differences between methods. Even when validity is saturated, proximity and plausibility metrics reveal meaningful variation, supporting the need for controlled, multi-dimensional benchmarking.

\input{small-tables/results_law_local}
\input{small-tables/results_wine_local}

Furthermore, we evaluate counterfactual methods on regression tasks. For regression, validity is measured as the Mean Absolute Error (MAE) between the counterfactual prediction and the target value. Table~\ref{tab:regression_lr_representative} shows example results for CEARM and Wachter (WACH). While both methods achieve comparable MAE, Wachter consistently finds counterfactuals with much smaller L2 distances, indicating more efficient perturbations. Wachter also demonstrates superior plausibility, with LOF scores closer to 1 and higher log-density values, suggesting its outputs are better aligned with the data distribution. This highlights that even with similar validity, methods can vary significantly in proximity and plausibility.

\input{small-tables/results_regression_representative}

\subsubsection{Global and Groupwise Methods}
We additionally evaluate group-wise and global counterfactual explanation methods. For group-wise approaches, we compare TCREx \cite{bewley2024tcrex} and GLANCE \cite{kavouras2024glance}. For global methods, we evaluate GLOBE-CE \cite{ley2023globe}, AReS \cite{rawal2020beyond}, and GLANCE configured with a single group, allowing it to operate in a global setting.

Example results for the Give Me Some Credit dataset are presented in Table~\ref{tab:global_give_me_some_credit}. In the global setting, methods learn a single shift applied across instances.

As shown in Table~\ref{tab:global_give_me_some_credit}, GLOBE-CE achieves perfect validity with minimal perturbation distance. AReS attains slightly lower validity but higher density values, while GlobalGLANCE reaches perfect validity at the cost of substantially larger perturbations and lower density. These results highlight trade-offs between validity, proximity, and plausibility in global counterfactual methods.

\input{small-tables/results_gmsc_global}

Example results for the Adult Census dataset are shown in Table~\ref{tab:group_adult_census}. Unlike local methods, group-wise and global approaches learn shared shift vectors applied to multiple instances, which changes how instance-level metrics should be interpreted. In this setting, validity reflects the fraction of instances successfully flipped by the shared shift.

As seen in Table~\ref{tab:group_adult_census}, TCREx yields very small perturbations (reported as $0.00$ due to rounding), but these shifts rarely change predictions, leading to low validity. In contrast, GroupGLANCE applies larger shifts that substantially improve validity at the cost of higher proximity values.

These results illustrate inherent trade-offs in group and global counterfactual paradigms, where shared transformations must balance effectiveness across instances against minimal change.

\input{small-tables/results_adult_census_group}


\section{Conclusions}
In this work, we introduced CEL, a unified benchmark and open-source library designed to address the critical need for standardized evaluation of counterfactual explanation methods. By providing a controlled experimental protocol with harmonized datasets, predictive models, and evaluation metrics, CEL enables fair and reproducible comparisons across diverse counterfactual generation paradigms, a significant step forward from the fragmented evaluation practices common in the field.

Our comprehensive benchmark of 13 methods on 18 datasets revealed key trade-offs inherent in current state-of-the-art approaches. We found that many methods excel on one or two dimensions of quality at the expense of others. For example, local methods often achieve high validity and proximity but may produce implausible counterfactuals that do not lie on the data manifold, unless explicitly optimized for plausibility using density estimators. Similarly, global and group-wise methods face a fundamental tension between the broad applicability of a single explanation and its effectiveness for individual instances. These findings underscore that there is no one-size-fits-all solution; the choice of a counterfactual method should be guided by the specific requirements of the application.

The CEL framework is designed to be extensible and we hope it will serve as a catalyst for future research. We encourage the community to contribute new methods, datasets, and metrics to further enrich the benchmark. Future work could focus on several promising directions, including the evaluation of counterfactuals for fairness and robustness against model changes, the development of methods that can be optimized for different trade-offs in a more user-centric manner, and the exploration of counterfactual explanations for more complex data modalities. By providing a solid foundation for rigorous and transparent evaluation, we believe CEL will help accelerate progress towards more reliable and trustworthy explainable AI.

%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
% \begin{acks}

% Tutaj potem granty trzeba wpisać

% \end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}


%%
%% If your work has an appendix, this is the place to put it.
\clearpage
\appendix
\onecolumn

\section{Evaluation Metrics in CEL}
\label{app:metrics}
To provide a comprehensive assessment of counterfactual methods, CEL implements a broad evaluation suite. We categorize these metrics into three primary dimensions: basic performance, proximity (cost) and plausibility.
\subsection{Basic Metrics}
These metrics assess the fundamental success and efficiency of the counterfactual generation process.
\begin{itemize}
    \item \textbf{Coverage:} The proportion of instances in the test set for which the method successfully returned a counterfactual candidate.
    \item \textbf{Validity:} The proportion of generated counterfactuals that successfully altered the model's prediction to the desired target class $y_{target}$.
    \item \textbf{Regression Validity:} For continuous targets, validity is measured as the mean absolute error (MAE) between the predicted value of the counterfactual and the desired target value.
    \item \textbf{Actionability:} The proportion of instances where the generated counterfactual is identical to the original input (lower is better, as it indicates a failure to provide a meaningful path to recourse).
    \item \textbf{Sparsity:} The average proportion of features modified to achieve the counterfactual. For an input $x$ and counterfactual $x'$, it is defined as:
    \begin{equation}
        \text{Sparsity}(x, x') = \frac{1}{d} \sum_{i=1}^{d} \mathbb{I}(x_i \neq x'_i)
    \end{equation}
\end{itemize}
\subsection{Proximity and Cost}
Proximity metrics measure the distance between the factual instance $x$ and the counterfactual $x'$, representing the "effort" required by the user to achieve recourse. CEL handles mixed-type data by calculating combined distances:
\begin{equation}
    D_{total}(x, x') = \alpha \cdot D_{cont}(x, x') + (1 - \alpha) \cdot D_{cat}(x, x')
\end{equation}
where $\alpha$ is the ratio of continuous features. The library supports:
\begin{itemize}
    \item \textbf{Continuous Distances ($D_{cont}$):} Euclidean ($L_2$), Manhattan ($L_1$), and Median Absolute Deviation (MAD) normalized distance.
    \item \textbf{Categorical Distances ($D_{cat}$):} Hamming distance and Jaccard distance.
\end{itemize}
\subsection{Plausibility and Realism}
Plausibility metrics evaluate whether the counterfactuals lie within the data manifold, ensuring they are not just adversarial perturbations but realistic instances.
\begin{itemize}
    \item \textbf{Log-Density:} The average log-likelihood of counterfactuals estimated by a pre-trained generative model (e.g., Masked Autoregressive Flow). Higher values indicate better alignment with the training distribution.
    \item \textbf{Probabilistic Plausibility:} The fraction of counterfactuals whose log-likelihood exceeds a specified threshold $\tau$ (e.g., the median log-likelihood of the training set) proposed by \citet{wielopolski2024probabilistically}.
    \item \textbf{LOF/Isolation Forest Scores:} Outlier-based measures where counterfactuals are compared against the training set $X_{train}$ using Local Outlier Factor or Isolation Forest. These identify whether the recourse path leads to "out-of-distribution" regions.
\end{itemize}


\section{Hyperparameter Configurations}
\label{app:hyperparameters}

To provide complete transparency and support reproducibility, this section details the hyperparameters used for all discriminative models, generative models, and counterfactual explanation methods in our benchmark.

\subsection{Discriminative Models}

The predictive models serve as the targets for explanation. We standardize the training procedure across all architectures: epochs: 5,000, learning rate: $0.001$, batch size: 128, and early stopping patience: 300 epochs.

\begin{itemize}
    \item \textbf{MLP:} 2 hidden layers of 256 units each.
    \item \textbf{Logistic Regression:} Standard implementation without additional regularization penalties beyond those inherent in the Adam optimizer.
\end{itemize}

\subsection{Density Estimation Models}

Masked Autoregressive Flow Parameters is used for density-based plausibility metrics and manifold-constrained search. Training hyperparameters include: epochs: 2,000, learning rate: $0.003$, batch size: 1,024. MAF architecture: 8 layers, 4 blocks/layer, 16 hidden features.

% \section{Full results}
% \label{app:full_results}

% % mixed datasets for local methods

% \input{tables/results_categorical}

% % numerical datasets for local methods

% \input{tables/results_numerical}

% % global and group-wise method
% \input{tables/results_global}
% \input{tables/results_group}

% % Regression

% \input{tables/results_regression}


\section{Full Results}
\label{app:full_results}

This appendix presents the complete quantitative results of our benchmark evaluation. The main text reports representative examples on selected datasets; here we provide the full set of results across all datasets, methods, and predictive models. All reported values are averaged over 5-fold cross-validation, with standard deviations indicated by $\pm$ notation. Bold values denote the best result for each metric within a given dataset and model combination.

For each experimental configuration, we report two complementary tables: (i) a \textit{proximity table} containing distance-based metrics (L2-Hamming, L2-Jaccard, L1-Jaccard, MAD-Jaccard, and MAD-Hamming), and (ii) an \textit{additional metrics table} reporting coverage, validity, sparsity, probabilistic plausibility, log-density, LOF score, Isolation Forest score, and wall-clock computation time. Together, these tables provide a multi-dimensional view of each method's performance, capturing trade-offs between validity, proximity, plausibility, and computational cost.

Results are organized by counterfactual paradigm and feature type as follows.

\subsection{Local Methods on Mixed-Type Datasets}

This subsection reports results for local counterfactual methods evaluated on seven classification datasets containing both categorical and numerical features: Adult Census, Bank Marketing, Credit Default, German Credit, Give Me Some Credit, Law, and Lending Club. Due to the presence of categorical features, only methods that natively support mixed feature types are included in this comparison: CADEX, CCHVAE, SACE, DiCE, and PPCEF. Results are reported separately for MLP and Logistic Regression predictive models.

Tables~\ref{tab:cat_proximity_mlp} and~\ref{tab:cat_proximity_lr} report proximity metrics for MLP and Logistic Regression backbones, respectively, including combined Euclidean-Hamming, Euclidean-Jaccard, L1-Jaccard, MAD-Jaccard, and MAD-Hamming distances. Tables~\ref{tab:cat_additional_mlp} and~\ref{tab:cat_additional_lr} report complementary metrics: coverage, validity, sparsity, probabilistic plausibility, log-density, LOF and Isolation Forest outlier scores, and computation time.

Across all datasets and both predictive models, all methods achieve perfect coverage. Validity is near-perfect for most methods, with the notable exception of CADEX, which on Adult Census achieves only $\sim$0.51 validity (MLP), indicating that its constrained adversarial perturbations are often insufficient to cross the decision boundary in this setting. DiCE also exhibits occasional drops in validity, particularly under the Logistic Regression backbone (e.g., 0.73 on German Credit, 0.79 on both Give Me Some Credit and Lending Club).

In terms of proximity, CADEX and PPCEF tend to achieve the lowest L2-Hamming distances on several datasets (e.g., Adult Census, Bank Marketing, Give Me Some Credit), while DiCE achieves the smallest perturbations on German Credit. On Law, PPCEF consistently achieves the best proximity across both backbones. However, low proximity does not always correspond to high plausibility: on datasets with large feature ranges such as Credit Default and Give Me Some Credit, CADEX and DiCE exhibit extremely high MAD-based distances and low log-density values, indicating that their counterfactuals, while close in normalized space, may fall in low-density regions.

SACE stands out for its consistently strong plausibility scores. It achieves the highest probabilistic plausibility and log-density values across most datasets (e.g., 0.93 and $-109.47$ on Adult Census with MLP; 0.97 and $22.11$ on Give Me Some Credit), along with the lowest LOF scores, suggesting that its case-based retrieval strategy naturally produces counterfactuals close to the training data manifold. CCHVAE also achieves competitive plausibility on smaller datasets such as Law and German Credit. In contrast, PPCEF, despite its flow-based density objective, sometimes yields low probabilistic plausibility on larger datasets (e.g., 0.00 on Give Me Some Credit and Lending Club), suggesting that its optimization may converge to regions outside the high-density support.

SACE and CCHVAE are also among the fastest methods, while PPCEF consistently requires the longest computation time due to its per-instance flow-based optimization.

\input{tables/results_categorical}

\subsection{Local Methods on Numerical Datasets}

This subsection reports results for local counterfactual methods evaluated on six classification datasets with exclusively numerical features: Blobs, Digits, Moons, Wine, Audit, and HELOC. Since these datasets do not contain categorical features, additional methods that operate only on continuous inputs are included: Artelt, CEGP, CEM, and Wachter, alongside CADEX, CCHVAE, SACE, DiCE, and PPCEF. Results are reported separately for MLP and Logistic Regression predictive models.

Tables~\ref{tab:num_proximity_mlp} and~\ref{tab:num_proximity_lr} report proximity metrics for MLP and Logistic Regression backbones, respectively. Tables~\ref{tab:num_additional_mlp} and~\ref{tab:num_additional_lr} report coverage, validity, sparsity, plausibility, outlier scores, and computation time.

All methods achieve perfect coverage except Artelt, which returns zero coverage on several datasets (Blobs, Moons, HELOC with MLP), indicating a failure to produce counterfactual candidates in these settings. Validity is generally high across most methods, though DiCE shows reduced validity on some datasets (e.g., 0.80 on Wine, 0.41 on Audit with MLP). CEM also struggles with validity on Audit under Logistic Regression (0.02).

In terms of proximity, CADEX and CEGP consistently achieve the lowest L2 distances across most datasets. On Wine and HELOC, CEGP achieves the smallest perturbations under both metrics and backbones, while CADEX leads on Moons and Blobs. Wachter achieves competitive proximity, particularly on low-dimensional datasets such as Moons and Blobs, where it closely matches CADEX.

Plausibility metrics reveal strong differentiation. PPCEF achieves perfect probabilistic plausibility (1.00) on low-dimensional datasets (Blobs, Moons, Wine) under both backbones, along with the highest log-density values, demonstrating that its flow-based objective effectively constrains counterfactuals to high-density regions when the data manifold is well-captured. SACE similarly achieves strong plausibility on datasets such as Digits and Audit, with the highest log-density and lowest LOF scores in these settings. In contrast, methods without explicit density objectives (Wachter, CADEX, CEGP) generally achieve near-zero probabilistic plausibility, indicating that proximity-focused optimization alone does not ensure distributional alignment.

The Audit dataset exposes numerical instabilities: several methods exhibit extremely high LOF values (on the order of $10^{7}$), suggesting that counterfactuals fall far outside the training data manifold on this high-dimensional dataset with heterogeneous feature scales.

Wachter is consistently the fastest method, often completing in under 0.1 seconds, while CEGP and CEM require substantially longer computation times, particularly on larger datasets such as HELOC ($>$1,500s for CEGP).

\input{tables/results_numerical}

\subsection{Regression Methods}

This subsection reports results for counterfactual explanation methods applied to regression tasks. Validity for regression is measured as the mean absolute error (MAE) between the predicted value of the counterfactual and the desired target value, rather than as a binary classification success rate. The evaluated methods include CEARM and Wachter (WACH), tested on five datasets: Synthetic, Concrete, Diabetes, Yacht, and SCM20D. Results are reported for both Linear Regression and MLP Regressor models. Tables~\ref{tab:regression_all_lr} and~\ref{tab:regression_all_dnn} report the performance metrics for Linear Regression and MLP backbones, respectively.

CEARM is not applicable to the SCM20D dataset (multi-output regression with 16 targets), leaving Wachter as the only method evaluated in this setting. On the remaining four datasets, both methods achieve comparable MAE values, indicating similar effectiveness in reaching the target prediction. For instance, on Synthetic both methods achieve MAE of approximately 0.05 under both backbones.

However, Wachter consistently achieves substantially lower proximity values. Its L2 distances are typically 3--8$\times$ smaller than CEARM's across all datasets and backbones (e.g., 0.14 vs.\ 0.37 on Synthetic with MLP; 0.09 vs.\ 1.16 on Concrete with MLP). This indicates that Wachter produces counterfactuals with more efficient perturbations while achieving comparable target accuracy.

In terms of plausibility, Wachter achieves better outlier scores across nearly all configurations. Its LOF values are consistently close to 1.0 (e.g., 1.05--1.17), while CEARM produces higher LOF values (1.14--2.27), suggesting that CEARM's counterfactuals are more likely to lie in low-density regions. Wachter also achieves higher Isolation Forest scores and substantially better log-density values. On Concrete with MLP, Wachter achieves a log-density of 5.70 compared to $-152.55$ for CEARM. CEARM achieves higher probabilistic plausibility only on the Synthetic dataset (0.16--0.20 vs.\ 0.01--0.03 for Wachter), suggesting that its Bayesian optimization strategy can find plausible counterfactuals on simple data manifolds.

Wachter is also significantly faster, completing in 3--14 seconds compared to CEARM's 24--100 seconds, due to its simpler gradient-based optimization.

\input{tables/results_regression}

\subsection{Global Methods}

This subsection reports results for global counterfactual explanation methods, which learn a single transformation or shift vector applied uniformly across all instances. The evaluated methods include AReS, GLOBE-CE, and GLANCE configured with a single group (GlobalGLANCE). Results are reported across all 13 classification datasets and for both MLP and Logistic Regression backbones.

Tables~\ref{tab:global_proximity_mlp} and~\ref{tab:global_proximity_lr} report proximity metrics for MLP and Logistic Regression, respectively. Tables~\ref{tab:global_additional_mlp} and~\ref{tab:global_additional_lr} report coverage, validity, sparsity, plausibility, outlier scores, and computation time.

A notable observation is that AReS fails to produce results (indicated by --) on many datasets, including Audit, Blobs, Digits, Moons, and Wine under both backbones, as well as Credit Default and Law. This substantially limits its applicability compared to the other global methods. Similarly, GLOBE-CE fails on the Digits dataset, likely due to the high dimensionality (64 features) making it difficult to find a single effective global shift.

Where all three methods produce results, clear trade-offs emerge. GLOBE-CE generally achieves the lowest proximity values, often by a large margin. For example, on Adult Census it achieves near-zero distances across all proximity metrics under both backbones, and on Give Me Some Credit it achieves L2-Hamming distances of 0.01 compared to 0.86 for GlobalGLANCE (MLP). GLOBE-CE also tends to achieve high or perfect validity on most datasets (e.g., 1.00 on Adult Census, Give Me Some Credit, Moons, Law). However, on some datasets it achieves lower validity (e.g., 0.67 on Bank Marketing, 0.75 on Credit Default with MLP), indicating that a single global shift may not always suffice.

GlobalGLANCE achieves the highest validity overall, reaching 0.97--1.00 on most datasets, but at the cost of substantially larger perturbations. Its proximity values are typically several times higher than GLOBE-CE. AReS, when it succeeds, generally achieves low proximity but often with very low validity (e.g., 0.17 on Adult Census, 0.06 on Bank Marketing with MLP), suggesting that its rule-based recourse often fails to flip predictions.

In terms of plausibility, no single method dominates. AReS achieves the highest probabilistic plausibility on several datasets where it produces results (e.g., 0.50 on Give Me Some Credit, 0.41 on German Credit). GLOBE-CE is consistently the fastest method, typically completing in under 10 seconds, while GlobalGLANCE requires 30--260 seconds depending on the dataset.

\input{tables/results_global}

\subsection{Group-wise Methods}

This subsection reports results for group-wise counterfactual explanation methods, which generate shared explanations for subgroups of instances. The evaluated methods include TCREx and GLANCE (GroupGLANCE). Unlike local methods, the reported proximity and validity metrics reflect the effectiveness of shared shift vectors applied to instance subgroups rather than individually optimized perturbations. Results are reported across all 13 classification datasets and for both MLP and Logistic Regression backbones.

Tables~\ref{tab:group_proximity_mlp} and~\ref{tab:group_proximity_lr} report proximity metrics for MLP and Logistic Regression, respectively. Tables~\ref{tab:group_additional_mlp} and~\ref{tab:group_additional_lr} report coverage, validity, sparsity, plausibility, outlier scores, and computation time.

TCREx exhibits limited applicability, failing to produce results on several datasets including Credit Default, Digits, German Credit, Give Me Some Credit, Lending Club, and Wine under both backbones. On datasets where it succeeds, TCREx consistently achieves the smallest perturbations, often by a substantial margin. For instance, on Adult Census it produces near-zero distances across all proximity metrics, and on HELOC it achieves L2-Hamming of 0.08 compared to 0.76 for GroupGLANCE (MLP). However, these minimal perturbations come at the cost of extremely low validity: TCREx achieves only 0.23 on Adult Census, 0.04 on Audit, 0.01 on Blobs, and 0.03 on HELOC (MLP). This indicates that the subgroup-level shifts learned by TCREx are often too conservative to change model predictions.

In contrast, GroupGLANCE achieves substantially higher validity across all datasets, reaching 0.94--1.00 on most configurations. On Adult Census, GroupGLANCE achieves 0.99 validity (MLP) compared to TCREx's 0.23, though with considerably larger perturbations. This pattern is consistent: GroupGLANCE applies larger, more aggressive shifts that successfully flip predictions for a high proportion of instances within each subgroup.

The trade-off between proximity and validity in the group-wise paradigm is more pronounced than in local methods. Because shared shifts must be effective across multiple instances within a subgroup, methods face a fundamental tension between minimal change and broad effectiveness. TCREx favors minimal perturbation at the expense of validity, while GroupGLANCE prioritizes prediction flipping.

In terms of plausibility, TCREx achieves higher probabilistic plausibility and better LOF scores on datasets where it produces results (e.g., 0.49 on Adult Census, 0.26 on HELOC), reflecting the fact that its small perturbations keep counterfactuals close to the original data manifold. GroupGLANCE generally achieves lower plausibility scores due to its larger perturbations. TCREx is also considerably faster, typically completing in under 1 second, while GroupGLANCE requires 27--260 seconds.

\input{tables/results_group}


\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
